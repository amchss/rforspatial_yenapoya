[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "National Workshop on Data Analysis and Spatial Epidemiology using R",
    "section": "",
    "text": "This website is intended as a companion website and reference material for the three-day National Workshop on Data Analysis and Spatial Epidemiology Using R: A Hands-on Training for Health Professionals, organized by the Department of Public Health, Yenepoya Faculty of Allied and Healthcare Professions, Yenepoya (Deemed to be University), Mangalore, in association with the Edward & Cynthia Institute of Public Health.\nThe overall goal of the workshop is to provide an introduction to R programming language, the RStudio interface and the tidyverse workflows in data analysis and build a strong foundation in spatial epidemiology for healthcare professionals. This is also aimed at providing basic instructions on data wrangling, visualization, and communicating research findings effectively using principles of reproducible research.\n\n\nWhat is covered in the workshop?\n\nThe workshop is designed to span three days, with each day consisting of both interactive lectures and hands-on sessions to provide a comprehensive introduction to data analysis and spatial epidemiology using R.\nThe morning of Day One starts with an introduction and icebreaker session, setting the stage for the workshop by covering key concepts in data science and spatial epidemiology using R. This is followed by a session to familiarize participants with the R programming environment and RStudio, including installation, interface layout, and key features. Participants will then engage in hands-on exercises to work with basic data types such as vectors, matrices, and data frames.\nThe afternoon session will focus on the fundamentals of working with data in R, including managing file paths, working directories, and importing data. Participants will also be introduced to the concept of tidy data and engage in practical exercises on data wrangling using the dplyr package. The day concludes with an introduction to the Quarto framework through a group activity.\nDay Two begins with a recap of Day One and an opportunity for participants to ask questions. The morning session dives into data visualization using the ggplot2 package, covering the visualization of single, two, and multiple variables, followed by hands-on tutorials. The session then shifts towards spatial epidemiology, introducing its relevance to public health and covering fundamental concepts such as geometry, projections, and spatial data structures (vector and raster). Participants will explore spatial data through hands-on activities, including importing, exporting, and visualizing spatial data using choropleths and boxplots.\nThe afternoon session will focus on spatial data visualization and map-making, where participants will create maps adhering to Cartographic Guidelines for Public Health. The day concludes with an introduction to Quarto and a session on generating publication-ready tables using the gtsummary package.\nDay Three continues with a recap and doubt clearance session, followed by an in-depth exploration of spatial epidemiology. Participants will engage with real-life examples, including the historical John Snow map exercise. The session will then introduce different types of spatial data (areal, geostatistical, point patterns, spatio-temporal, and mobility data) along with practical exercises.\nThe afternoon session covers advanced spatial epidemiology concepts, such as spatial autocorrelation and clustering, with hands-on exercises using global and local measures. The workshop will conclude with demonstrations of real-life examples, followed by a feedback and valedictory session.\n\n\n\nWhat is not covered in the workshop?\n\nThis workshop does not provide in-depth coverage of advanced spatial epidemiology methods or geospatial programming beyond the basics. While we will introduce key concepts in spatial epidemiology, data science, and visualization using R, the focus is on practical, hands-on skills rather than detailed theoretical or statistical analyses. Participants seeking more advanced or specialized training in areas like advanced spatial statistics or geostatistical modeling may consider future workshops if there is enough interest and a sufficient number of participants.\n\n\n\nSchedule\n\n\n\n\n\n\n\n\n\nDate\nSession\nTime\nTopic\n\n\n\n\nDay 1\nMorning\n09:00 – 10:15\nRegistration & Inauguration\n\n\n\n\n10:15 – 10:45\nIntroduction and Icebreaker\n\n\n\n\n10:45 – 11:00\nTea\n\n\n\n\n11:00 – 11:15\nGetting comfortable with R and RStudio (Interactive Lecture)\n\n\n\n\n11:15 – 11:45\nGetting Comfortable with R and RStudio (Hands-On)\n\n\n\n\n11:45 – 12:00\nPrimer Concepts (Interactive Lecture)\n\n\n\n\n12:00 – 12:30\nPrimer Concepts (Hands - On)\n\n\n\n\n12:30 – 13:15\nLunch\n\n\n\nAfternoon\n13:15 – 13:45\nFundamentals of Working with Data (Interactive Lecture)\n\n\n\n\n13:45 – 14:15\nFundamentals of Working with Data (Hands-On)\n\n\n\n\n14:15 – 14:30\nFundamentals of Working with Data (Interactive Lecture)\n\n\n\n\n14:30 – 14:45\nFundamentals of Working with Data (Hands-On)\n\n\n\n\n14:45 – 15:15\nExploring Data with R (Interactive Lecture)\n\n\n\n\n15:15 – 15:30\nExploring Data with R (Hands- On)\n\n\n\n\n15:30 – 16:00\nExploring Data with R (Interactive Lecture)\n\n\n\n\n16:00 – 16:15\nTea\n\n\n\n\n16:15 –17:00\nExploring Data with R & Introduction to Quarto (Interactive Lecture and Hands-On)\n\n\nDay 2\nMorning\n09:00 – 09:30\nRecap of the previous day & Doubt Clearance Session\n\n\n\n\n09:30 – 10:00\nData Visualization (Interactive Lecture)\n\n\n\n\n10:00 – 10:20\nBreak fast & Tea break\n\n\n\n\n10:20 – 10:45\nData Visualization (Hands-On)\n\n\n\n\n10:45 – 11:15\nIntroduction to Spatial Epidemiology (Interactive lecture)\n\n\n\n\n11:15 – 11:45\nRefresh on the basic concepts (Interactive lecture)\n\n\n\n\n11:45 – 12:15\nSpatial Exploratory Data Analysis (Interactive lecture)\n\n\n\n\n12:30 – 13:15\nLunch\n\n\n\nAfternoon\n13:15 – 13:45\nSpatial Exploratory Data Analysis (Hands-on)\n\n\n\n\n13:45 – 14:30\nSpatial Data Visualization using R & Map making (Interactive lecture)\n\n\n\n\n14:30 – 15:30\nSpatial Data Visualization using R& Map Making (Hands on)\n\n\n\n\n15:30 – 16:00\nIntroduction to Quarto\n\n\n\n\n16:00 – 16:15\nTea\n\n\n\n\n16:15 – 17:00\nPublication Ready Tables (Lecture cum Hands on)\n\n\nDay 3\nMorning\n09:00 – 09:30\nRecap of the previous day & Doubt Clearance Session\n\n\n\n\n09:30 – 10:00\nSpatial Epidemiology (Interactive lecture)\n\n\n\n\n10:00 – 10:20\nTea\n\n\n\n\n10:20 – 10:45\nSpatial Epidemiology (Hands-on)\n\n\n\n\n10:45 – 12:30\nSpatial Data Types (lecture & Hands-on) and basics of spatial data analysis\n\n\n\n\n12:30 – 13:30\nLunch\n\n\n\nAfternoon\n13:30 – 14:30\nSpatial autocorrelation and advanced spatial Epidemiology\n\n\n\n\n14:30 – 14:35\nBio-Break\n\n\n\n\n14:35 – 15:00\nSpatial autocorrelation and advanced spatial Epidemiology (hands on)\n\n\n\n\n15:00 – 15:30\nAdvanced concepts and real-life examples- demo session\n\n\n\n\n15:30 – 16:00\nValedictory and Feedback"
  },
  {
    "objectID": "spatial_data_visual.html",
    "href": "spatial_data_visual.html",
    "title": "Visualizing Spatial Data",
    "section": "",
    "text": "The role of spatial data visualization in understanding distribution and determinants of a disease were classically demonstrated in John Snow’s work in cholera epidemic of Golden Square, London in 1854 and maps have remained the most common applied aspect of spatial epidemiology till date. Maps enable real time visualizations. However, though the concepts of spatial data visualization has its historical roots in the practice of cartography1, with the evolution of map making computational abilities, the requirement to know the cartography concepts for visualization spatial data for general use has decreased to minimal. For eg. we can point a location in Google maps and reach a destination without deliberations on cartographic concepts (such as northing, scale, etc). This has led to neogeography2 and thus crowd-sourced databases due to volunteer contributions.\nIn professional and academic forums, the concepts of cartography are pre-requisites to meaningful spatial data visualization. Further, the concept of using visualization methods has evolved from just a tool for communication to the concept of visualization as an analysis tool. Visualization of spatial data is used to understand distribution patterns and generate hypothesis as well as for inferential procedures. The underlying principles in Scientific visualization has led to use of visualization methods as complementary to statistical inference procedures in spatial epidemiology. For example, the interpretation of Moran’s I3 is incomplete without accompanying visualization of dataset. The maps as analysis tools have several possible meanings and are thus polysemic in nature.\n\n\nIt is expected that by end of this session, students should be able to develop visualizations, understand and communicate epidemiological information, and develop hypothesis based on the visual representations of spatial datasets.\n\n\n\n\n\n\\(Jacques\\) \\(Bertin\\), a french cartographer published a book \\(Semiologie\\) \\(Graphique\\), which brought forward concept of graphic variables. These variables highlight methods using which different information can be obtained from the data. The seven graphic variables as mentioned in \\(Semiology\\) \\(of\\) \\(graphics\\) (English version) are as under:-\n\n\n\n\nThe location of a spatial object varies with changing projections. It is important to understand that the transformation of CRS from geographic to projected, from raster to vector should be done with caution. The method of choice should be guided by the research objectives and need to preserve shape or location.\n\n\n\n\n\nThe value of a variable is related to the darkness/ lightness of a symbol. It is usually used for variables on an interval or ratio scale. Traditionally, higher values are depicted darker.\n\n\n\n\n\nColor can be considered as most complex graphic variable. It represents varied sensations to different viewers. In addition, human eye is not equally sensitive across the spectrum of colors. Further, cultural affiliations, traditional representations and understandings provide challanges. A golden rule recommended is to first use color-blind friendly palettes. In the domain of Public Health, it is important to consider the color and its depiction during visualization to avoid ambiguous communication. For example: Color choice in vaccination coverage visualization as compared to color choice in obesity.\n\n\n\n\n\nIn proportional symbol maps, the size of a symbol represents the difference in quantum of observations. The underlying type of data, classification adopted, transformations, and outlier management among others should be explored during visualizations.\n\n\n\n\n\nSimilar to size, choice of shape of the symbol can provide clarity or increase confusion, if not considered carefully. Different shapes can be provided to distinguish different types of variables. The same can be considered to distinguish sub-types of a variable. For example, dotted lines and solid lines can be used to distinguish roads from rivers but can also be used to distinguish main roads from side-roads. Understanding of datasets and variables help in deciding correct shapes for representation of data.\n\n\n\n\n\nThe arrangement of symbols can also be used to represent differences. For example: In a dot map representing dengue occurrence data, space between the dots represent disease clustering, if any. Again, the interpretation of a variable needs careful consideration, as the case occurrence may be higher just because of underlying population figures and not otherwise.\n\n\n\n\n\nUnderstanding the orientation of a pattern may provide critical insights into disease epidemiology. For eg. occurrence of a disease along with the course of a river may indicate risk factors associated with disease.\n\n\n\n\n\n\n\nThe vector or raster map is central to map visualizations.\n\n\n\n\nA short and clear title must be provided. In addition, subtitles can be added, if required.\n\n\n\n\n\nThere are two options for displaying information related to distance and scale in a map.\n1. Display of a scale bar.\n2. Use of x and y axes of the display as labeling axes.\n\n\n\n\n\nSimilar to symbolism of scale, direction can be displayed as a symbol or can be implicit in the labeled axes. Conventionally, true North is provided as a symbol in maps.\n\n\n\n\n\nNot all maps require a legend. When present, due consideration to Graphic variables should be given.\n\n\n\n\n\nThe data sources, source of geospatial files should be provided.\n\n\n\n\n\n\n\n\nworld &lt;- read_rds(here::here( \"data\",\"world_india_compliant.rds\"))\n\nst_crs(world)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\n\nindia &lt;- readRDS(here::here(\"data\", \"india_states.rds\"))\n\nnames(india) &lt;- epitrix::clean_labels(names(india))\n\nst_crs(india)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"unknown\"],\n        AREA[\"World - 85Â°S to 85Â°N\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\n## Transform to Geog CRS as that of world map\nindia &lt;- st_transform(india, crs = 4326)\n\n\n\n\n\nkerala &lt;- readRDS(here::here( \"data\",\"ker_panchayats.rds\"))\n\n## Clean names\nnames(kerala) &lt;- epitrix::clean_labels(names(kerala))\n\n## Check CRS\nst_crs(kerala)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"unknown\"],\n        AREA[\"World\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Reading RDS\ndat &lt;- readRDS(here::here( \"data\", \"dengue_sample.rds\"))\n\n## Cleaning names \ndat &lt;- dat |&gt; \n  janitor::clean_names()\n\n## Converting as sf object\ndat &lt;- st_as_sf(dat, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\n\n\n\n\nindia %&gt;%  \n  ggplot() + ## Using template\n  geom_sf() + ## Creating a Base map.\n  labs(title = \"India map\", ## Adding Title.\n       subtitle = \"For Workshops\") + ## Adding sub-title. \n  ggspatial::annotation_scale(location = \"bl\") + ## Describing Distance and Scale.\n  ggspatial::annotation_north_arrow(location = \"tr\") + ## Direction symbols.\n  xlab(\"Longitude\") + ## Adding axis labels  \n  ylab(\"Latitude\") +\n  geom_text(aes(x = 87.5, y = 10.3, label = \"Credits: AMCHSS, SCTIMST\"), ## Acknowledging Credits.\n             color = \"black\", check_overlap = T, size = 3)\n\nWarning in geom_text(aes(x = 87.5, y = 10.3, label = \"Credits: AMCHSS, SCTIMST\"), : All aesthetics have length 1, but the data has 36 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nScale on map varies by more than 10%, scale bar may be inaccurate\n\n\n\n\n\n\n\n\n\n\n\n\nAddition of labels and legends is optional in map making. In the India map, we have just used above for demonstration,\n\n\n\n\n\n#adding abbreviations\nindia$abbr &lt;- c(\"LA\",\"AP\",\n              \"AR\", \"AS\",\n              \"BR\", \"CH\",\n              \"CT\",\n              \"DDDN\", \"DL\",\n              \"GA\", \"GJ\",\n              \"HR\", \"HP\",\n              \"JH\", \"KA\",\n              \"KL\", \"MP\",\n              \"MH\",\"MN\",\n              \"ML\", \"MZ\",\n              \"NL\", \"OR\",\n              \"PY\", \"PB\",\n              \"RJ\", \"SK\",\n              \"TN\", \"TR\",\n              \"UP\", \"UT\",\n              \"WB\", \"TG\",\n              \"JK\", \"LD\", \"AN\")\n### labeling abbreviations\nindia %&gt;%  \n  ggplot() + ## Using template\n  geom_sf() + ## Creating a Base map.\n  labs(title = \"India map\", ## Adding Title.\n       subtitle = \"For Workshops\") + ## Adding sub-title. \n  ggspatial::annotation_scale(location = \"bl\") + ## Describing Distance and Scale.\n  ggspatial::annotation_north_arrow(location = \"tr\") + ## Direction symbols.\n  xlab(\"Longitude\") + ## Adding axis labels  \n  ylab(\"Latitude\") +\n  geom_text(aes(x = 87.5, y = 10.3, label = \"Credits: AMCHSS, SCTIMST\"), ## Acknowledging Credits.\n             color = \"black\", check_overlap = T, size = 3) +\n  geom_sf_label(aes(label = abbr))\n\nWarning in geom_text(aes(x = 87.5, y = 10.3, label = \"Credits: AMCHSS, SCTIMST\"), : All aesthetics have length 1, but the data has 36 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\nScale on map varies by more than 10%, scale bar may be inaccurate\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Adding centroids\n\nindia &lt;- cbind(india, st_coordinates(st_centroid(india)))\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n## plot\nindia %&gt;%  \n  ggplot() + ## Using template\n  geom_sf() + ## Creating a Base map.\n  labs(title = \"India map\", ## Adding Title.\n       subtitle = \"For Workshops\") + ## Adding sub-title. \n  ggspatial::annotation_scale(location = \"bl\") + ## Describing Distance and Scale.\n  ggspatial::annotation_north_arrow(location = \"tr\") + ## Direction symbols.\n  xlab(\"Longitude\") + ## Adding axis labels  \n  ylab(\"Latitude\") +\n  geom_text(aes(x = 87.5, y = 10.3, label = \"Credits: AMCHSS, SCTIMST\"), ## Acknowledging Credits.\n             color = \"black\", check_overlap = T, size = 3) +\n  geom_text(aes(x = X, y = Y, label = abbr),\n            color = \"black\", fontface = \"bold\", check_overlap = T, size = 2)\n\nWarning in geom_text(aes(x = 87.5, y = 10.3, label = \"Credits: AMCHSS, SCTIMST\"), : All aesthetics have length 1, but the data has 36 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nScale on map varies by more than 10%, scale bar may be inaccurate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKindly note: For the purpose of clarity on a particular aspect of the map, we will avoid placing all components of a map across the session. This is important to understand that when communicating maps/ figures, all components should be present.\n\n\n\n\nThe simplest method to visualize disease occurrence patterns is creation of dot maps. They help us understand spatial dependencies in disease occurrence. The spatial clustering of a disease is suggestive of factors present/ absent in locations with high disease occurrence as compared to locations with low disease occurrence. Thus, point maps provides visual tool for providing estimates of Spatial Clustering. Important limitation of these dot maps is the inability to visualize independent events as the number of observations increase or multiple events are observed at the same location. To overcome the same methods such as smoothing, rasterization and dot density maps is recommended.\n\n\n\n\nLet us create a dot map of dengue cases on Kerala map.\n\n## Dot map of Dengue occurrence \nggplot()+\n  geom_sf(data = kerala)+\n  geom_sf(data = dat)\n\n\n\n\n\n\n\n\nAs seen above, all the cases are seen clustered in lower part of Kerala. Hence, we will subset Kerala map and plot again.\n\ntvm &lt;- kerala %&gt;% \n  filter(district == \"Thiruvananthapuram\")\n\nggplot()+\n  geom_sf(data = tvm)+\n  geom_sf(data = dat)\n\n\n\n\n\n\n\n\nLets separate the cases according to gender distribution.\n\nggplot()+\n  geom_sf(data = tvm)+\n  geom_sf(data = dat, aes(color = sex1))+ #To change size, use size argument in aes\n  labs(title = \"Dengue occurence in Trivandrum\",\n       subtitle = \"Color according to gender.\",\n       color = \"Sex\")+\n  xlab(\"Longitude\")+\n  ylab(\"Latitude\")+\n  ggspatial::annotation_north_arrow(location = \"br\")+\n  ggspatial::annotation_scale(location = \"bl\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe term “choropleth” is derived from Greek words \\(Khoros\\) meaning “place” and \\(plethein\\) meaning “full”. In choropleth maps, aggregated value for a given area forms the basis of visual representation. The three major challanges in intrepretation of choropleth maps arise from large area polygons, Modified Unit Area Problem, and presence of skewed data. Deliberation on these issues, creation of cartograms, data transformation methods such Box-Cox transformations and multi-level analysis are recommended to overcome these limitations.\n\n\n\n\n\n## Aggregate dengue occurence based on polygon file\njoined &lt;- st_join(dat, tvm)\n\naggregated &lt;- joined %&gt;% \n  group_by(joined$block) %&gt;% \n  summarise(cases = n()) \n\nchoropleth &lt;- st_join(tvm, aggregated)\n\n## Plot choropleth map\nnames(choropleth)\n\n [1] \"join_count\"   \"area\"         \"code\"         \"panchayat\"    \"block\"       \n [6] \"municipal\"    \"corporate\"    \"district\"     \"location\"     \"stateid\"     \n[11] \"districtid\"   \"hospitalid\"   \"nameofhos\"    \"typeofhos\"    \"placename\"   \n[16] \"d\"            \"m\"            \"s\"            \"x\"            \"d1\"          \n[21] \"m1\"           \"s1\"           \"y\"            \"postoffice\"   \"hosattach\"   \n[26] \"pincode\"      \"lobodyname\"   \"lobodytype\"   \"phoneno\"      \"bedcount\"    \n[31] \"timing\"       \"ambuservi\"    \"crsavailab\"   \"crsnameadd\"   \"hostiming\"   \n[36] \"doctors\"      \"nurses\"       \"paramedics\"   \"ambientser\"   \"wastedispo\"  \n[41] \"facility\"     \"service\"      \"departname\"   \"no_of_asha\"   \"no_of_jphn\"  \n[46] \"joined$block\" \"cases\"        \"geometry\"    \n\nggplot()+\n  geom_sf(data = choropleth, aes(fill = cases))\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo provide understanding of the study location, inset maps are useful."
  },
  {
    "objectID": "spatial_data_visual.html#graphic-variables",
    "href": "spatial_data_visual.html#graphic-variables",
    "title": "Visualizing Spatial Data",
    "section": "",
    "text": "\\(Jacques\\) \\(Bertin\\), a french cartographer published a book \\(Semiologie\\) \\(Graphique\\), which brought forward concept of graphic variables. These variables highlight methods using which different information can be obtained from the data. The seven graphic variables as mentioned in \\(Semiology\\) \\(of\\) \\(graphics\\) (English version) are as under:-\n\n\n\n\nThe location of a spatial object varies with changing projections. It is important to understand that the transformation of CRS from geographic to projected, from raster to vector should be done with caution. The method of choice should be guided by the research objectives and need to preserve shape or location.\n\n\n\n\n\nThe value of a variable is related to the darkness/ lightness of a symbol. It is usually used for variables on an interval or ratio scale. Traditionally, higher values are depicted darker.\n\n\n\n\n\nColor can be considered as most complex graphic variable. It represents varied sensations to different viewers. In addition, human eye is not equally sensitive across the spectrum of colors. Further, cultural affiliations, traditional representations and understandings provide challanges. A golden rule recommended is to first use color-blind friendly palettes. In the domain of Public Health, it is important to consider the color and its depiction during visualization to avoid ambiguous communication. For example: Color choice in vaccination coverage visualization as compared to color choice in obesity.\n\n\n\n\n\nIn proportional symbol maps, the size of a symbol represents the difference in quantum of observations. The underlying type of data, classification adopted, transformations, and outlier management among others should be explored during visualizations.\n\n\n\n\n\nSimilar to size, choice of shape of the symbol can provide clarity or increase confusion, if not considered carefully. Different shapes can be provided to distinguish different types of variables. The same can be considered to distinguish sub-types of a variable. For example, dotted lines and solid lines can be used to distinguish roads from rivers but can also be used to distinguish main roads from side-roads. Understanding of datasets and variables help in deciding correct shapes for representation of data.\n\n\n\n\n\nThe arrangement of symbols can also be used to represent differences. For example: In a dot map representing dengue occurrence data, space between the dots represent disease clustering, if any. Again, the interpretation of a variable needs careful consideration, as the case occurrence may be higher just because of underlying population figures and not otherwise.\n\n\n\n\n\nUnderstanding the orientation of a pattern may provide critical insights into disease epidemiology. For eg. occurrence of a disease along with the course of a river may indicate risk factors associated with disease."
  },
  {
    "objectID": "spatial_data_visual.html#map-components",
    "href": "spatial_data_visual.html#map-components",
    "title": "Visualizing Spatial Data",
    "section": "",
    "text": "The vector or raster map is central to map visualizations.\n\n\n\n\nA short and clear title must be provided. In addition, subtitles can be added, if required.\n\n\n\n\n\nThere are two options for displaying information related to distance and scale in a map.\n1. Display of a scale bar.\n2. Use of x and y axes of the display as labeling axes.\n\n\n\n\n\nSimilar to symbolism of scale, direction can be displayed as a symbol or can be implicit in the labeled axes. Conventionally, true North is provided as a symbol in maps.\n\n\n\n\n\nNot all maps require a legend. When present, due consideration to Graphic variables should be given.\n\n\n\n\n\nThe data sources, source of geospatial files should be provided."
  },
  {
    "objectID": "spatial_data_visual.html#datasets-used",
    "href": "spatial_data_visual.html#datasets-used",
    "title": "Visualizing Spatial Data",
    "section": "",
    "text": "world &lt;- read_rds(here::here( \"data\",\"world_india_compliant.rds\"))\n\nst_crs(world)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\n\nindia &lt;- readRDS(here::here(\"data\", \"india_states.rds\"))\n\nnames(india) &lt;- epitrix::clean_labels(names(india))\n\nst_crs(india)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"unknown\"],\n        AREA[\"World - 85Â°S to 85Â°N\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\n## Transform to Geog CRS as that of world map\nindia &lt;- st_transform(india, crs = 4326)\n\n\n\n\n\nkerala &lt;- readRDS(here::here( \"data\",\"ker_panchayats.rds\"))\n\n## Clean names\nnames(kerala) &lt;- epitrix::clean_labels(names(kerala))\n\n## Check CRS\nst_crs(kerala)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"unknown\"],\n        AREA[\"World\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Reading RDS\ndat &lt;- readRDS(here::here( \"data\", \"dengue_sample.rds\"))\n\n## Cleaning names \ndat &lt;- dat |&gt; \n  janitor::clean_names()\n\n## Converting as sf object\ndat &lt;- st_as_sf(dat, coords = c(\"longitude\", \"latitude\"), crs = 4326)"
  },
  {
    "objectID": "spatial_data_visual.html#understanding-the-process-of-map-creation.",
    "href": "spatial_data_visual.html#understanding-the-process-of-map-creation.",
    "title": "Visualizing Spatial Data",
    "section": "",
    "text": "india %&gt;%  \n  ggplot() + ## Using template\n  geom_sf() + ## Creating a Base map.\n  labs(title = \"India map\", ## Adding Title.\n       subtitle = \"For Workshops\") + ## Adding sub-title. \n  ggspatial::annotation_scale(location = \"bl\") + ## Describing Distance and Scale.\n  ggspatial::annotation_north_arrow(location = \"tr\") + ## Direction symbols.\n  xlab(\"Longitude\") + ## Adding axis labels  \n  ylab(\"Latitude\") +\n  geom_text(aes(x = 87.5, y = 10.3, label = \"Credits: AMCHSS, SCTIMST\"), ## Acknowledging Credits.\n             color = \"black\", check_overlap = T, size = 3)\n\nWarning in geom_text(aes(x = 87.5, y = 10.3, label = \"Credits: AMCHSS, SCTIMST\"), : All aesthetics have length 1, but the data has 36 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nScale on map varies by more than 10%, scale bar may be inaccurate\n\n\n\n\n\n\n\n\n\n\n\n\nAddition of labels and legends is optional in map making. In the India map, we have just used above for demonstration,\n\n\n\n\n\n#adding abbreviations\nindia$abbr &lt;- c(\"LA\",\"AP\",\n              \"AR\", \"AS\",\n              \"BR\", \"CH\",\n              \"CT\",\n              \"DDDN\", \"DL\",\n              \"GA\", \"GJ\",\n              \"HR\", \"HP\",\n              \"JH\", \"KA\",\n              \"KL\", \"MP\",\n              \"MH\",\"MN\",\n              \"ML\", \"MZ\",\n              \"NL\", \"OR\",\n              \"PY\", \"PB\",\n              \"RJ\", \"SK\",\n              \"TN\", \"TR\",\n              \"UP\", \"UT\",\n              \"WB\", \"TG\",\n              \"JK\", \"LD\", \"AN\")\n### labeling abbreviations\nindia %&gt;%  \n  ggplot() + ## Using template\n  geom_sf() + ## Creating a Base map.\n  labs(title = \"India map\", ## Adding Title.\n       subtitle = \"For Workshops\") + ## Adding sub-title. \n  ggspatial::annotation_scale(location = \"bl\") + ## Describing Distance and Scale.\n  ggspatial::annotation_north_arrow(location = \"tr\") + ## Direction symbols.\n  xlab(\"Longitude\") + ## Adding axis labels  \n  ylab(\"Latitude\") +\n  geom_text(aes(x = 87.5, y = 10.3, label = \"Credits: AMCHSS, SCTIMST\"), ## Acknowledging Credits.\n             color = \"black\", check_overlap = T, size = 3) +\n  geom_sf_label(aes(label = abbr))\n\nWarning in geom_text(aes(x = 87.5, y = 10.3, label = \"Credits: AMCHSS, SCTIMST\"), : All aesthetics have length 1, but the data has 36 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\nScale on map varies by more than 10%, scale bar may be inaccurate\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Adding centroids\n\nindia &lt;- cbind(india, st_coordinates(st_centroid(india)))\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n## plot\nindia %&gt;%  \n  ggplot() + ## Using template\n  geom_sf() + ## Creating a Base map.\n  labs(title = \"India map\", ## Adding Title.\n       subtitle = \"For Workshops\") + ## Adding sub-title. \n  ggspatial::annotation_scale(location = \"bl\") + ## Describing Distance and Scale.\n  ggspatial::annotation_north_arrow(location = \"tr\") + ## Direction symbols.\n  xlab(\"Longitude\") + ## Adding axis labels  \n  ylab(\"Latitude\") +\n  geom_text(aes(x = 87.5, y = 10.3, label = \"Credits: AMCHSS, SCTIMST\"), ## Acknowledging Credits.\n             color = \"black\", check_overlap = T, size = 3) +\n  geom_text(aes(x = X, y = Y, label = abbr),\n            color = \"black\", fontface = \"bold\", check_overlap = T, size = 2)\n\nWarning in geom_text(aes(x = 87.5, y = 10.3, label = \"Credits: AMCHSS, SCTIMST\"), : All aesthetics have length 1, but the data has 36 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nScale on map varies by more than 10%, scale bar may be inaccurate"
  },
  {
    "objectID": "spatial_data_visual.html#visualisation-of-point-data.",
    "href": "spatial_data_visual.html#visualisation-of-point-data.",
    "title": "Visualizing Spatial Data",
    "section": "",
    "text": "Kindly note: For the purpose of clarity on a particular aspect of the map, we will avoid placing all components of a map across the session. This is important to understand that when communicating maps/ figures, all components should be present.\n\n\n\n\nThe simplest method to visualize disease occurrence patterns is creation of dot maps. They help us understand spatial dependencies in disease occurrence. The spatial clustering of a disease is suggestive of factors present/ absent in locations with high disease occurrence as compared to locations with low disease occurrence. Thus, point maps provides visual tool for providing estimates of Spatial Clustering. Important limitation of these dot maps is the inability to visualize independent events as the number of observations increase or multiple events are observed at the same location. To overcome the same methods such as smoothing, rasterization and dot density maps is recommended.\n\n\n\n\nLet us create a dot map of dengue cases on Kerala map.\n\n## Dot map of Dengue occurrence \nggplot()+\n  geom_sf(data = kerala)+\n  geom_sf(data = dat)\n\n\n\n\n\n\n\n\nAs seen above, all the cases are seen clustered in lower part of Kerala. Hence, we will subset Kerala map and plot again.\n\ntvm &lt;- kerala %&gt;% \n  filter(district == \"Thiruvananthapuram\")\n\nggplot()+\n  geom_sf(data = tvm)+\n  geom_sf(data = dat)\n\n\n\n\n\n\n\n\nLets separate the cases according to gender distribution.\n\nggplot()+\n  geom_sf(data = tvm)+\n  geom_sf(data = dat, aes(color = sex1))+ #To change size, use size argument in aes\n  labs(title = \"Dengue occurence in Trivandrum\",\n       subtitle = \"Color according to gender.\",\n       color = \"Sex\")+\n  xlab(\"Longitude\")+\n  ylab(\"Latitude\")+\n  ggspatial::annotation_north_arrow(location = \"br\")+\n  ggspatial::annotation_scale(location = \"bl\")"
  },
  {
    "objectID": "spatial_data_visual.html#visualisation-of-aggregate-data",
    "href": "spatial_data_visual.html#visualisation-of-aggregate-data",
    "title": "Visualizing Spatial Data",
    "section": "",
    "text": "The term “choropleth” is derived from Greek words \\(Khoros\\) meaning “place” and \\(plethein\\) meaning “full”. In choropleth maps, aggregated value for a given area forms the basis of visual representation. The three major challanges in intrepretation of choropleth maps arise from large area polygons, Modified Unit Area Problem, and presence of skewed data. Deliberation on these issues, creation of cartograms, data transformation methods such Box-Cox transformations and multi-level analysis are recommended to overcome these limitations.\n\n\n\n\n\n## Aggregate dengue occurence based on polygon file\njoined &lt;- st_join(dat, tvm)\n\naggregated &lt;- joined %&gt;% \n  group_by(joined$block) %&gt;% \n  summarise(cases = n()) \n\nchoropleth &lt;- st_join(tvm, aggregated)\n\n## Plot choropleth map\nnames(choropleth)\n\n [1] \"join_count\"   \"area\"         \"code\"         \"panchayat\"    \"block\"       \n [6] \"municipal\"    \"corporate\"    \"district\"     \"location\"     \"stateid\"     \n[11] \"districtid\"   \"hospitalid\"   \"nameofhos\"    \"typeofhos\"    \"placename\"   \n[16] \"d\"            \"m\"            \"s\"            \"x\"            \"d1\"          \n[21] \"m1\"           \"s1\"           \"y\"            \"postoffice\"   \"hosattach\"   \n[26] \"pincode\"      \"lobodyname\"   \"lobodytype\"   \"phoneno\"      \"bedcount\"    \n[31] \"timing\"       \"ambuservi\"    \"crsavailab\"   \"crsnameadd\"   \"hostiming\"   \n[36] \"doctors\"      \"nurses\"       \"paramedics\"   \"ambientser\"   \"wastedispo\"  \n[41] \"facility\"     \"service\"      \"departname\"   \"no_of_asha\"   \"no_of_jphn\"  \n[46] \"joined$block\" \"cases\"        \"geometry\"    \n\nggplot()+\n  geom_sf(data = choropleth, aes(fill = cases))"
  },
  {
    "objectID": "spatial_data_visual.html#inset-maps",
    "href": "spatial_data_visual.html#inset-maps",
    "title": "Visualizing Spatial Data",
    "section": "",
    "text": "To provide understanding of the study location, inset maps are useful."
  },
  {
    "objectID": "spatial_data_visual.html#footnotes",
    "href": "spatial_data_visual.html#footnotes",
    "title": "Visualizing Spatial Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe science and art of making maps↩︎\nThe process of map creation as primary objective but owing little to past cartographic traditions.↩︎\nMeasure of spatial clustering. Details covered in “Measures of Spatial Clustering” session↩︎"
  },
  {
    "objectID": "spatial_analysis.html",
    "href": "spatial_analysis.html",
    "title": "Spatial Analysis",
    "section": "",
    "text": "The steps involved in the clustering analysis are as follows:\n\nFiltering the malnutrition data for the stunting indicator.\nJoining the stunting data with the spatial data for India districts.\nCreating spatial weights to measure spatial relationships between districts.\nComputing Local Moran’s I statistics to identify spatial clustering patterns.\nAssigning colors to different types of clustering patterns.\nCreating cluster labels and categorizing the clusters.\nAdding the clustering information to the spatial data.\nPlotting the districts colored by the identified clustering patterns.\n\n\n\n\nlibrary(tidyverse)   # For data manipulation and visualization\nlibrary(sf)          # For handling spatial data\nlibrary(here)        # For managing file paths\nlibrary(patchwork)   # For organizing multiple plots\nlibrary(rgeoda)      # LISA Statistics\nlibrary(spdep)       # Global Moran's Statistics\n\nsf_use_s2(FALSE)\n\n\n\n\n\nmalnutrition_df &lt;- read_csv(here(\"data\", \"malnutrition_districts_nfhs.csv\"))\n\n\n\n\n\nindia_district_sf &lt;- read_rds(here(\"data\",\"india_district_sf.rds\"))\n\n\n\n\n\nmalnutrition_df |&gt; \n  glimpse()\n\nRows: 2,904\nColumns: 6\n$ state           &lt;chr&gt; \"Andaman & Nicobar Islands\", \"Andaman & Nicobar Island…\n$ district        &lt;chr&gt; \"Nicobar\", \"Nicobar\", \"Nicobar\", \"Nicobar\", \"North & M…\n$ district_unique &lt;chr&gt; \"nicobar_andaman_nicobar_islands\", \"nicobar_andaman_ni…\n$ indicator       &lt;chr&gt; \"stunting\", \"wasting\", \"underweight\", \"overweight\", \"s…\n$ nfhs4           &lt;dbl&gt; 20.3, 2.3, 10.5, 4.8, 32.5, 18.5, 33.5, 2.1, 20.1, 4.0…\n$ nfhs5           &lt;dbl&gt; 21.6, 7.8, 24.6, 1.5, 27.0, 8.3, 42.8, 0.8, 21.1, 3.5,…\n\n\n\n\n\n\nindia_district_sf |&gt; \n  glimpse()\n\nRows: 726\nColumns: 2\n$ district_unique &lt;chr&gt; \"adilabad_telangana\", \"agar_malwa_madhya_pradesh\", \"ag…\n$ geometry        &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((78.84098 19..., MULTIPOLY…\n\n\n\n\n\n\nindia_district_sf |&gt; \n  ggplot() +\n  geom_sf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us check for global clustering using spdep package\nFilter data for stunting indicator\n\nstunting_df &lt;- malnutrition_df |&gt; \n  filter(indicator == \"stunting\")\n\nJoin spatial data with stunting data\n\nstunting_sf &lt;- india_district_sf |&gt; \n  left_join(stunting_df)\n\nJoining with `by = join_by(district_unique)`\n\n\n\n\n\n\n\nnb &lt;- poly2nb(stunting_sf, queen=TRUE)\n\n\n\n\n\n\n\n\nlw &lt;- nb2listw(nb, style=\"W\", zero.policy=TRUE)\n\n\n\n\n\n\n\n\nstunting_sf &lt;- stunting_sf |&gt; \n  mutate(spatial_lag = lag.listw(lw, stunting_sf$nfhs4))\n\n\n\n\n\n\n\n\nstunting_sf |&gt; \n  ggplot(aes(x = nfhs4, y = spatial_lag)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nspdep::moran.plot(stunting_sf$nfhs4, lw)\n\n\n\n\n\n\n\n\n\n\n\n\n\nols_model &lt;- lm(spatial_lag ~ nfhs4, data = stunting_sf)\ncoef(ols_model)[2]\n\n    nfhs4 \n0.7046616 \n\n\n\n\n\n\n\n\nmoran(stunting_sf$nfhs4, lw, length(nb), Szero(lw))\n\n$I\n[1] 0.6848853\n\n$K\n[1] 2.443471\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmoran.test(stunting_sf$nfhs4, lw, alternative=\"greater\")\n\n\n    Moran I test under randomisation\n\ndata:  stunting_sf$nfhs4  \nweights: lw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 29.494, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.6811118323     -0.0013869626      0.0005354613 \n\n\n\n\n\nRun Monte Carlo simulation to test Moran’s I statistic\n\nset.seed(1234)\nmc_moran &lt;- moran.mc(stunting_sf$nfhs4, lw, nsim=9999, alternative=\"greater\")\n\n\n\n\nPlot the null distribution of Moran’s I values\n\nplot(mc_moran)\n\n\n\n\n\n\n\n\nInterpretation: The observed statistic (0.68111) is significantly higher than expected, indicating clustering.\n\n\n\nPrint the results of the Monte Carlo simulation\n\nmc_moran\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  stunting_sf$nfhs4 \nweights: lw  \nnumber of simulations + 1: 10000 \n\nstatistic = 0.68111, observed rank = 10000, p-value = 1e-04\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\n\n\nlibrary(rgeoda)  # For local spatial analysis\n\n\n\n\nCreate a spatial weights object using Queen contiguity\n\nqueen_w &lt;- queen_weights(stunting_sf)\n\n\n\n\nCompute Local Moran’s I statistics for stunting data\n\nlisa &lt;- local_moran(queen_w, stunting_sf[\"nfhs4\"])\n\nDefine color schemes for visualization based on Local Moran’s I results\n\nlisa_colors &lt;- lisa_colors(lisa)\n\nDefine labels and clusters from Local Moran’s I results\n\nlisa_labels &lt;- lisa_labels(lisa)\nlisa_clusters &lt;- lisa_clusters(lisa)\nlisa_pvals &lt;- lisa_pvalues(lisa)\nlisa_vals &lt;- lisa_values(lisa)\n\nCreate factor labels for clustering based on Local Moran’s I results\n\nlisa_labs &lt;- factor(lisa_clusters, levels = 0:(length(lisa$labels)-1), labels = lisa$labels)\n\nAdd Local Moran’s I results to the spatial data\n\nclustering_sf &lt;- stunting_sf |&gt;  bind_cols(lisa_clusters = lisa_clusters, \n                                           lisa_labs = lisa_labs, \n                                           lisa_pvals = lisa_pvals,\n                                           lisa_vals = lisa_vals)\n\n\n\n\nCategorize p-values for plotting significance\n\nclustering_sf &lt;- clustering_sf |&gt; \n  mutate(lisa_pvals_cat = case_when(\n    lisa_pvals &lt;= 0.001 ~ \"p &lt;= 0.001\",\n    lisa_pvals &lt;= 0.01 ~ \"p &lt;= 0.01\",\n    lisa_pvals &lt;= 0.05 ~ \"p &lt;= 0.05\",\n    TRUE ~ \"Not Significant\",\n    is.na(lisa_pvals) ~ NA_character_\n  ))\n\nDefine colors for p-value categories\n\npval_colors &lt;- c(\"#eeeeee\", \"#1b7837\", \"#a6dba0\", \"#d9f0d3\")\n\n\n\n\nCreate plot showing local clustering results\n\np_hs &lt;- clustering_sf  |&gt;  \n  ggplot() +\n  geom_sf(aes(fill = lisa_labs), lwd=0.2) +\n  scale_fill_manual(name = \"Clustering\", values = lisa_colors) +\n  theme_bw()\n\nCreate plot showing p-value categories\n\np_pval &lt;- clustering_sf  |&gt;  \n  ggplot() +\n  geom_sf(aes(fill = lisa_pvals_cat), lwd=0.2) +\n  scale_fill_manual(name = \"Local Moran's P Value\", values = pval_colors) +\n  theme_bw()\n\n\n\n\nCombine the two plots into one figure for comparison\n\np_stunting &lt;- (p_hs + p_pval) + \n  plot_layout(ncol = 2, guides = \"collect\") +\n  plot_annotation(title = \"Spatial Clustering of Stunting (%) among Under-5 Children in India\",\n                  caption = \"Data Source: National Family Health Survey (NFHS) \\nhttps://rchiips.org/nfhs/\",\n                  theme = theme(plot.title = element_text(hjust = 0.5, size = 20)))\n\nDisplay the combined plot\n\np_stunting\n\n\n\n\n\n\nSave the final plot to a file\n\nggsave(here(\"plots\", \"clustering_stunting_plot.png\"), width = 9, height = 6)\n\n\n\n\nCan you try it for the rest of the indicators?"
  },
  {
    "objectID": "spatial_analysis.html#spatial-clustering",
    "href": "spatial_analysis.html#spatial-clustering",
    "title": "Spatial Analysis",
    "section": "",
    "text": "The steps involved in the clustering analysis are as follows:\n\nFiltering the malnutrition data for the stunting indicator.\nJoining the stunting data with the spatial data for India districts.\nCreating spatial weights to measure spatial relationships between districts.\nComputing Local Moran’s I statistics to identify spatial clustering patterns.\nAssigning colors to different types of clustering patterns.\nCreating cluster labels and categorizing the clusters.\nAdding the clustering information to the spatial data.\nPlotting the districts colored by the identified clustering patterns.\n\n\n\n\nlibrary(tidyverse)   # For data manipulation and visualization\nlibrary(sf)          # For handling spatial data\nlibrary(here)        # For managing file paths\nlibrary(patchwork)   # For organizing multiple plots\nlibrary(rgeoda)      # LISA Statistics\nlibrary(spdep)       # Global Moran's Statistics\n\nsf_use_s2(FALSE)\n\n\n\n\n\nmalnutrition_df &lt;- read_csv(here(\"data\", \"malnutrition_districts_nfhs.csv\"))\n\n\n\n\n\nindia_district_sf &lt;- read_rds(here(\"data\",\"india_district_sf.rds\"))\n\n\n\n\n\nmalnutrition_df |&gt; \n  glimpse()\n\nRows: 2,904\nColumns: 6\n$ state           &lt;chr&gt; \"Andaman & Nicobar Islands\", \"Andaman & Nicobar Island…\n$ district        &lt;chr&gt; \"Nicobar\", \"Nicobar\", \"Nicobar\", \"Nicobar\", \"North & M…\n$ district_unique &lt;chr&gt; \"nicobar_andaman_nicobar_islands\", \"nicobar_andaman_ni…\n$ indicator       &lt;chr&gt; \"stunting\", \"wasting\", \"underweight\", \"overweight\", \"s…\n$ nfhs4           &lt;dbl&gt; 20.3, 2.3, 10.5, 4.8, 32.5, 18.5, 33.5, 2.1, 20.1, 4.0…\n$ nfhs5           &lt;dbl&gt; 21.6, 7.8, 24.6, 1.5, 27.0, 8.3, 42.8, 0.8, 21.1, 3.5,…\n\n\n\n\n\n\nindia_district_sf |&gt; \n  glimpse()\n\nRows: 726\nColumns: 2\n$ district_unique &lt;chr&gt; \"adilabad_telangana\", \"agar_malwa_madhya_pradesh\", \"ag…\n$ geometry        &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((78.84098 19..., MULTIPOLY…\n\n\n\n\n\n\nindia_district_sf |&gt; \n  ggplot() +\n  geom_sf()"
  },
  {
    "objectID": "spatial_analysis.html#clustering-of-districts-with-high-proportions-of-stunting-nfhs-4",
    "href": "spatial_analysis.html#clustering-of-districts-with-high-proportions-of-stunting-nfhs-4",
    "title": "Spatial Analysis",
    "section": "",
    "text": "Let us check for global clustering using spdep package\nFilter data for stunting indicator\n\nstunting_df &lt;- malnutrition_df |&gt; \n  filter(indicator == \"stunting\")\n\nJoin spatial data with stunting data\n\nstunting_sf &lt;- india_district_sf |&gt; \n  left_join(stunting_df)\n\nJoining with `by = join_by(district_unique)`\n\n\n\n\n\n\n\nnb &lt;- poly2nb(stunting_sf, queen=TRUE)\n\n\n\n\n\n\n\n\nlw &lt;- nb2listw(nb, style=\"W\", zero.policy=TRUE)\n\n\n\n\n\n\n\n\nstunting_sf &lt;- stunting_sf |&gt; \n  mutate(spatial_lag = lag.listw(lw, stunting_sf$nfhs4))\n\n\n\n\n\n\n\n\nstunting_sf |&gt; \n  ggplot(aes(x = nfhs4, y = spatial_lag)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nspdep::moran.plot(stunting_sf$nfhs4, lw)\n\n\n\n\n\n\n\n\n\n\n\n\n\nols_model &lt;- lm(spatial_lag ~ nfhs4, data = stunting_sf)\ncoef(ols_model)[2]\n\n    nfhs4 \n0.7046616 \n\n\n\n\n\n\n\n\nmoran(stunting_sf$nfhs4, lw, length(nb), Szero(lw))\n\n$I\n[1] 0.6848853\n\n$K\n[1] 2.443471\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmoran.test(stunting_sf$nfhs4, lw, alternative=\"greater\")\n\n\n    Moran I test under randomisation\n\ndata:  stunting_sf$nfhs4  \nweights: lw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 29.494, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.6811118323     -0.0013869626      0.0005354613 \n\n\n\n\n\nRun Monte Carlo simulation to test Moran’s I statistic\n\nset.seed(1234)\nmc_moran &lt;- moran.mc(stunting_sf$nfhs4, lw, nsim=9999, alternative=\"greater\")\n\n\n\n\nPlot the null distribution of Moran’s I values\n\nplot(mc_moran)\n\n\n\n\n\n\n\n\nInterpretation: The observed statistic (0.68111) is significantly higher than expected, indicating clustering.\n\n\n\nPrint the results of the Monte Carlo simulation\n\nmc_moran\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  stunting_sf$nfhs4 \nweights: lw  \nnumber of simulations + 1: 10000 \n\nstatistic = 0.68111, observed rank = 10000, p-value = 1e-04\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\n\n\nlibrary(rgeoda)  # For local spatial analysis\n\n\n\n\nCreate a spatial weights object using Queen contiguity\n\nqueen_w &lt;- queen_weights(stunting_sf)\n\n\n\n\nCompute Local Moran’s I statistics for stunting data\n\nlisa &lt;- local_moran(queen_w, stunting_sf[\"nfhs4\"])\n\nDefine color schemes for visualization based on Local Moran’s I results\n\nlisa_colors &lt;- lisa_colors(lisa)\n\nDefine labels and clusters from Local Moran’s I results\n\nlisa_labels &lt;- lisa_labels(lisa)\nlisa_clusters &lt;- lisa_clusters(lisa)\nlisa_pvals &lt;- lisa_pvalues(lisa)\nlisa_vals &lt;- lisa_values(lisa)\n\nCreate factor labels for clustering based on Local Moran’s I results\n\nlisa_labs &lt;- factor(lisa_clusters, levels = 0:(length(lisa$labels)-1), labels = lisa$labels)\n\nAdd Local Moran’s I results to the spatial data\n\nclustering_sf &lt;- stunting_sf |&gt;  bind_cols(lisa_clusters = lisa_clusters, \n                                           lisa_labs = lisa_labs, \n                                           lisa_pvals = lisa_pvals,\n                                           lisa_vals = lisa_vals)\n\n\n\n\nCategorize p-values for plotting significance\n\nclustering_sf &lt;- clustering_sf |&gt; \n  mutate(lisa_pvals_cat = case_when(\n    lisa_pvals &lt;= 0.001 ~ \"p &lt;= 0.001\",\n    lisa_pvals &lt;= 0.01 ~ \"p &lt;= 0.01\",\n    lisa_pvals &lt;= 0.05 ~ \"p &lt;= 0.05\",\n    TRUE ~ \"Not Significant\",\n    is.na(lisa_pvals) ~ NA_character_\n  ))\n\nDefine colors for p-value categories\n\npval_colors &lt;- c(\"#eeeeee\", \"#1b7837\", \"#a6dba0\", \"#d9f0d3\")\n\n\n\n\nCreate plot showing local clustering results\n\np_hs &lt;- clustering_sf  |&gt;  \n  ggplot() +\n  geom_sf(aes(fill = lisa_labs), lwd=0.2) +\n  scale_fill_manual(name = \"Clustering\", values = lisa_colors) +\n  theme_bw()\n\nCreate plot showing p-value categories\n\np_pval &lt;- clustering_sf  |&gt;  \n  ggplot() +\n  geom_sf(aes(fill = lisa_pvals_cat), lwd=0.2) +\n  scale_fill_manual(name = \"Local Moran's P Value\", values = pval_colors) +\n  theme_bw()\n\n\n\n\nCombine the two plots into one figure for comparison\n\np_stunting &lt;- (p_hs + p_pval) + \n  plot_layout(ncol = 2, guides = \"collect\") +\n  plot_annotation(title = \"Spatial Clustering of Stunting (%) among Under-5 Children in India\",\n                  caption = \"Data Source: National Family Health Survey (NFHS) \\nhttps://rchiips.org/nfhs/\",\n                  theme = theme(plot.title = element_text(hjust = 0.5, size = 20)))\n\nDisplay the combined plot\n\np_stunting\n\n\n\n\n\n\nSave the final plot to a file\n\nggsave(here(\"plots\", \"clustering_stunting_plot.png\"), width = 9, height = 6)\n\n\n\n\nCan you try it for the rest of the indicators?"
  },
  {
    "objectID": "intro_spatial.html",
    "href": "intro_spatial.html",
    "title": "Principles of Spatial Epidemiology",
    "section": "",
    "text": "Spatial epidemiology is the description and analysis of geographically indexed health data with respect to demographic, environmental, behavioural, socio-economic, genetic, and infectious risk factors.\nSome of the definitions of spatial epidemiology are:\nThe description of spatial patterns of disease incidence and mortality1\nSpatial Epidemiology concerns the analysis of the spatial/geographical distribution of the incidence of disease2\nHealth phenomena often involve spatial relationships among individuals and risk factors.\n\n\n\n\n\nThis section provides an introduction to geographic information systems (GIS) in R. The field of GIS in healthcare has become extremely useful in providing a fresh outlook to public health. GIS provides an opportunity to enable overlaying data with its spatial representation to support better planning and decision-making in healthcare. The convergence of many new sub-disciplines such as medical geography, public health, health informatics and data science help us better understand the similarities and differences in population health across the world. Some of the applications of GIS in healthcare include disease surveillance, environmental health, infectious diseases such as mathematical modelling and agent based modelling, and even medical imagining. While traditional uses of GIS in healthcare still are relevant, newer methods and advancing technology would be monumental for public health research.\nIt is now being realized that the healthcare industry can benefit tremendously from the potential of GIS. Innovative ways are being developed to harness the power of GIS through data integration pipelines and spatial visualization. Both the public and private sectors are adopting GIS to provide value addition across different sectors of healthcare - from public health departments and public health policy and research organizations to hospitals, medical centres, and health insurance organizations. With this background, let us dive right in!\nIt must be evident by now that geographic information systems (GIS) try to answer the question of WHERE:\n\nWhere diseases are prevalent?\nWhere do vulnerable populations live?\nWhere are we supposed to undertake public health correction measures?\nWhere are resources most needed for improving health conditions?\n\nGIS enabled systems has the potential to offer crucial insights into such questions? However, it is also important that the right questions as being asked. In order to understand better on how to ask the right questions, we may have to revisit the history of GIS in Public Health.\nEarly examples using maps as tools to better understand disease and death have been applications such as medical mapping and disease topography. From early 17th Century, maps have not simply been used to illustrate a situation but also to prove an argument. Pioneering works in this regard have helped disprove the Theory of Miasma and identified that water as the cause of disease rather than air. Soon by the 18th Century medical maps saw many applications in public health such as plague, yellow fever, and cholera.\n\n\n\nMap of the plague in the province of Bari, Naples, 1690-1692, by Filippo Arrieta. The map shows areas most affected and the boundaries of a military quarantine imposed to prevent its spread to neighboring towns and to other provinces.Source: Koch T. Mapping the miasma: air, health, and place in early medical mapping. Cartographic Perspectives. 2005 Sep 1(52):4-27.\n\n\nIn 1854, an English physician, John Snow, provided the classic example of how GIS mapping can be used in epidemiological research. He identified the water source responsible for an outbreak of cholera in London by mapping the locations of those afflicted.\n\n\n\nJohn Snow’s map, published in his On The Mode Of Communication Of Cholera, of the 1854 cholera outbreak in London is one of the best known examples of data visualization and information design.\n\n\nBy plotting the number and location of fatalities using stacks of bars on a map, Snow was able to perform a task that is now easily taken for granted: he visualized a spatial distribution. Looking at the results, the pattern on the map seems unmistakable. The map appears to support Snow’s claims that cholera is a waterborne disease and that the pump on Broad Street is the source of the outbreak.\n\n\n\nThe Pump Neighborhood\n\n\nDespite its hand-drawn, back-of-the-envelope appearance, Snow writes:\n“The inner dotted line on the map shows the various points which have been found by careful measurement to be at an equal distance by the nearest road from the pump in Broad Street and the surrounding pumps …”\nWith these words let us recreate John Snows famous map of Cholera in R.\nBut first, some best practices:\n\n\n\n\n\nGeography and Geospatial Science Working Group (GeoSWG) has recognised the need for best practices in cartography. With this purpose Centre for Disease Control (CDC, Atlanta) has come up with the Cartographic Guidelines for Public Health. Adhering to these guidelines, help the researchers develop high-quality, consistent map products to promote the mission of public health.\nSome of the important aspects of these guidelines are:\n\nMap Elements\n\nTitle and Borders\nNorth Arrow / Graticule / Scale\nInset Maps\nLabels and Legend\n\nOther Elements\n\nData Sources\nDates\nInformation about Data Processing\nProjection\n\n\n\n\n\nA illustration of the various map elements arranged according to the Cartographic Guidelines by the CDC\n\n\n\n\n\n\nProjections transform the curved, three-dimensional surface of the earth into a flat, two-dimensional plane. All map projections have distortions (distance, area, direction, and/or shape). The choice of projection depends on the intended use of the map. An equal-area map projection is a good selection for portraying geographic data distributions and is suitable for most other maps. However, if the map is attempting to show distance from patients to providers, for example, then an equidistant projection is appropriate because it preserves distance.\n\n\n\nThe three families of map projections. They can be represented by: a) cylindrical projections, b) conical projections or c) planar projections.\n\n\nWe sometimes refer to coordinate systems (or grid systems) and datums in context with map projections. With the help of coordinate reference systems (CRS) every place on the earth can be specified by a set of three numbers, called coordinates. Various coordinate systems and datums are used throughout the world. Most mapping starts with a projected map and a coordinate system overlay, which enables locational referencing. Datums, based upon different ellipsoids (idealized versions of the shape of the earth), define the origin and orientation of latitude and longitude lines. The most recently developed and widely used datum is WGS 1984. Global positioning system (GPS) data are often collected in the WGS 1984 datum.\n\n\n\nDifferent Examples of projection systems\n\n\nIt is important to note that this CRS represented in can be represented in many ways. For example:\nPROJ.4 format as +proj=longlat +datum=WGS84 +no_defs +type=crs, and in Well Known Text (WKT) format as\n\nGEOGCS[“WGS 84”, DATUM[“WGS_1984”, SPHEROID[“WGS 84”,6378137,298.257223563]], PRIMEM[“Greenwich”,0], UNIT[“degree”,0.0174532925199433, AUTHORITY[“EPSG”,“9122”]], AUTHORITY[“EPSG”,“4326”]]\n\n\n\n\n\n\n\nThere are over 60 different file formats in GIS. They can be broadly classified into vector file formats and raster file formats.\nVector graphics are comprised of vertices and paths. The three basic symbol types in vector data are points, lines and polygons. Among vector data, the commonly used file formats are the ESRI Shape file extensions (.shp, .dbf, .shx), Geographic JavaScript Object Notation (.geojson, .json) and Google Keyhole Markup Language (.kml, .kmz). Please note that all files of the ESRI Shapefiles are necessary for the file to work properly.\nOn the contrary, raster data is made up of pixels, also referred to as grid cells). They are usually square but can also be hexagons or other shapes. Rasters have pixels that are associated with a value (continuous) or class (discrete). One of the most widely used raster formats are the GeoTiff extensions (.tif, .tiff).\nWith this background, we can now begin to recreate John Snow’s map.\n\n\n\n\n\n\n\n\n\n\nThere are several packages that are available on CRAN that can perform spatial analytical tasks and operations. However, not all of them are having continued support from the original authors and are turning obsolete.\nThe figure below shows the different spatial packages on CRAN and their popularity, its is clear that the sf package has surpassed all other packages in terms of popularity and user downloads.\n\n\n\nDownloads of selected R packages for working with geographic data from early 2013 to present. The y axis shows the average number of dailly downloads from the popular cloud.r-project.org CRAN mirror with a 3-month rolling window (log scale)\n\n\nThe sf package provides simple features access for R. Simple Features is a set of standards that specify a common storage and access model of geographic feature made of mostly two-dimensional geometries (point, line, polygon, multi-point, multi-line, etc.) used by geographic information systems. It is formalized by both the Open Geospatial Consortium (OGC) and the International Organization for Standardization (ISO).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA feature is thought of as a thing, or an object in the real world, such as a building or a tree. As is the case with objects, they often consist of other objects. A set of features can form a single feature. A tree in a forest can be a feature, a forest can be a feature, a city can be a feature. An image pixel from a satellite can be a feature, or a complete MRI scan image can be a feature too.\n\n\n\n\n\n\n\nCommon types of geometry in `sf` package\n\n\n\n\n\n\n\nToday we shall be working with one of the in built datasets in R. There are other ways to import the different spatial files in R with the sf package. You can look into the documentation of the sf package for additional help (https://r-spatial.github.io/sf/). The datasets from the cholera outbreak in 1854 are available from the HistData package. The data sets used in this chapter are:\n\nSnow.deaths\nSnow.streets\nSnow.pumps\n\nThe datasets are available as dataframe objects. We need to manipulate them into sf objects as it is the best practice (and also very intuitive and easy to work with). Let us look at the head of dataset Snow.deaths to understand its structure.\n\nlibrary(tidyverse) \n\nHistData::Snow.deaths %&gt;% head()\n\n  case         x         y\n1    1 13.588010 11.095600\n2    2  9.878124 12.559180\n3    3 14.653980 10.180440\n4    4 15.220570  9.993003\n5    5 13.162650 12.963190\n6    6 13.806170  8.889046\n\n\nWe can now manipulate the spatial file into an sf object.\n\nlibrary(tidyverse)\nlibrary(sf)\n\nsnow_deaths &lt;- HistData::Snow.deaths %&gt;%\n  as_tibble() %&gt;% \n  st_as_sf(., coords = c('x', 'y'), crs = 4326)\n\nThe above code takes the dataset Snow.deaths from the HistData package and converts it into an sf object with additional arguments coords (which specifies the column representing the latitude and longitude respectively) and crs which takes the European Petroleum Survey Group (EPSG) code for the coordinate reference system (https://epsg.io). We are also creating it as a new object names snow_pumps for future reference.\nWe can check the class of the newly created object.\n\nsnow_deaths %&gt;% class()\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nLets print the object in the console and have a look.\n\nsnow_deaths\n\nSimple feature collection with 578 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 8.280715 ymin: 6.090047 xmax: 17.93893 ymax: 16.97276\nGeodetic CRS:  WGS 84\n# A tibble: 578 × 2\n    case            geometry\n * &lt;int&gt;         &lt;POINT [°]&gt;\n 1     1  (13.58801 11.0956)\n 2     2 (9.878124 12.55918)\n 3     3 (14.65398 10.18044)\n 4     4 (15.22057 9.993003)\n 5     5 (13.16265 12.96319)\n 6     6 (13.80617 8.889046)\n 7     7 (13.10214 10.56081)\n 8     8 (11.00403 11.86713)\n 9     9 (15.15475 11.70451)\n10    10 (11.12639 9.643859)\n# ℹ 568 more rows\n\n\nCongratulations! You’ve just created your first sf object.\nNow, lets plot the sf object using the ggplot2’s geom_sf wrapper.\n\nsnow_deaths %&gt;% \n  ggplot() +\n  geom_sf() \n\n\n\n\n\n\n\n\nNow, Lets load and look at the streets of Soho, London. However, the streets are represented as points which might be tricky to manipulate. However, thanks to the mighty sf package, we can perform this task in an elegant way.\n\nsnow_streets &lt;- HistData::Snow.streets %&gt;% \n  st_as_sf(., coords = c('x', 'y'), crs = 4326) %&gt;% \n  group_by(street) %&gt;% \n  summarize(n = mean(n)) %&gt;% \n  st_cast('LINESTRING')\n\nsnow_streets %&gt;% \n  ggplot() + \n  geom_sf()\n\n\n\n\n\n\n\n\nNow lets overlay the cholera deaths over the streets. We can optionally color the deaths in red and set transparency at 20% for better visualisation.\n\nggplot() +\n  geom_sf(data = snow_streets) +\n  geom_sf(data = snow_deaths, color = 'red', alpha = 0.2, size = 3)\n\n\n\n\n\n\n\n\nLets add the pumps now… Add nifty little labels using the function geom_sf_label.\n\nsnow_pumps &lt;- HistData::Snow.pumps %&gt;% \n  st_as_sf(., coords = c('x', 'y'), crs = 4326)\n\nggplot() +\n  geom_sf(data = snow_streets) +\n  geom_sf(data = snow_deaths, color = 'red', \n          alpha = 0.2, size = 3) +\n  geom_sf(data = snow_pumps , shape = 22, \n          size = 4, fill = 'blue', color = 'blue') +\n  geom_sf_label(data = snow_pumps, aes(label = label),\n                nudge_x = 0.025, nudge_y = -0.5)\n\n\n\n\n\n\n\nCongratulations on making your first map in R.\nExercise\n\nRecreate the famous John Snow Cholera map, according to the guiding principles of Good Practices in Cartography."
  },
  {
    "objectID": "intro_spatial.html#what-is-spatial-epidemiology",
    "href": "intro_spatial.html#what-is-spatial-epidemiology",
    "title": "Principles of Spatial Epidemiology",
    "section": "",
    "text": "Spatial epidemiology is the description and analysis of geographically indexed health data with respect to demographic, environmental, behavioural, socio-economic, genetic, and infectious risk factors.\nSome of the definitions of spatial epidemiology are:\nThe description of spatial patterns of disease incidence and mortality1\nSpatial Epidemiology concerns the analysis of the spatial/geographical distribution of the incidence of disease2\nHealth phenomena often involve spatial relationships among individuals and risk factors."
  },
  {
    "objectID": "intro_spatial.html#gis-in-spatial-epidemiology-and-public-health",
    "href": "intro_spatial.html#gis-in-spatial-epidemiology-and-public-health",
    "title": "Principles of Spatial Epidemiology",
    "section": "",
    "text": "This section provides an introduction to geographic information systems (GIS) in R. The field of GIS in healthcare has become extremely useful in providing a fresh outlook to public health. GIS provides an opportunity to enable overlaying data with its spatial representation to support better planning and decision-making in healthcare. The convergence of many new sub-disciplines such as medical geography, public health, health informatics and data science help us better understand the similarities and differences in population health across the world. Some of the applications of GIS in healthcare include disease surveillance, environmental health, infectious diseases such as mathematical modelling and agent based modelling, and even medical imagining. While traditional uses of GIS in healthcare still are relevant, newer methods and advancing technology would be monumental for public health research.\nIt is now being realized that the healthcare industry can benefit tremendously from the potential of GIS. Innovative ways are being developed to harness the power of GIS through data integration pipelines and spatial visualization. Both the public and private sectors are adopting GIS to provide value addition across different sectors of healthcare - from public health departments and public health policy and research organizations to hospitals, medical centres, and health insurance organizations. With this background, let us dive right in!\nIt must be evident by now that geographic information systems (GIS) try to answer the question of WHERE:\n\nWhere diseases are prevalent?\nWhere do vulnerable populations live?\nWhere are we supposed to undertake public health correction measures?\nWhere are resources most needed for improving health conditions?\n\nGIS enabled systems has the potential to offer crucial insights into such questions? However, it is also important that the right questions as being asked. In order to understand better on how to ask the right questions, we may have to revisit the history of GIS in Public Health.\nEarly examples using maps as tools to better understand disease and death have been applications such as medical mapping and disease topography. From early 17th Century, maps have not simply been used to illustrate a situation but also to prove an argument. Pioneering works in this regard have helped disprove the Theory of Miasma and identified that water as the cause of disease rather than air. Soon by the 18th Century medical maps saw many applications in public health such as plague, yellow fever, and cholera.\n\n\n\nMap of the plague in the province of Bari, Naples, 1690-1692, by Filippo Arrieta. The map shows areas most affected and the boundaries of a military quarantine imposed to prevent its spread to neighboring towns and to other provinces.Source: Koch T. Mapping the miasma: air, health, and place in early medical mapping. Cartographic Perspectives. 2005 Sep 1(52):4-27.\n\n\nIn 1854, an English physician, John Snow, provided the classic example of how GIS mapping can be used in epidemiological research. He identified the water source responsible for an outbreak of cholera in London by mapping the locations of those afflicted.\n\n\n\nJohn Snow’s map, published in his On The Mode Of Communication Of Cholera, of the 1854 cholera outbreak in London is one of the best known examples of data visualization and information design.\n\n\nBy plotting the number and location of fatalities using stacks of bars on a map, Snow was able to perform a task that is now easily taken for granted: he visualized a spatial distribution. Looking at the results, the pattern on the map seems unmistakable. The map appears to support Snow’s claims that cholera is a waterborne disease and that the pump on Broad Street is the source of the outbreak.\n\n\n\nThe Pump Neighborhood\n\n\nDespite its hand-drawn, back-of-the-envelope appearance, Snow writes:\n“The inner dotted line on the map shows the various points which have been found by careful measurement to be at an equal distance by the nearest road from the pump in Broad Street and the surrounding pumps …”\nWith these words let us recreate John Snows famous map of Cholera in R.\nBut first, some best practices:"
  },
  {
    "objectID": "intro_spatial.html#cartographic-guidelines-for-public-health",
    "href": "intro_spatial.html#cartographic-guidelines-for-public-health",
    "title": "Principles of Spatial Epidemiology",
    "section": "",
    "text": "Geography and Geospatial Science Working Group (GeoSWG) has recognised the need for best practices in cartography. With this purpose Centre for Disease Control (CDC, Atlanta) has come up with the Cartographic Guidelines for Public Health. Adhering to these guidelines, help the researchers develop high-quality, consistent map products to promote the mission of public health.\nSome of the important aspects of these guidelines are:\n\nMap Elements\n\nTitle and Borders\nNorth Arrow / Graticule / Scale\nInset Maps\nLabels and Legend\n\nOther Elements\n\nData Sources\nDates\nInformation about Data Processing\nProjection\n\n\n\n\n\nA illustration of the various map elements arranged according to the Cartographic Guidelines by the CDC\n\n\n\n\n\n\nProjections transform the curved, three-dimensional surface of the earth into a flat, two-dimensional plane. All map projections have distortions (distance, area, direction, and/or shape). The choice of projection depends on the intended use of the map. An equal-area map projection is a good selection for portraying geographic data distributions and is suitable for most other maps. However, if the map is attempting to show distance from patients to providers, for example, then an equidistant projection is appropriate because it preserves distance.\n\n\n\nThe three families of map projections. They can be represented by: a) cylindrical projections, b) conical projections or c) planar projections.\n\n\nWe sometimes refer to coordinate systems (or grid systems) and datums in context with map projections. With the help of coordinate reference systems (CRS) every place on the earth can be specified by a set of three numbers, called coordinates. Various coordinate systems and datums are used throughout the world. Most mapping starts with a projected map and a coordinate system overlay, which enables locational referencing. Datums, based upon different ellipsoids (idealized versions of the shape of the earth), define the origin and orientation of latitude and longitude lines. The most recently developed and widely used datum is WGS 1984. Global positioning system (GPS) data are often collected in the WGS 1984 datum.\n\n\n\nDifferent Examples of projection systems\n\n\nIt is important to note that this CRS represented in can be represented in many ways. For example:\nPROJ.4 format as +proj=longlat +datum=WGS84 +no_defs +type=crs, and in Well Known Text (WKT) format as\n\nGEOGCS[“WGS 84”, DATUM[“WGS_1984”, SPHEROID[“WGS 84”,6378137,298.257223563]], PRIMEM[“Greenwich”,0], UNIT[“degree”,0.0174532925199433, AUTHORITY[“EPSG”,“9122”]], AUTHORITY[“EPSG”,“4326”]]"
  },
  {
    "objectID": "intro_spatial.html#spatial-files-and-extensions",
    "href": "intro_spatial.html#spatial-files-and-extensions",
    "title": "Principles of Spatial Epidemiology",
    "section": "",
    "text": "There are over 60 different file formats in GIS. They can be broadly classified into vector file formats and raster file formats.\nVector graphics are comprised of vertices and paths. The three basic symbol types in vector data are points, lines and polygons. Among vector data, the commonly used file formats are the ESRI Shape file extensions (.shp, .dbf, .shx), Geographic JavaScript Object Notation (.geojson, .json) and Google Keyhole Markup Language (.kml, .kmz). Please note that all files of the ESRI Shapefiles are necessary for the file to work properly.\nOn the contrary, raster data is made up of pixels, also referred to as grid cells). They are usually square but can also be hexagons or other shapes. Rasters have pixels that are associated with a value (continuous) or class (discrete). One of the most widely used raster formats are the GeoTiff extensions (.tif, .tiff).\nWith this background, we can now begin to recreate John Snow’s map."
  },
  {
    "objectID": "intro_spatial.html#introducing-the-sf-package",
    "href": "intro_spatial.html#introducing-the-sf-package",
    "title": "Principles of Spatial Epidemiology",
    "section": "",
    "text": "There are several packages that are available on CRAN that can perform spatial analytical tasks and operations. However, not all of them are having continued support from the original authors and are turning obsolete.\nThe figure below shows the different spatial packages on CRAN and their popularity, its is clear that the sf package has surpassed all other packages in terms of popularity and user downloads.\n\n\n\nDownloads of selected R packages for working with geographic data from early 2013 to present. The y axis shows the average number of dailly downloads from the popular cloud.r-project.org CRAN mirror with a 3-month rolling window (log scale)\n\n\nThe sf package provides simple features access for R. Simple Features is a set of standards that specify a common storage and access model of geographic feature made of mostly two-dimensional geometries (point, line, polygon, multi-point, multi-line, etc.) used by geographic information systems. It is formalized by both the Open Geospatial Consortium (OGC) and the International Organization for Standardization (ISO).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA feature is thought of as a thing, or an object in the real world, such as a building or a tree. As is the case with objects, they often consist of other objects. A set of features can form a single feature. A tree in a forest can be a feature, a forest can be a feature, a city can be a feature. An image pixel from a satellite can be a feature, or a complete MRI scan image can be a feature too.\n\n\n\n\n\n\n\nCommon types of geometry in `sf` package"
  },
  {
    "objectID": "intro_spatial.html#the-data",
    "href": "intro_spatial.html#the-data",
    "title": "Principles of Spatial Epidemiology",
    "section": "",
    "text": "Today we shall be working with one of the in built datasets in R. There are other ways to import the different spatial files in R with the sf package. You can look into the documentation of the sf package for additional help (https://r-spatial.github.io/sf/). The datasets from the cholera outbreak in 1854 are available from the HistData package. The data sets used in this chapter are:\n\nSnow.deaths\nSnow.streets\nSnow.pumps\n\nThe datasets are available as dataframe objects. We need to manipulate them into sf objects as it is the best practice (and also very intuitive and easy to work with). Let us look at the head of dataset Snow.deaths to understand its structure.\n\nlibrary(tidyverse) \n\nHistData::Snow.deaths %&gt;% head()\n\n  case         x         y\n1    1 13.588010 11.095600\n2    2  9.878124 12.559180\n3    3 14.653980 10.180440\n4    4 15.220570  9.993003\n5    5 13.162650 12.963190\n6    6 13.806170  8.889046\n\n\nWe can now manipulate the spatial file into an sf object.\n\nlibrary(tidyverse)\nlibrary(sf)\n\nsnow_deaths &lt;- HistData::Snow.deaths %&gt;%\n  as_tibble() %&gt;% \n  st_as_sf(., coords = c('x', 'y'), crs = 4326)\n\nThe above code takes the dataset Snow.deaths from the HistData package and converts it into an sf object with additional arguments coords (which specifies the column representing the latitude and longitude respectively) and crs which takes the European Petroleum Survey Group (EPSG) code for the coordinate reference system (https://epsg.io). We are also creating it as a new object names snow_pumps for future reference.\nWe can check the class of the newly created object.\n\nsnow_deaths %&gt;% class()\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nLets print the object in the console and have a look.\n\nsnow_deaths\n\nSimple feature collection with 578 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 8.280715 ymin: 6.090047 xmax: 17.93893 ymax: 16.97276\nGeodetic CRS:  WGS 84\n# A tibble: 578 × 2\n    case            geometry\n * &lt;int&gt;         &lt;POINT [°]&gt;\n 1     1  (13.58801 11.0956)\n 2     2 (9.878124 12.55918)\n 3     3 (14.65398 10.18044)\n 4     4 (15.22057 9.993003)\n 5     5 (13.16265 12.96319)\n 6     6 (13.80617 8.889046)\n 7     7 (13.10214 10.56081)\n 8     8 (11.00403 11.86713)\n 9     9 (15.15475 11.70451)\n10    10 (11.12639 9.643859)\n# ℹ 568 more rows\n\n\nCongratulations! You’ve just created your first sf object.\nNow, lets plot the sf object using the ggplot2’s geom_sf wrapper.\n\nsnow_deaths %&gt;% \n  ggplot() +\n  geom_sf() \n\n\n\n\n\n\n\n\nNow, Lets load and look at the streets of Soho, London. However, the streets are represented as points which might be tricky to manipulate. However, thanks to the mighty sf package, we can perform this task in an elegant way.\n\nsnow_streets &lt;- HistData::Snow.streets %&gt;% \n  st_as_sf(., coords = c('x', 'y'), crs = 4326) %&gt;% \n  group_by(street) %&gt;% \n  summarize(n = mean(n)) %&gt;% \n  st_cast('LINESTRING')\n\nsnow_streets %&gt;% \n  ggplot() + \n  geom_sf()\n\n\n\n\n\n\n\n\nNow lets overlay the cholera deaths over the streets. We can optionally color the deaths in red and set transparency at 20% for better visualisation.\n\nggplot() +\n  geom_sf(data = snow_streets) +\n  geom_sf(data = snow_deaths, color = 'red', alpha = 0.2, size = 3)\n\n\n\n\n\n\n\n\nLets add the pumps now… Add nifty little labels using the function geom_sf_label.\n\nsnow_pumps &lt;- HistData::Snow.pumps %&gt;% \n  st_as_sf(., coords = c('x', 'y'), crs = 4326)\n\nggplot() +\n  geom_sf(data = snow_streets) +\n  geom_sf(data = snow_deaths, color = 'red', \n          alpha = 0.2, size = 3) +\n  geom_sf(data = snow_pumps , shape = 22, \n          size = 4, fill = 'blue', color = 'blue') +\n  geom_sf_label(data = snow_pumps, aes(label = label),\n                nudge_x = 0.025, nudge_y = -0.5)\n\n\n\n\n\n\n\nCongratulations on making your first map in R.\nExercise\n\nRecreate the famous John Snow Cholera map, according to the guiding principles of Good Practices in Cartography."
  },
  {
    "objectID": "intro_spatial.html#footnotes",
    "href": "intro_spatial.html#footnotes",
    "title": "Principles of Spatial Epidemiology",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEnglish D. 1992↩︎\nLawson, AB. 2003↩︎"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "NWDASE 2024",
    "section": "",
    "text": "Open source (free!) statistical programming language/software\nIt can be used for:\n\nWorking with data - cleaning, wrangling and transforming\nConducting analyses including advanced statistical methods\nCreating high-quality tables & figures\nCommunicate research with R Markdown\n\nIt is constantly growing!\nHas a strong online support community\nSince it’s one programming language, it is versatile enough to take you from raw data to publishable research using free, reproducible code!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio is a free, open source IDE (integrated development environment) for R. (You must install R before you can install RStudio.)\nIts interface is organized so that the user can clearly view graphs, tables, R code, and output all at the same time.\nIt also offers an Import-Wizard-like feature that allows users to import CSV, Excel, SPSS (*.sav), and Stata (*.dta) files into R without having to write the code to do so.\n\n\n\n\n\n\n\n\n\n\n\n\nExcel and SPSS are convenient for data entry, and for quickly manipulating rows and columns prior to statistical analysis. However, they are a poor choice for statistical analysis beyond the simplest descriptive statistics, or for more than a very few columns.\n\n\n\n\nProportion of articles in health decision sciences using the identified software\n\n\n\n\n\n\n\nR is becoming the “lingua franca” of data science\nMost widely used and it is rising in popularity\nR is also the tool of choice for data scientists at Microsoft, Google, Facebook, Amazon\nR’s popularity in academia is important because that creates a pool of talent that feeds industry.\nLearning the “skills of data science” is easiest in R\n\n\n\n\nIncreasing use of R in scientific research\n\n\nSome of the reasons for chosing R over others are are:\n\nMissing values are handled inconsistently, and sometimes incorrectly.\nData organisation difficult.\nAnalyses can only be done on one column at a time.\nOutput is poorly organised.\nNo record of how an analysis was accomplished.\nSome advanced analyses are impossible\n\n\n\n\n\n\n\n\nHealth Data Science is an emerging discipline, combining mathematics, statistics, epidemiology and informatics.\nR is widely used in the field of health data science and especially in healthcare industry domains like genetics, drug discovery, bioinformatics, vaccine reasearch, deep learning, epidemiology, public health, vaccine research, etc.\n\n\n\n\nApplications of Data Science in Healthcare\n\n\nAs data-generating technologies have proliferated throughout society and industry, leading hospitals are trying to ensure this data is harnessed to achieve the best outcomes for patients. These internet of things (IoT) technologies include everything from sensors that monitor patient health and the condition of machines to wearables and patients’ mobile phones. All these comprise the “Big Data” in healthcare.\n\n\n\n\n\nResearch is considered to be reproducible when the exact results can be reproduced if given access to the original data, software, or code.\n\nThe same results should be obtained under the same conditions\nIt should be possible to recreate the same conditions\n\n\nReproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results. Reproducibility is a minimum necessary condition for a finding to be believable and informative. — U.S. National Science Foundation (NSF) subcommittee on Replicability in Science\n\n\nThere are four key elements of reproducible research:\n\ndata documentation\ndata publication\ncode publication\noutput publication\n\n\n\n\n\nBaker, M. 1,500 scientists lift the lid on reproducibility. Nature 533, 452–454 (2016)\n\n\n\n\n\n\nFlavours of Reproducible Research\n\n\nFactors behind irreproducible research\n\n\nNot enough documentation on how experiment is conducted and data is generated\nData used to generate original results unavailable\nSoftware used to generate original results unavailable\nDifficult to recreate software environment (libraries, versions) used to generate original results\nDifficult to rerun the computational steps\n\n\n\n\n\nThreats to Reproducibility (Munafo. et. al, 2017)\n\n\n\nWhile reproducibility is the minimum requirement and can be solved with “good enough” computational practices, replicability/ robustness/ generalisability of scientific findings are an even greater concern involving research misconduct, questionable research practices (p-hacking, HARKing, cherry-picking), sloppy methods, and other conscious and unconscious biases.\n\nWhat are the good practices of reproducible research?\nHow to make your work reproducible?\nReproducible workflows give you credibility!\n\n\n\nCartoon created by Sidney Harris (The New Yorker)\n\n\n\n\n\nReproducibility spectrum for published research. Source: Peng, RD Reproducible Research in Computational Science Science (2011)"
  },
  {
    "objectID": "intro.html#basic-concepts",
    "href": "intro.html#basic-concepts",
    "title": "NWDASE 2024",
    "section": "",
    "text": "Open source (free!) statistical programming language/software\nIt can be used for:\n\nWorking with data - cleaning, wrangling and transforming\nConducting analyses including advanced statistical methods\nCreating high-quality tables & figures\nCommunicate research with R Markdown\n\nIt is constantly growing!\nHas a strong online support community\nSince it’s one programming language, it is versatile enough to take you from raw data to publishable research using free, reproducible code!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio is a free, open source IDE (integrated development environment) for R. (You must install R before you can install RStudio.)\nIts interface is organized so that the user can clearly view graphs, tables, R code, and output all at the same time.\nIt also offers an Import-Wizard-like feature that allows users to import CSV, Excel, SPSS (*.sav), and Stata (*.dta) files into R without having to write the code to do so.\n\n\n\n\n\n\n\n\n\n\n\n\nExcel and SPSS are convenient for data entry, and for quickly manipulating rows and columns prior to statistical analysis. However, they are a poor choice for statistical analysis beyond the simplest descriptive statistics, or for more than a very few columns.\n\n\n\n\nProportion of articles in health decision sciences using the identified software\n\n\n\n\n\n\n\nR is becoming the “lingua franca” of data science\nMost widely used and it is rising in popularity\nR is also the tool of choice for data scientists at Microsoft, Google, Facebook, Amazon\nR’s popularity in academia is important because that creates a pool of talent that feeds industry.\nLearning the “skills of data science” is easiest in R\n\n\n\n\nIncreasing use of R in scientific research\n\n\nSome of the reasons for chosing R over others are are:\n\nMissing values are handled inconsistently, and sometimes incorrectly.\nData organisation difficult.\nAnalyses can only be done on one column at a time.\nOutput is poorly organised.\nNo record of how an analysis was accomplished.\nSome advanced analyses are impossible\n\n\n\n\n\n\n\n\nHealth Data Science is an emerging discipline, combining mathematics, statistics, epidemiology and informatics.\nR is widely used in the field of health data science and especially in healthcare industry domains like genetics, drug discovery, bioinformatics, vaccine reasearch, deep learning, epidemiology, public health, vaccine research, etc.\n\n\n\n\nApplications of Data Science in Healthcare\n\n\nAs data-generating technologies have proliferated throughout society and industry, leading hospitals are trying to ensure this data is harnessed to achieve the best outcomes for patients. These internet of things (IoT) technologies include everything from sensors that monitor patient health and the condition of machines to wearables and patients’ mobile phones. All these comprise the “Big Data” in healthcare.\n\n\n\n\n\nResearch is considered to be reproducible when the exact results can be reproduced if given access to the original data, software, or code.\n\nThe same results should be obtained under the same conditions\nIt should be possible to recreate the same conditions\n\n\nReproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results. Reproducibility is a minimum necessary condition for a finding to be believable and informative. — U.S. National Science Foundation (NSF) subcommittee on Replicability in Science\n\n\nThere are four key elements of reproducible research:\n\ndata documentation\ndata publication\ncode publication\noutput publication\n\n\n\n\n\nBaker, M. 1,500 scientists lift the lid on reproducibility. Nature 533, 452–454 (2016)\n\n\n\n\n\n\nFlavours of Reproducible Research\n\n\nFactors behind irreproducible research\n\n\nNot enough documentation on how experiment is conducted and data is generated\nData used to generate original results unavailable\nSoftware used to generate original results unavailable\nDifficult to recreate software environment (libraries, versions) used to generate original results\nDifficult to rerun the computational steps\n\n\n\n\n\nThreats to Reproducibility (Munafo. et. al, 2017)\n\n\n\nWhile reproducibility is the minimum requirement and can be solved with “good enough” computational practices, replicability/ robustness/ generalisability of scientific findings are an even greater concern involving research misconduct, questionable research practices (p-hacking, HARKing, cherry-picking), sloppy methods, and other conscious and unconscious biases.\n\nWhat are the good practices of reproducible research?\nHow to make your work reproducible?\nReproducible workflows give you credibility!\n\n\n\nCartoon created by Sidney Harris (The New Yorker)\n\n\n\n\n\nReproducibility spectrum for published research. Source: Peng, RD Reproducible Research in Computational Science Science (2011)"
  },
  {
    "objectID": "fundamentals_dplyr.html",
    "href": "fundamentals_dplyr.html",
    "title": "Fundamentals of Working with Data",
    "section": "",
    "text": "In this section, we will be learning how to import, and export data from R. We will also be talking about the different file types. This section is based on the relevant chapters from two of the renowned textbooks on tidyverse.1 These textbooks take different approaches for importing and working with data in RStudio using tidyverse packages. We present to you the most optimal workflows to facilitate reproducibility and ease of understanding.\n\n\n\n\nLet us recap once again creating a project in R. It is a best practice to use create a project for each data analysis you are planning to perform. You can create a New Project using the File menu in RStudio. Let us create one now!\nReading and writing files often involves the use of file paths. A file path is a string of characters that point R and RStudio to the location of the file on your computer.\nThese file paths can be a complete location2 or just the file name.3 If you pass R a partial file path, R will append it to the end of the file path that leads to your working directory. The working directory is the directory where your .Rproj file is.\nRun here::here() to see the file path that leads to your current working directory.\n\n\n\n\n\nThe RStudio IDE provides an Import Dataset button in the Environment pane, which appears in the top right corner of the IDE by default. You can use this button to import data that is stored in plain text files as well as in Excel, SAS, SPSS, and Stata files.\n\n\n\n\n\nWe recommend using .csv file type to read and write your data as a best practice. This will ensure cross compatibility between various programs as it is just a raw text file but just separated by a comma.\nNote\nThere are different packages to import different types of data.\n\nhaven : SPSS, Stata, or SAS\nreadxl : Excel spreadsheets\nreadr : csv, txt, tsv etc.\n\nHowever, we recommend using the rio package.\n\n\n\n\n\nThere are two main functions in the rio package, import() and export(). The import() function takes the file path as an input argument, while the export() function takes the object and destination file path as arguments.\nThe import() function has an additional argument setclass which needs to be set to tibble to import the data as a tibble (the workhorse of tidyverse workflows).\n\n\n\n\n\nThere are a number of differences between tibbles and data.frames. To see a full vignette about tibbles and how they differ from data.frame, please run vignette(\"tibble\") in the console and read through that vignette.\nSome major differences are:\n\nInput type remains unchanged - data.frame changes strings as factors; tibble will not\nVariable names remain unchanged - data.frame will remove spaces or add “x” before numeric column names. tibble will not.\nThere are no row.names() for a tibble\ntibble print first ten rows and columns that fit on one screen\n\n\n\n\nImport the provided .csv file into the RStudio environment using the method mentioned above.\nCan you see anything apprearing on the RStudio Console pane once you have imported the file?\nCan you see anything apprearing on the RStudio Environment pane once you have imported the file?\nImport the provided .xls file into the RStudio environment using the method mentioned above.\nChange the sheet to import as well as skip the first row. See the changes happen in the R code syntax in the bottom right pane of the GUI\n\n\n\n\n\n\n\n.rds is a file format native to R for saving compressed content. .rds files are not text files and are not human readable in their raw form. Each .rds file contains a single object, which makes it easy to assign its output directly to a single R object. This is not necessarily the case for .RData files, which makes .rds files safer to use.\nUse the write_rds() function from the readr package to write an .rds file. Save the previously loaded data, as an .rds file using this function. You can look at the help menu to know more on the syntax or you can type ?write_rds in the Console pane.\nNow you can open your file explorer go to your working directory and check if the file has been saved. Similarly, you can use the write_csv() function from the readr package to write a .csv file.\n\n\n\n\n\n\n\n\nTidy data is a way to describe data that’s organized with a particular structure – a rectangular structure, where each variable has its own column, and each observation has its own row. — Hadley Wickham, 2014\n\n\n\n\n\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nThese three rules are interrelated because it’s impossible to only satisfy two of the three.\n\n\n\n\n\n\n\n\n\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. - Hadley Wickham\n\n\n\n\nSource: R for Data Science (http://r4ds.had.co.nz/)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with messy data can be messy!. You need to build custom tools from scratch each time you work with a new dataset.\nIllustrations from : https://github.com/allisonhorst/stats-illustrations\n\n\n\n\n\nPackages like tidyr and dplyr can enable you to get on with analysing your data and start answering key questions rather than spending time in trying to clean the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\nTidy data allows you to be more efficient by using specialised tools built for the tidy workflow. There are a lot of tools specifically built to wrangle untidy data into tidy data.\n\n\n\n\n\nOne other advantage of working with Tidy data is that it makes it easier for collaboration, as your colleagues can use the same familiar tools rather than getting overwhelmed with all the work you did from scratch. It is also helpful for your future self as it becomes a consistent workflow and takes less adjustment time for any incremental changes.\nTidy data also makes it easier to reproduce analyses because they are easier to understand, update, and reuse. By using tools together that all expect tidy data as inputs, you can build and iterate really powerful workflows.\n\n\n\n\n\n\nWhen loading data into R using the RStudio GUI using tidyverse, the data is automatically saved as a tibble. A tibble is a data frame, but they have some new functionalities and properties to make our life easier. It is the single most important workhorse of tidyverse.\n\n\n\ntibble() vs data.frame()\n\n\nYou can change data.frame objects to a tibble using the as_tibble() function.\n\n\n\n\n\nNow that you have imported data into RStudio its a good practice to have a look at the data. There are many ways you can do it within RStudio.\n\nThrough the Environment pane\nView() function\nSimply typing the name of the dataset in the Console\n\nSome other things you can do to have a look at your data are:\n\nChecking the class of the dataset using class() function\nChecking the structure of the dataset using str() function\n\nNote\nclass() and str() are not just limited to datasets, they can be used for any R objects.\n\nSome additional tips for quickly looking at your data:\n\nhead()\ntail()\nglimpse()\n\n\n\n\n\n\nType the name of the dataset in the console and see what happens?\nHow many rows and columns can you visualize?\nNow, try the head(), tail(), and glimpse() functions\nTry to create a tibble manually in RStudio with a numeric, character, and factor variable. (Hint: vignette(‘tibble’) )"
  },
  {
    "objectID": "fundamentals_dplyr.html#before-we-get-started",
    "href": "fundamentals_dplyr.html#before-we-get-started",
    "title": "Fundamentals of Working with Data",
    "section": "",
    "text": "Let us recap once again creating a project in R. It is a best practice to use create a project for each data analysis you are planning to perform. You can create a New Project using the File menu in RStudio. Let us create one now!\nReading and writing files often involves the use of file paths. A file path is a string of characters that point R and RStudio to the location of the file on your computer.\nThese file paths can be a complete location2 or just the file name.3 If you pass R a partial file path, R will append it to the end of the file path that leads to your working directory. The working directory is the directory where your .Rproj file is.\nRun here::here() to see the file path that leads to your current working directory."
  },
  {
    "objectID": "fundamentals_dplyr.html#importing-data-using-the-rstudio-gui",
    "href": "fundamentals_dplyr.html#importing-data-using-the-rstudio-gui",
    "title": "Fundamentals of Working with Data",
    "section": "",
    "text": "The RStudio IDE provides an Import Dataset button in the Environment pane, which appears in the top right corner of the IDE by default. You can use this button to import data that is stored in plain text files as well as in Excel, SAS, SPSS, and Stata files.\n\n\n\n\n\nWe recommend using .csv file type to read and write your data as a best practice. This will ensure cross compatibility between various programs as it is just a raw text file but just separated by a comma.\nNote\nThere are different packages to import different types of data.\n\nhaven : SPSS, Stata, or SAS\nreadxl : Excel spreadsheets\nreadr : csv, txt, tsv etc.\n\nHowever, we recommend using the rio package.\n\n\n\n\n\nThere are two main functions in the rio package, import() and export(). The import() function takes the file path as an input argument, while the export() function takes the object and destination file path as arguments.\nThe import() function has an additional argument setclass which needs to be set to tibble to import the data as a tibble (the workhorse of tidyverse workflows).\n\n\n\n\n\nThere are a number of differences between tibbles and data.frames. To see a full vignette about tibbles and how they differ from data.frame, please run vignette(\"tibble\") in the console and read through that vignette.\nSome major differences are:\n\nInput type remains unchanged - data.frame changes strings as factors; tibble will not\nVariable names remain unchanged - data.frame will remove spaces or add “x” before numeric column names. tibble will not.\nThere are no row.names() for a tibble\ntibble print first ten rows and columns that fit on one screen\n\n\n\n\nImport the provided .csv file into the RStudio environment using the method mentioned above.\nCan you see anything apprearing on the RStudio Console pane once you have imported the file?\nCan you see anything apprearing on the RStudio Environment pane once you have imported the file?\nImport the provided .xls file into the RStudio environment using the method mentioned above.\nChange the sheet to import as well as skip the first row. See the changes happen in the R code syntax in the bottom right pane of the GUI"
  },
  {
    "objectID": "fundamentals_dplyr.html#saving-and-loading-a-compressed-.rds-file",
    "href": "fundamentals_dplyr.html#saving-and-loading-a-compressed-.rds-file",
    "title": "Fundamentals of Working with Data",
    "section": "",
    "text": ".rds is a file format native to R for saving compressed content. .rds files are not text files and are not human readable in their raw form. Each .rds file contains a single object, which makes it easy to assign its output directly to a single R object. This is not necessarily the case for .RData files, which makes .rds files safer to use.\nUse the write_rds() function from the readr package to write an .rds file. Save the previously loaded data, as an .rds file using this function. You can look at the help menu to know more on the syntax or you can type ?write_rds in the Console pane.\nNow you can open your file explorer go to your working directory and check if the file has been saved. Similarly, you can use the write_csv() function from the readr package to write a .csv file."
  },
  {
    "objectID": "fundamentals_dplyr.html#principles-of-tidy-data",
    "href": "fundamentals_dplyr.html#principles-of-tidy-data",
    "title": "Fundamentals of Working with Data",
    "section": "",
    "text": "Tidy data is a way to describe data that’s organized with a particular structure – a rectangular structure, where each variable has its own column, and each observation has its own row. — Hadley Wickham, 2014\n\n\n\n\n\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nThese three rules are interrelated because it’s impossible to only satisfy two of the three.\n\n\n\n\n\n\n\n\n\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. - Hadley Wickham\n\n\n\n\nSource: R for Data Science (http://r4ds.had.co.nz/)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with messy data can be messy!. You need to build custom tools from scratch each time you work with a new dataset.\nIllustrations from : https://github.com/allisonhorst/stats-illustrations\n\n\n\n\n\nPackages like tidyr and dplyr can enable you to get on with analysing your data and start answering key questions rather than spending time in trying to clean the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\nTidy data allows you to be more efficient by using specialised tools built for the tidy workflow. There are a lot of tools specifically built to wrangle untidy data into tidy data.\n\n\n\n\n\nOne other advantage of working with Tidy data is that it makes it easier for collaboration, as your colleagues can use the same familiar tools rather than getting overwhelmed with all the work you did from scratch. It is also helpful for your future self as it becomes a consistent workflow and takes less adjustment time for any incremental changes.\nTidy data also makes it easier to reproduce analyses because they are easier to understand, update, and reuse. By using tools together that all expect tidy data as inputs, you can build and iterate really powerful workflows."
  },
  {
    "objectID": "fundamentals_dplyr.html#a-word-on-tibble",
    "href": "fundamentals_dplyr.html#a-word-on-tibble",
    "title": "Fundamentals of Working with Data",
    "section": "",
    "text": "When loading data into R using the RStudio GUI using tidyverse, the data is automatically saved as a tibble. A tibble is a data frame, but they have some new functionalities and properties to make our life easier. It is the single most important workhorse of tidyverse.\n\n\n\ntibble() vs data.frame()\n\n\nYou can change data.frame objects to a tibble using the as_tibble() function."
  },
  {
    "objectID": "fundamentals_dplyr.html#working-with-tibbles",
    "href": "fundamentals_dplyr.html#working-with-tibbles",
    "title": "Fundamentals of Working with Data",
    "section": "",
    "text": "Now that you have imported data into RStudio its a good practice to have a look at the data. There are many ways you can do it within RStudio.\n\nThrough the Environment pane\nView() function\nSimply typing the name of the dataset in the Console\n\nSome other things you can do to have a look at your data are:\n\nChecking the class of the dataset using class() function\nChecking the structure of the dataset using str() function\n\nNote\nclass() and str() are not just limited to datasets, they can be used for any R objects.\n\nSome additional tips for quickly looking at your data:\n\nhead()\ntail()\nglimpse()\n\n\n\n\n\n\nType the name of the dataset in the console and see what happens?\nHow many rows and columns can you visualize?\nNow, try the head(), tail(), and glimpse() functions\nTry to create a tibble manually in RStudio with a numeric, character, and factor variable. (Hint: vignette(‘tibble’) )"
  },
  {
    "objectID": "fundamentals_dplyr.html#footnotes",
    "href": "fundamentals_dplyr.html#footnotes",
    "title": "Fundamentals of Working with Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTidyverse Skills for Data Science and The Tidyverse Cookbook↩︎\nEg: C:/Users/Arun/RIntro_Book.Rmd↩︎\nEg: RIntro_Book.Rmd↩︎"
  },
  {
    "objectID": "day_two.html",
    "href": "day_two.html",
    "title": "NWDASE 2024",
    "section": "",
    "text": "The morning of Day One starts with an introduction and icebreaker session, setting the stage for the workshop by covering key concepts in data science and spatial epidemiology using R. This is followed by a session to familiarize participants with the R programming environment and RStudio, including installation, interface layout, and key features. Participants will then engage in hands-on exercises to work with basic data types such as vectors, matrices, and data frames.\nThe afternoon session will focus on the fundamentals of working with data in R, including managing file paths, working directories, and importing data. Participants will also be introduced to the concept of tidy data and engage in practical exercises on data wrangling using the dplyr package. The day concludes with an introduction to the Quarto framework through a group activity."
  },
  {
    "objectID": "day_two.html#overview-day-two",
    "href": "day_two.html#overview-day-two",
    "title": "NWDASE 2024",
    "section": "",
    "text": "The morning of Day One starts with an introduction and icebreaker session, setting the stage for the workshop by covering key concepts in data science and spatial epidemiology using R. This is followed by a session to familiarize participants with the R programming environment and RStudio, including installation, interface layout, and key features. Participants will then engage in hands-on exercises to work with basic data types such as vectors, matrices, and data frames.\nThe afternoon session will focus on the fundamentals of working with data in R, including managing file paths, working directories, and importing data. Participants will also be introduced to the concept of tidy data and engage in practical exercises on data wrangling using the dplyr package. The day concludes with an introduction to the Quarto framework through a group activity."
  },
  {
    "objectID": "day_two.html#topics-covered-today",
    "href": "day_two.html#topics-covered-today",
    "title": "NWDASE 2024",
    "section": "Topics Covered Today",
    "text": "Topics Covered Today\n\nData Visualization using R\nIntroduction to Spatial Epidemiology\nSpatial Data Manipulation\nSpatial Data Visualisation\nPublication Ready Tables\nCommunicating Research with Quarto"
  },
  {
    "objectID": "day_one.html",
    "href": "day_one.html",
    "title": "NWDASE 2024",
    "section": "",
    "text": "The morning of Day One starts with an introduction and icebreaker session, setting the stage for the workshop by covering key concepts in data science and spatial epidemiology using R. This is followed by a session to familiarize participants with the R programming environment and RStudio, including installation, interface layout, and key features. Participants will then engage in hands-on exercises to work with basic data types such as vectors, matrices, and data frames.\nThe afternoon session will focus on the fundamentals of working with data in R, including managing file paths, working directories, and importing data. Participants will also be introduced to the concept of tidy data and engage in practical exercises on data wrangling using the dplyr package. The day concludes with an introduction to the Quarto framework through a group activity."
  },
  {
    "objectID": "day_one.html#overview-day-one",
    "href": "day_one.html#overview-day-one",
    "title": "NWDASE 2024",
    "section": "",
    "text": "The morning of Day One starts with an introduction and icebreaker session, setting the stage for the workshop by covering key concepts in data science and spatial epidemiology using R. This is followed by a session to familiarize participants with the R programming environment and RStudio, including installation, interface layout, and key features. Participants will then engage in hands-on exercises to work with basic data types such as vectors, matrices, and data frames.\nThe afternoon session will focus on the fundamentals of working with data in R, including managing file paths, working directories, and importing data. Participants will also be introduced to the concept of tidy data and engage in practical exercises on data wrangling using the dplyr package. The day concludes with an introduction to the Quarto framework through a group activity."
  },
  {
    "objectID": "day_one.html#topics-covered-today",
    "href": "day_one.html#topics-covered-today",
    "title": "NWDASE 2024",
    "section": "Topics Covered Today",
    "text": "Topics Covered Today\n\nIntroduction: Basic Concepts\nGetting comfortable with R and RStudio\nFundamentals of Working with Data\nExploring Data with R\nCommunicating Research with Quarto"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "NWDASE 2024",
    "section": "",
    "text": "We acknowledge the contributions of Dr Arun Mitra Peddireddy and Dr Gurpreet Singh, who developed the concepts and design of these RIntro workshop series for health professionals in 2021 at SCTIMST with Prof. Biju Soman. Since then, we have conducted over ten training programs, incrementally modifying the content and design. The philosophy of this workshop series is hand-holding and collaboration so that more health professionals can come into data science and reproducible research using the open philosophy. The profiles of the current resource persons for the workshop are given here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDr. Biju Soman is the Professor and Head of Achutha Menon Centre for Health Science Studies, Sree Chitra Tirunal Institute for Medical Sciences and Technology (SCTIMST), Trivandrum, Kerala. Dr. Soman had completed his MBBS in 1992, DPH in 1996 and M.D. (Community Medicine) in 1999 from Government Medical College Trivandrum (University of Kerala) and his MSc (Control of Infectious Diseases) in 2011 from the University of London and DLSHTM in 2012 from the London School of Hygiene and Tropical Medicine (U.K.). He has undergone advanced training in health informatics from Oslo University (Norway), Teaching Methodology from Boston School of Public Health (USA), Multilevel Modelling from Bielefeld University (Germany), and Advances in Geographic Information Systems (GIS) from National Remote Sensing Centre (Hyderabad). Dr. Soman teaches Public Health Technologies, Infectious Disease Epidemiology, and Database Management in Epidemiology modules at AMCHSS as part of the PhD and MPH curriculum. He has contributed chapters to a few books and has many peer-reviewed publications. Some of his professional involvements at SCTIMST include Chairperson, Board of Studies (Health Sciences); Member, Standing Academic Committee; Executive Member, Health Action by People (NGO); Member, Research Advisory Committee, Centre for Environment Development, Trivandrum; and Executive Member, Centre for Advancement of Global Health (NGO). He also heads the Regional Technical Resource Centre (RTRC) of the Health Technology Assessment India (HTAIn) at SCTIMST. His current research and teaching are around data science in public health, including public health informatics, geospatial analysis, telemedicine, health technology assessment, and infectious disease epidemiology. He endorses open data initiative, FOSS resources, and reproducible research initiatives.\n\nURL: www.sctimst.ac.in/people/bijusoman\nEmail: bijusoman@sctimst.ac.in, bijusoman@gmail.com\nLinkedin: https://www.linkedin.com/in/drbijusoman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArun Jose is a researcher currently immersed in the pursuit of a Ph.D. in Health Sciences at the Achutha Menon Centre for Health Science Studies, SCTIMST, Trivandrum. With a profound academic foundation, he holds a Master of Science degree in Demography and Biostatistics from the University of Kerala.\nHis current focus lies in the domain of projecting population dynamics at granular levels, employing diverse modeling techniques to unravel insights crucial for informed decision-making.\n\nEmail: arunjose@sctimst.ac.in\nLinkedin: https://www.linkedin.com/in/arun-jose-20808a183/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDr. Rehna C Mohamed is the Jr Health Economist at the Health Technology Assessment, Regional Resource Centre, at Achutha Menon Centre for Health Science Studies, SCTIMST. She holds a Bachelor of Dental Surgery (BDS) degree from Government Dental College, Kozhikode, and a Master of Public Health (MPH) from Amrita Institute of Medical Sciences and Research Centre. Her current research area focuses on understanding equity in general and in Health Technology Assessment (HTA) in particular. She has previously worked as Scientist- B, at the HTA Regional Resource Hub, NIRRCH, Mumbai, contributing to research and policy recommendations. Additionally, she has experience working with the Centre for Health Research and Innovation, a PATH affiliate, as a Latent TB Infection Coordinator, in Ernakulam district where she coordinated efforts in researching on latent TB infections.\n\nEmail: drrehnacmohamed@gmail.com\nLinkedin: www.linkedin.com/in/dr-rehna-mohamed-b6793615b\n\n\n\n\n\nCopyright © 2024 by the authors\nAll rights reserved. No part of this publication may be reproduced in any form by any electronic or mechanical means (including photocopying, recording or information storage and retrieval) without permission in writing from the individual authors. Some of the artwork and illustrations used in this document were taken from the work of Allison Horst (https://github.com/allisonhorst/stats-illustrations), and the authors do not claim ownership of these."
  },
  {
    "objectID": "about.html#about-us",
    "href": "about.html#about-us",
    "title": "NWDASE 2024",
    "section": "",
    "text": "We acknowledge the contributions of Dr Arun Mitra Peddireddy and Dr Gurpreet Singh, who developed the concepts and design of these RIntro workshop series for health professionals in 2021 at SCTIMST with Prof. Biju Soman. Since then, we have conducted over ten training programs, incrementally modifying the content and design. The philosophy of this workshop series is hand-holding and collaboration so that more health professionals can come into data science and reproducible research using the open philosophy. The profiles of the current resource persons for the workshop are given here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDr. Biju Soman is the Professor and Head of Achutha Menon Centre for Health Science Studies, Sree Chitra Tirunal Institute for Medical Sciences and Technology (SCTIMST), Trivandrum, Kerala. Dr. Soman had completed his MBBS in 1992, DPH in 1996 and M.D. (Community Medicine) in 1999 from Government Medical College Trivandrum (University of Kerala) and his MSc (Control of Infectious Diseases) in 2011 from the University of London and DLSHTM in 2012 from the London School of Hygiene and Tropical Medicine (U.K.). He has undergone advanced training in health informatics from Oslo University (Norway), Teaching Methodology from Boston School of Public Health (USA), Multilevel Modelling from Bielefeld University (Germany), and Advances in Geographic Information Systems (GIS) from National Remote Sensing Centre (Hyderabad). Dr. Soman teaches Public Health Technologies, Infectious Disease Epidemiology, and Database Management in Epidemiology modules at AMCHSS as part of the PhD and MPH curriculum. He has contributed chapters to a few books and has many peer-reviewed publications. Some of his professional involvements at SCTIMST include Chairperson, Board of Studies (Health Sciences); Member, Standing Academic Committee; Executive Member, Health Action by People (NGO); Member, Research Advisory Committee, Centre for Environment Development, Trivandrum; and Executive Member, Centre for Advancement of Global Health (NGO). He also heads the Regional Technical Resource Centre (RTRC) of the Health Technology Assessment India (HTAIn) at SCTIMST. His current research and teaching are around data science in public health, including public health informatics, geospatial analysis, telemedicine, health technology assessment, and infectious disease epidemiology. He endorses open data initiative, FOSS resources, and reproducible research initiatives.\n\nURL: www.sctimst.ac.in/people/bijusoman\nEmail: bijusoman@sctimst.ac.in, bijusoman@gmail.com\nLinkedin: https://www.linkedin.com/in/drbijusoman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArun Jose is a researcher currently immersed in the pursuit of a Ph.D. in Health Sciences at the Achutha Menon Centre for Health Science Studies, SCTIMST, Trivandrum. With a profound academic foundation, he holds a Master of Science degree in Demography and Biostatistics from the University of Kerala.\nHis current focus lies in the domain of projecting population dynamics at granular levels, employing diverse modeling techniques to unravel insights crucial for informed decision-making.\n\nEmail: arunjose@sctimst.ac.in\nLinkedin: https://www.linkedin.com/in/arun-jose-20808a183/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDr. Rehna C Mohamed is the Jr Health Economist at the Health Technology Assessment, Regional Resource Centre, at Achutha Menon Centre for Health Science Studies, SCTIMST. She holds a Bachelor of Dental Surgery (BDS) degree from Government Dental College, Kozhikode, and a Master of Public Health (MPH) from Amrita Institute of Medical Sciences and Research Centre. Her current research area focuses on understanding equity in general and in Health Technology Assessment (HTA) in particular. She has previously worked as Scientist- B, at the HTA Regional Resource Hub, NIRRCH, Mumbai, contributing to research and policy recommendations. Additionally, she has experience working with the Centre for Health Research and Innovation, a PATH affiliate, as a Latent TB Infection Coordinator, in Ernakulam district where she coordinated efforts in researching on latent TB infections.\n\nEmail: drrehnacmohamed@gmail.com\nLinkedin: www.linkedin.com/in/dr-rehna-mohamed-b6793615b\n\n\n\n\n\nCopyright © 2024 by the authors\nAll rights reserved. No part of this publication may be reproduced in any form by any electronic or mechanical means (including photocopying, recording or information storage and retrieval) without permission in writing from the individual authors. Some of the artwork and illustrations used in this document were taken from the work of Allison Horst (https://github.com/allisonhorst/stats-illustrations), and the authors do not claim ownership of these."
  },
  {
    "objectID": "data_visual.html",
    "href": "data_visual.html",
    "title": "Data Visualization",
    "section": "",
    "text": "The present session will focus on basics of data visualization.\n\n\n\n\nBefore we start with learning how to create graphs/ plots/ figures using R, it is important to understand that the selection of type of plots is dependent entirely upon the number and type of variables selected. Though we are not covering the bio statistics module, it is strongly recommended that the same be refreshed and strengthened by participants from time to time for advanced use of data analytics in future.\n\n\n\n\n\nData visualization is a powerful tool for data science in epidmeiology. The basics of programming for creating plots using ggplot package includes understanding of four important aspects.\n\nUse of reusable/ reproducible templates.\nCreation of different types of plots using geoms.\nAddition of variables using mapping.\nCustomization of plots using settings.\n\nFor illustration of concepts, we will be using birthwt data set from MASS package which includes data on Risk Factors Associated with Low Infant Birth Weight in the present session.\n\n\n\n\n\n\nbirthwt dataset from the MASS package\n\n\nFor illustrative examples in the sections on data visualization, data analysis, and summary tables, Risk Factors Associated with Low Infant Birth Weight dataset (birthwt) dataset has been used. It is an inbuilt dataset from the package MASS. To load the dataset, type the following R code\n\ndf &lt;- MASS::birthwt\n\nSince, it has been labelled as df, it has been reffered as df from here on. The dataset used has 189 rows and 10 columns. The data were collected at Baystate Medical Center, Springfield, Mass during 1986. The variables present in the data includes the following variables/ columns:-\n\nlow : indicator of birth weight less than 2.5 kg\nage: mother’s age in years\nlwt: mother’s weight in pounds at last menstrual period\nrace : mother’s race (1 = white, 2 = black, 3 = other)\nsmoke : smoking status during pregnancy\nptl : number of previous premature labours\nht : history of hypertension\nui : presence of uterine irritability\nftv : number of physician visits during the first trimester\nbwt : birth weight in grams\n\n Hosmer, D.W. and Lemeshow, S. (1989) Applied Logistic Regression. New York: Wiley\n Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer.\n\n\n\n\n\n# install.packages(\"tidyverse\")  includes ggplot\n# install.packages(\"MASS\") includes multiple data sets for learning purposes\noptions(tidyverse.quiet = TRUE)\nlibrary(tidyverse)\ndf &lt;- MASS::birthwt\n\nData cleaning\n\ndf &lt;- df |&gt; \n  mutate(smoke = factor(smoke,\n                        levels = c(0,1),\n                        labels = c(\"Non Smoker\",\n                                   \"Smoker\"))) |&gt; \n  mutate(race = factor(race,\n                       levels = c(1,2,3),\n                       labels = c(\"White\",\n                                  \"Black\",\n                                  \"Other\"))) |&gt; \n  mutate(low = factor(low,\n                      levels =  c(0,1),\n                      labels = c(\"Normal\", \n                                 \"Low Birth Weight\")))\n\n\n\n\n\n\n\nFor illustration, we will plot birth weights to understand the distribution pattern among all study participants in birth weight data set.\n\n\n\n\ngeom_histogram visualize the distribution of a single continuous variable by dividing the x axis into bins and counting the number of observations in each bin. Histograms geom_histogram() display the counts with bars\n\nggplot(data = df, aes(x = bwt)) + # Template\n  geom_histogram( # geom\n    mapping = aes(y = after_stat(density)), # mapping\n    binwidth = 350, # mandatory settings\n    color = \"blue\", # optional settings\n    fill = \"red\",\n    linetype = 1,\n    alpha = 0.5,\n    size = 1) +\n  geom_density(color = 'darkgreen', linetype = 1, linewidth = 2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nTips\n\nColor: Determines color of the lines of histogram\nFill: Fills the histogram with specified color\nLinetype: try numbers from 0 to 6\nAlpha: Used for transparency adjustments. Varies from 0 (transparent) to 1 (Opaque)\nSize: Determines thicknes of the lines\n\n\n\n\n\n\ngeom_freqpoly() display the counts with lines. Further, frequency polygons are more suitable when you want to compare the distribution across the levels of a categorical variable (we shall see later!).\n\nggplot(data = df) +\n  geom_freqpoly(mapping = aes(x = bwt),\n                binwidth = 350) # Alternative use of bins\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn a dot plot, the width of a dot corresponds to the bin width, and dots are stacked, with each dot representing one observation.\n\nggplot(data = df) +\n  geom_dotplot(mapping = aes(x = bwt),\n               binwidth = 150) \n\n\n\n\n\n\n\n\n\n\n\n\n\nThe boxplot compactly displays the distribution of a continuous variable. It visualizes five summary statistics (the median, two hinges and two whiskers), and all “outlying” points individually.\n\nggplot(data = df) +\n  geom_boxplot(mapping = aes(y = bwt),\n               coef = 1.5) \n\n\n\n\n\n\n\n\nWhat happens if we change the coef argument to 3?\n\n\n\n\n\ngeom_density computes and draws kernel density estimate, which is a smoothed version of the histogram. This is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\n\nggplot(data = df) + \n  geom_density(mapping = aes(x = bwt))\n\n\n\n\n\n\n\n\nTip\nMultiple plots can be combined for enhanced visualization. For example, we can combine histogram and frequency polygon.\n\n##Create a histogram and save it as an object\nhistogram &lt;- ggplot(data = df) + \n  geom_histogram( \n    mapping = aes(x = bwt), \n    binwidth = 350, \n    color = \"blue\",\n    fill = \"red\",\n    alpha = 0.2) \n##Add a frequency polygon\nhistogram +  \n  geom_freqpoly(mapping = aes(x = bwt),\n                binwidth = 350)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIllustrative example: In the birth weight dataset, if we are interested to understand the distribution of smoking history (Present/Absent) among mothers.\n\n\n\n\ngeom_bar() makes the height of the bar proportional to the number of cases in each group.\n\nggplot(data = df) +\n  geom_bar(mapping = aes(x = smoke)) \n\n\n\n\n\n\n\n\nAdd optional arguments (settings) to enhance visualizations.\nWhat happens if we provide y axis rather than x axis?\nWhat happens if instead of writing , we write only in the code chunk?\n\n\n\n\n\n\n\n\n\nIllustrative example: In the birth weights dataset, mother’s weight at last menstrual period and birth weight of the infant are continuous variables. We might be interested in looking at how mother’s weight is associated with birth weight of an infant.\n\n\n\n\nThe geom_point is used to create scatter plots. The scatter plot is most useful for displaying the relationship between two continuous variables.\n\nggplot(data = df) + \n  geom_point(aes(x = lwt, y = bwt), \n             color = \"red\",\n             size = 1,\n             shape = 1,\n             stroke = 1) \n\n\n\n\n\n\n\n\nTips\n\nShape of a scatter plot can be changed using numbers (see below)\nStroke argument in scatter plot determines the width of the border of the shapes\nIn scatter plots, the fill argument works with selected shapes\n\n\n\n\n\n\n\nWhat happens if the size is changed to 10?\nWhen would you like to reduce the size further?\nCan we use geom_point to look at relationships between two variables, even if they are not continuous variables?\nTry using instead of . What do you observe?\n\n\n\n\n\n\nFurther, we can add a regression line to understand the relationship by using geom_smooth. It aids the eye in seeing patterns, especially in the presence of overplotting.\n\nggplot(data = df) +\n  geom_point(aes(x = lwt, y = bwt), \n             color = \"red\",\n             size = 1,\n             shape = 1,\n             stroke = 1) +\n  geom_smooth(aes(x = lwt, y = bwt),\n              method = lm,\n              se = T)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTips Change the method to loess. What do you observe?\n\n\n\n\n\n\nIllustrative example: In the birth weights dataset, smoking history is categorical and birth weight is a continuous variable. We might be interested to estimate if maternal smoking history has effect on infant birth weights.\n\n\n\n\nWhile using bar charts for two variables, an important additional argument which is used is stat = . Lets see!\n\nggplot(data = df) + \n  geom_bar(aes(x = smoke, y = bwt),\n           stat = \"identity\")\n\n\n\n\n\n\n\n\nAlternatively, geom_col can be used. As compared to geom_bar, since counts the number of cases at each x position, additional argument stat = is not required.\n\nggplot(data = df) + \n  geom_col(aes(x = smoke, y = bwt))\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = df) + \n  geom_boxplot(aes(x = smoke, y = bwt),\n               coef = 1.5)\n\n\n\n\n\n\n\n\n\nTips Look at help menu to see additional arguments and change outlier color to red.\n\n\n\n\n\nWhile creating a dot plot an additional mandatory argument in settings is binaxis = \" \". The axis chosen is the axis with continuous variable.\nUse geom_dotplot to create dot plot between smoking history and birth weight.\n\n\n\n\n\n\nIllustrative example: In the dataset, smoking history and whether the infants birth weight was low or not are two discrete/ categorical variables. As a researcher, we would like to see the relationship between these two discrete variables.\n\n\n\n\nTo see relationships between two discrete variables, multiple bar charts and component bar charts are used. Till now, the fill argument has been used in the setting section of the code. If you look carefully, while in the setting section, we manually placed the value/ color. The same argument can also be used in mapping section within aesthetics.\n\nggplot(data = df) + \n  geom_bar(aes(x = smoke, fill = race),\n           position = \"dodge\")\n\n\n\n\n\n\n\n\nReplace to . What happens?\n\n\n\n\n\n\n\nIllustrative example: In the birth weight dataset, a researcher is interested to look at relationship between maternal weight, smoking history, and infants birth weight.\nTip\nWhen the setting arguments are shifted to mapping section, the computer automatically maps the value to develop the plot.\nTo visualize three or more variables, the settings arguments of color, shape, and size can be shifted to mapping section of the template we were using. Lets see!\n\nggplot(data = df) + \n  geom_point(aes(x = lwt, y = bwt, color = smoke))\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, lets see whether the race of mother is also playing a role in addition to her weight and smoking history in determining birth weight of the child\n\nggplot(data = df) + \n  geom_point(aes(x = lwt, y = bwt, \n                 color = smoke, \n                 shape = factor(race)),\n             size = 3)\n\n\n\n\n\n\n\n\nFor incorporating additional variables, multiple approaches such as 3D visualizations, animations, facet charts, etc can be used. For further details, refer to https://exts.ggplot2.tidyverse.org/gallery/\n\n\n\n\n\nfacet_grid() forms a matrix of panels defined by row and column faceting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data. If you have only one variable with many levels, try facet_wrap().\n\nggplot(data = df) + \n  geom_point(aes(x = lwt, y = bwt, color = smoke))+\n  facet_grid(.~race)\n\n\n\n\n\n\n\n\n\nWhat happens if we change to \nUse facet_wrap instead of facet_grid. What do you observe?\n\n\n\n\n\n\n\n\nNow, we know how to create the plot. However, plot has additional components such as title, subtitle, caption, axis labels, legend, etc. The same also requires deliberation and details for the same can be learnt from multiple resources. We recommend R Graphics Cookbook https://r-graphics.org/ as a good resource for the same. We are introducing you to this important aspect of data visualization, however considering the present workshop as an introductory workshop and with time constraints, we plan to cover these aspects during intermediate/ advanced levels only.\n\nggplot(data = df) + # Template\n  geom_freqpoly( # geom\n    mapping = aes(x = bwt, color = smoke), # mapping\n    binwidth = 400,\n    size = 1.5)  + \n  labs(title = \"Birth weight and maternal smoking\", # Title\n       subtitle = \"Density plots\", # Subtitle\n       caption = \"Source: Low Birth Weight data (MASS)\", # Caption\n       x = \"Birth Weights (grams)\", # x axis label\n       y = \"Frequency\",# y axis label\n       color = \"Maternal Smoking history\")  + # Legend title\n  geom_text(x = 2400, # Additional text in the plot\n            y = 10, \n            label = \" Ideal Birth weight\", \n            color = \"#990000\", # RGB color coding palette\n            angle = 90,\n            alpha = .5,)+\n  geom_vline(xintercept= 2500, colour=\"#009900\") + # Line insert \n  theme(legend.position = \"left\") + # Legend settings\n  theme_minimal() # Choosing background theme\n\n\n\n\n\n\n\n\n\n\nR is a powerful programming language. It has capabilities to create its own GIS environment. Though spatial analytics including spatial data visualization requires additional packages, to introduce the domain, we shall be using India spatial file and make a basic map within the tidyverse!\n\n\n\n\n\n\n\n\n\nSince the map provided as demo is against the cartographic principles, despite being an introductory module, we would like you to get an exposure of cartographically correct map too!\n\n\n\n\n\n\n\n\n\n\n\nThe domain of data visualization has gone beyond just visualizing data. Visual analytics and data visualization is now used across the data life cycle in epidemiology and health care. It has moved beyond descriptive epidemiology and is used for hypothesis testing, machine learning, artificial intelligence, time series analysis, spatio-temporal epidemiology, and many others.\nHope you have enjoyed the session! The Jump board is ready! Practice and see the power of visual analytics!! Best Wishes."
  },
  {
    "objectID": "data_visual.html#basics",
    "href": "data_visual.html#basics",
    "title": "Data Visualization",
    "section": "",
    "text": "The present session will focus on basics of data visualization.\n\n\n\n\nBefore we start with learning how to create graphs/ plots/ figures using R, it is important to understand that the selection of type of plots is dependent entirely upon the number and type of variables selected. Though we are not covering the bio statistics module, it is strongly recommended that the same be refreshed and strengthened by participants from time to time for advanced use of data analytics in future.\n\n\n\n\n\nData visualization is a powerful tool for data science in epidmeiology. The basics of programming for creating plots using ggplot package includes understanding of four important aspects.\n\nUse of reusable/ reproducible templates.\nCreation of different types of plots using geoms.\nAddition of variables using mapping.\nCustomization of plots using settings.\n\nFor illustration of concepts, we will be using birthwt data set from MASS package which includes data on Risk Factors Associated with Low Infant Birth Weight in the present session."
  },
  {
    "objectID": "data_visual.html#about-the-dataset",
    "href": "data_visual.html#about-the-dataset",
    "title": "Data Visualization",
    "section": "",
    "text": "birthwt dataset from the MASS package\n\n\nFor illustrative examples in the sections on data visualization, data analysis, and summary tables, Risk Factors Associated with Low Infant Birth Weight dataset (birthwt) dataset has been used. It is an inbuilt dataset from the package MASS. To load the dataset, type the following R code\n\ndf &lt;- MASS::birthwt\n\nSince, it has been labelled as df, it has been reffered as df from here on. The dataset used has 189 rows and 10 columns. The data were collected at Baystate Medical Center, Springfield, Mass during 1986. The variables present in the data includes the following variables/ columns:-\n\nlow : indicator of birth weight less than 2.5 kg\nage: mother’s age in years\nlwt: mother’s weight in pounds at last menstrual period\nrace : mother’s race (1 = white, 2 = black, 3 = other)\nsmoke : smoking status during pregnancy\nptl : number of previous premature labours\nht : history of hypertension\nui : presence of uterine irritability\nftv : number of physician visits during the first trimester\nbwt : birth weight in grams\n\n Hosmer, D.W. and Lemeshow, S. (1989) Applied Logistic Regression. New York: Wiley\n Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth edition. Springer."
  },
  {
    "objectID": "data_visual.html#load-dataset",
    "href": "data_visual.html#load-dataset",
    "title": "Data Visualization",
    "section": "",
    "text": "# install.packages(\"tidyverse\")  includes ggplot\n# install.packages(\"MASS\") includes multiple data sets for learning purposes\noptions(tidyverse.quiet = TRUE)\nlibrary(tidyverse)\ndf &lt;- MASS::birthwt\n\nData cleaning\n\ndf &lt;- df |&gt; \n  mutate(smoke = factor(smoke,\n                        levels = c(0,1),\n                        labels = c(\"Non Smoker\",\n                                   \"Smoker\"))) |&gt; \n  mutate(race = factor(race,\n                       levels = c(1,2,3),\n                       labels = c(\"White\",\n                                  \"Black\",\n                                  \"Other\"))) |&gt; \n  mutate(low = factor(low,\n                      levels =  c(0,1),\n                      labels = c(\"Normal\", \n                                 \"Low Birth Weight\")))"
  },
  {
    "objectID": "data_visual.html#visualization-of-single-variable.",
    "href": "data_visual.html#visualization-of-single-variable.",
    "title": "Data Visualization",
    "section": "",
    "text": "For illustration, we will plot birth weights to understand the distribution pattern among all study participants in birth weight data set.\n\n\n\n\ngeom_histogram visualize the distribution of a single continuous variable by dividing the x axis into bins and counting the number of observations in each bin. Histograms geom_histogram() display the counts with bars\n\nggplot(data = df, aes(x = bwt)) + # Template\n  geom_histogram( # geom\n    mapping = aes(y = after_stat(density)), # mapping\n    binwidth = 350, # mandatory settings\n    color = \"blue\", # optional settings\n    fill = \"red\",\n    linetype = 1,\n    alpha = 0.5,\n    size = 1) +\n  geom_density(color = 'darkgreen', linetype = 1, linewidth = 2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nTips\n\nColor: Determines color of the lines of histogram\nFill: Fills the histogram with specified color\nLinetype: try numbers from 0 to 6\nAlpha: Used for transparency adjustments. Varies from 0 (transparent) to 1 (Opaque)\nSize: Determines thicknes of the lines\n\n\n\n\n\n\ngeom_freqpoly() display the counts with lines. Further, frequency polygons are more suitable when you want to compare the distribution across the levels of a categorical variable (we shall see later!).\n\nggplot(data = df) +\n  geom_freqpoly(mapping = aes(x = bwt),\n                binwidth = 350) # Alternative use of bins\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn a dot plot, the width of a dot corresponds to the bin width, and dots are stacked, with each dot representing one observation.\n\nggplot(data = df) +\n  geom_dotplot(mapping = aes(x = bwt),\n               binwidth = 150) \n\n\n\n\n\n\n\n\n\n\n\n\n\nThe boxplot compactly displays the distribution of a continuous variable. It visualizes five summary statistics (the median, two hinges and two whiskers), and all “outlying” points individually.\n\nggplot(data = df) +\n  geom_boxplot(mapping = aes(y = bwt),\n               coef = 1.5) \n\n\n\n\n\n\n\n\nWhat happens if we change the coef argument to 3?\n\n\n\n\n\ngeom_density computes and draws kernel density estimate, which is a smoothed version of the histogram. This is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\n\nggplot(data = df) + \n  geom_density(mapping = aes(x = bwt))\n\n\n\n\n\n\n\n\nTip\nMultiple plots can be combined for enhanced visualization. For example, we can combine histogram and frequency polygon.\n\n##Create a histogram and save it as an object\nhistogram &lt;- ggplot(data = df) + \n  geom_histogram( \n    mapping = aes(x = bwt), \n    binwidth = 350, \n    color = \"blue\",\n    fill = \"red\",\n    alpha = 0.2) \n##Add a frequency polygon\nhistogram +  \n  geom_freqpoly(mapping = aes(x = bwt),\n                binwidth = 350)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIllustrative example: In the birth weight dataset, if we are interested to understand the distribution of smoking history (Present/Absent) among mothers.\n\n\n\n\ngeom_bar() makes the height of the bar proportional to the number of cases in each group.\n\nggplot(data = df) +\n  geom_bar(mapping = aes(x = smoke)) \n\n\n\n\n\n\n\n\nAdd optional arguments (settings) to enhance visualizations.\nWhat happens if we provide y axis rather than x axis?\nWhat happens if instead of writing , we write only in the code chunk?"
  },
  {
    "objectID": "data_visual.html#visualization-of-two-variables.",
    "href": "data_visual.html#visualization-of-two-variables.",
    "title": "Data Visualization",
    "section": "",
    "text": "Illustrative example: In the birth weights dataset, mother’s weight at last menstrual period and birth weight of the infant are continuous variables. We might be interested in looking at how mother’s weight is associated with birth weight of an infant.\n\n\n\n\nThe geom_point is used to create scatter plots. The scatter plot is most useful for displaying the relationship between two continuous variables.\n\nggplot(data = df) + \n  geom_point(aes(x = lwt, y = bwt), \n             color = \"red\",\n             size = 1,\n             shape = 1,\n             stroke = 1) \n\n\n\n\n\n\n\n\nTips\n\nShape of a scatter plot can be changed using numbers (see below)\nStroke argument in scatter plot determines the width of the border of the shapes\nIn scatter plots, the fill argument works with selected shapes\n\n\n\n\n\n\n\nWhat happens if the size is changed to 10?\nWhen would you like to reduce the size further?\nCan we use geom_point to look at relationships between two variables, even if they are not continuous variables?\nTry using instead of . What do you observe?\n\n\n\n\n\n\nFurther, we can add a regression line to understand the relationship by using geom_smooth. It aids the eye in seeing patterns, especially in the presence of overplotting.\n\nggplot(data = df) +\n  geom_point(aes(x = lwt, y = bwt), \n             color = \"red\",\n             size = 1,\n             shape = 1,\n             stroke = 1) +\n  geom_smooth(aes(x = lwt, y = bwt),\n              method = lm,\n              se = T)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTips Change the method to loess. What do you observe?\n\n\n\n\n\n\nIllustrative example: In the birth weights dataset, smoking history is categorical and birth weight is a continuous variable. We might be interested to estimate if maternal smoking history has effect on infant birth weights.\n\n\n\n\nWhile using bar charts for two variables, an important additional argument which is used is stat = . Lets see!\n\nggplot(data = df) + \n  geom_bar(aes(x = smoke, y = bwt),\n           stat = \"identity\")\n\n\n\n\n\n\n\n\nAlternatively, geom_col can be used. As compared to geom_bar, since counts the number of cases at each x position, additional argument stat = is not required.\n\nggplot(data = df) + \n  geom_col(aes(x = smoke, y = bwt))\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = df) + \n  geom_boxplot(aes(x = smoke, y = bwt),\n               coef = 1.5)\n\n\n\n\n\n\n\n\n\nTips Look at help menu to see additional arguments and change outlier color to red.\n\n\n\n\n\nWhile creating a dot plot an additional mandatory argument in settings is binaxis = \" \". The axis chosen is the axis with continuous variable.\nUse geom_dotplot to create dot plot between smoking history and birth weight.\n\n\n\n\n\n\nIllustrative example: In the dataset, smoking history and whether the infants birth weight was low or not are two discrete/ categorical variables. As a researcher, we would like to see the relationship between these two discrete variables.\n\n\n\n\nTo see relationships between two discrete variables, multiple bar charts and component bar charts are used. Till now, the fill argument has been used in the setting section of the code. If you look carefully, while in the setting section, we manually placed the value/ color. The same argument can also be used in mapping section within aesthetics.\n\nggplot(data = df) + \n  geom_bar(aes(x = smoke, fill = race),\n           position = \"dodge\")\n\n\n\n\n\n\n\n\nReplace to . What happens?"
  },
  {
    "objectID": "data_visual.html#visualization-of-three-variables.",
    "href": "data_visual.html#visualization-of-three-variables.",
    "title": "Data Visualization",
    "section": "",
    "text": "Illustrative example: In the birth weight dataset, a researcher is interested to look at relationship between maternal weight, smoking history, and infants birth weight.\nTip\nWhen the setting arguments are shifted to mapping section, the computer automatically maps the value to develop the plot.\nTo visualize three or more variables, the settings arguments of color, shape, and size can be shifted to mapping section of the template we were using. Lets see!\n\nggplot(data = df) + \n  geom_point(aes(x = lwt, y = bwt, color = smoke))"
  },
  {
    "objectID": "data_visual.html#visualization-of-four-variables.",
    "href": "data_visual.html#visualization-of-four-variables.",
    "title": "Data Visualization",
    "section": "",
    "text": "Now, lets see whether the race of mother is also playing a role in addition to her weight and smoking history in determining birth weight of the child\n\nggplot(data = df) + \n  geom_point(aes(x = lwt, y = bwt, \n                 color = smoke, \n                 shape = factor(race)),\n             size = 3)\n\n\n\n\n\n\n\n\nFor incorporating additional variables, multiple approaches such as 3D visualizations, animations, facet charts, etc can be used. For further details, refer to https://exts.ggplot2.tidyverse.org/gallery/"
  },
  {
    "objectID": "data_visual.html#facet-plots.",
    "href": "data_visual.html#facet-plots.",
    "title": "Data Visualization",
    "section": "",
    "text": "facet_grid() forms a matrix of panels defined by row and column faceting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data. If you have only one variable with many levels, try facet_wrap().\n\nggplot(data = df) + \n  geom_point(aes(x = lwt, y = bwt, color = smoke))+\n  facet_grid(.~race)\n\n\n\n\n\n\n\n\n\nWhat happens if we change to \nUse facet_wrap instead of facet_grid. What do you observe?"
  },
  {
    "objectID": "data_visual.html#way-forward",
    "href": "data_visual.html#way-forward",
    "title": "Data Visualization",
    "section": "",
    "text": "Now, we know how to create the plot. However, plot has additional components such as title, subtitle, caption, axis labels, legend, etc. The same also requires deliberation and details for the same can be learnt from multiple resources. We recommend R Graphics Cookbook https://r-graphics.org/ as a good resource for the same. We are introducing you to this important aspect of data visualization, however considering the present workshop as an introductory workshop and with time constraints, we plan to cover these aspects during intermediate/ advanced levels only.\n\nggplot(data = df) + # Template\n  geom_freqpoly( # geom\n    mapping = aes(x = bwt, color = smoke), # mapping\n    binwidth = 400,\n    size = 1.5)  + \n  labs(title = \"Birth weight and maternal smoking\", # Title\n       subtitle = \"Density plots\", # Subtitle\n       caption = \"Source: Low Birth Weight data (MASS)\", # Caption\n       x = \"Birth Weights (grams)\", # x axis label\n       y = \"Frequency\",# y axis label\n       color = \"Maternal Smoking history\")  + # Legend title\n  geom_text(x = 2400, # Additional text in the plot\n            y = 10, \n            label = \" Ideal Birth weight\", \n            color = \"#990000\", # RGB color coding palette\n            angle = 90,\n            alpha = .5,)+\n  geom_vline(xintercept= 2500, colour=\"#009900\") + # Line insert \n  theme(legend.position = \"left\") + # Legend settings\n  theme_minimal() # Choosing background theme\n\n\n\n\n\n\n\n\n\n\nR is a powerful programming language. It has capabilities to create its own GIS environment. Though spatial analytics including spatial data visualization requires additional packages, to introduce the domain, we shall be using India spatial file and make a basic map within the tidyverse!\n\n\n\n\n\n\n\n\n\nSince the map provided as demo is against the cartographic principles, despite being an introductory module, we would like you to get an exposure of cartographically correct map too!\n\n\n\n\n\n\n\n\n\n\n\nThe domain of data visualization has gone beyond just visualizing data. Visual analytics and data visualization is now used across the data life cycle in epidemiology and health care. It has moved beyond descriptive epidemiology and is used for hypothesis testing, machine learning, artificial intelligence, time series analysis, spatio-temporal epidemiology, and many others.\nHope you have enjoyed the session! The Jump board is ready! Practice and see the power of visual analytics!! Best Wishes."
  },
  {
    "objectID": "day_three.html",
    "href": "day_three.html",
    "title": "NWDASE 2024",
    "section": "",
    "text": "Day Three continues with a recap and doubt clearance session, followed by an in-depth exploration of spatial epidemiology. Participants will engage with real-life examples, including the historical John Snow map exercise. The session will then introduce different types of spatial data (areal, geostatistical, point patterns, spatio-temporal, and mobility data) along with practical exercises.\nThe afternoon session covers advanced spatial epidemiology concepts, such as spatial autocorrelation and clustering, with hands-on exercises using global and local measures. The workshop will conclude with demonstrations of real-life examples, followed by a feedback and valedictory session."
  },
  {
    "objectID": "day_three.html#topics-covered-today",
    "href": "day_three.html#topics-covered-today",
    "title": "NWDASE 2024",
    "section": "Topics Covered Today",
    "text": "Topics Covered Today\n\nIntroduction to Spatial Epidemiology\nSpatial Data Manipulation\nSpatial Analysis"
  },
  {
    "objectID": "exploring_data.html",
    "href": "exploring_data.html",
    "title": "Exploring Data with R",
    "section": "",
    "text": "To recap what we learnt in the previous sessions.. we now know to work within the R Project environment. here::here() makes it easy for us to manage file paths. You can quickly have a look at your data using the View() and glimpse() functions. Most of the tidy data is read as tibble which is a workhorse of tidyverse.\n\n\n\nIt is here::here() is better than setwd()\n\n\n\n\n\nhere::here() allows us to filepaths very easily\n\n\n\n\n\n\n\n\n# Load the required packages\nlibrary(tidyverse) # required for tidy workflows\nlibrary(rio) # required for importing and exporting data\nlibrary(here) # required for managing file paths\n\nhere() starts at D:/websites/rforspatial_yenapoya\n\n\n\nNote\nThe shortcut for code commenting is Ctrl+Shift+C.\n\n\n\n\n\n\nThe dataset we will be working with has been cleaned (to an extent) for the purposes of this workshop. It is a dataset about tuberculosis that has been downloaded from the World Health Organization and cleaned up for our use. The raw dataset and many more such data are publicly available at: https://www.who.int/teams/global-tuberculosis-programme/data for download.\n\n\n# Check the file path\nhere::here(\"data\", \"who_tubercolosis_data.csv\")\n\n[1] \"D:/websites/rforspatial_yenapoya/data/who_tubercolosis_data.csv\"\n\n# Read Data\ntb &lt;- rio::import(\n  here::here(\"data\", \"who_tubercolosis_data.csv\"), setclass = 'tibble')\n\nTry the following functions using tb as the argument:\n\nglimpse()\nhead()\nnames()\n\nToday, we will be introducing you to three new packages:\n\ndplyr\nskimr\nDataExplorer\n\n\n\n\n\n\n\n\nThe dplyr is a powerful R-package to manipulate, clean and summarize unstructured data. In short, it makes data exploration and data manipulation easy and fast in R.\n\n\n\n\n\nThere are many verbs in dplyr that are useful, some of them are given here…\n\n\n\nImportant functions of the dplyr package to remember\n\n\n\n\n\nSyntax structure of the dplyr verb\n\n\n\n\n\n\n\n\n\nThe pipe operator in dplyr\n\n\nNote\nThe pipe |&gt; means THEN…\nThe pipe is an operator in R that allows you to chain together functions in dplyr.\nLet’s find the bottom 50 rows of tb without and with the pipe.\nTips The native pipe |&gt; is preferred.\n\n#without the pipe\ntail(tb, n = 50)\n\n#with the pipe\ntb |&gt; tail(n = 50)\n\nNow let’s see what the code looks like if we need 2 functions. Find the unique countries in the bottom 50 rows of tb.\n\n#without the pipe\nunique(tail(tb, n = 50)$country)\n\n# with the pipe\ntb |&gt; \n  tail(50) |&gt;\n  distinct(country)\n\nNote\nThe shortcut for the pipe is Ctrl+Shift+M\nYou will notice that we used different functions to complete our task. The code without the pipe uses functions from base R while the code with the pipe uses a mixture (tail() from base R and distinct() from dplyr). Not all functions work with the pipe, but we will usually opt for those that do when we have a choice.\n\n\n\n\n\nThe distinct() function will return the distinct values of a column, while count() provides both the distinct values of a column and then number of times each value shows up. The following example investigates the different regions (who_region) in the tb dataset:\n\ntb |&gt; \n  distinct(who_region) \n\ntb |&gt; \n  count(who_region)\n\nNotice that there is a new column produced by the count function called n.\n\n\n\n\n\nThe arrange() function does what it sounds like. It takes a data frame or tbl and arranges (or sorts) by column(s) of interest. The first argument is the data, and subsequent arguments are columns to sort on. Use the desc() function to arrange by descending.\nThe following code would get the number of times each region is in the dataset:\n\ntb |&gt; \n  count(who_region) |&gt; \n  arrange(n)\n\n# Since the default is ascending order, \n# we are not getting the results that are probably useful, \n# so let's use the desc() function\ntb |&gt; \n  count(who_region) |&gt; \n  arrange(desc(n))\n\n# shortcut for desc() is -\ntb |&gt; \n  count(who_region) |&gt; \n  arrange(-n)\n\n\n\n\n\n“How many countries are there in the tb dataset?\nWhich countries have fewer than 18 rows of data?\nWhich country in which year has the highest tb incidence per 100k?\nWhich country in which year has the highest tb incidence number?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to return rows of the data where some criteria are met, use the filter() function. This is how we subset in the tidyverse. (Base R function is subset())\n\n\n\n\n\nHere are the logical criteria in R:\n\n==: Equal to\n!=: Not equal to\n&gt;: Greater than\n&gt;=: Greater than or equal to\n&lt;: Less than\n&lt;=: Less than or equal to\n\nIf you want to satisfy all of multiple conditions, you can use the “and” operator, &.\nThe “or” operator | (the vertical pipe character, shift-backslash) will return a subset that meet any of the conditions.\nLet’s see all the data from 2015 or more recent\n\ntb |&gt; \n  filter(year &gt;= 2015)\n\nLet’s just see data from India\n\ntb |&gt; \n  filter(country == \"India\")\n\nBoth India and 2015 or more recent\n\ntb |&gt; \n  filter(year &gt;= 2015 & country == \"India\")\n\nWhich countries have incidence_100k below 5?\n\ntb |&gt; \n  filter(incidence_100k &lt; 5) |&gt; \n  distinct(country)\n\n# see them all\ntb |&gt; \n  filter(incidence_100k &lt; 5) |&gt; \n  distinct(country) |&gt;\n  print(n = Inf)\n\n\n\n\n\n\nTo filter() a categorical variable for only certain levels, we can use the %in% operator.\nLet’s see data from India, Nepal, Pakistan and Bangladesh First we will have to figure out how those are spelled in this dataset. Open the spreadsheet viewer and find out. We’ll see a way to find them in code later on in the course.\nOk, so we figured out that they are spelled:\n\n“India”\n“Nepal”\n“Pakistan”\n“Bangladesh”\n\nNow we’ll create a vector of countries we are interested in\n\nindian_subcont &lt;- c(\"India\",\n           \"Nepal\",\n           \"Pakistan\",\n           \"Bangladesh\")\n\nAnd use that vector to filter() tb for countries %in% indian_subcont\n\ntb |&gt; filter(country %in% indian_subcont)\n\nYou can also save the results of a pipeline. Notice that the rows belonging to Indian Subcontinent are returned in the console. If we wanted to do something with those rows, it might be helpful to save them as their own dataset. To create a new object, we use the &lt;- operator.\n\nindian_subcont_tb &lt;- tb |&gt; filter(country %in% indian_subcont)\n\n\n\n\n\n\nThe drop_na() function is extremely useful for when we need to subset a variable to remove missing values.\nReturn the tb dataset without rows that were missing on the hiv_incidence_100k variable\n\ntb |&gt; drop_na(hiv_incidence_100k)\n\nReturn the tb dataset without any rows that had an NA in any column. *Use with caution because this will remove a lot of data\n\ntb |&gt; drop_na()\n\n\n\n\n\n\nWhereas the filter() function allows you to return only certain rows matching a condition, the select() function returns only certain columns. The first argument is the data, and subsequent arguments are the columns you want.\nSee just the country, year, incidence_100k columns\n\n# list the column names you want to see separated by a comma\ntb |&gt; \n  select(country, year, incidence_100k)\n\nUse the - sign to drop these same columns\n\ntb |&gt; \n  select(-country, -year, -incidence_100k)\n\n\n\n\n\n\nThe starts_with(), ends_with() and contains() functions provide very useful tools for dropping/keeping several variables at once without having to list each and every column you want to keep. The function will return columns that either start with a specific string of text, ends with a certain string of text, or contain a certain string of text.\n\n# these functions are all case sensitive\ntb |&gt;  \n  select(starts_with(\"percent\"))\n\ntb |&gt; \n  select(ends_with(\"r\"))\n\ntb |&gt; \n  select(contains(\"_\"))\n\n# columns that do not contain -\ntb |&gt; \n  select(-contains(\"_\"))\n\n\n\n\n\n\nThe summarize() function summarizes multiple values to a single value. On its own the summarize() function doesn’t seem to be all that useful. The dplyr package provides a few convenience functions called n() and n_distinct() that tell you the number of observations or the number of distinct values of a particular variable.\nNote summarize() is the same as summarise()\nNotice that summarize takes a data frame and returns a data frame. In this case it’s a 1x1 data frame with a single row and a single column.\n\ntb |&gt; \n  summarize(mean(hiv_percent))\n\n# watch out for nas. Use na.rm = TRUE to run the calculation after excluding nas.\ntb |&gt; \n  summarize(mean(hiv_percent, na.rm = TRUE))\n\nThe name of the column is the expression used to summarize the data. This usually isn’t pretty, and if we wanted to work with this resulting data frame later on, we’d want to name that returned value something better.\n\ntb |&gt; \n  summarize(hiv_percent = mean(hiv_percent, na.rm = TRUE))\n\n\n\n\n\n\nWe saw that summarize() isn’t that useful on its own. Neither is group_by(). All this does is takes an existing data frame and converts it into a grouped data frame where operations are performed by group.\n\ntb |&gt; \n  group_by(year)\n\ntb |&gt; \n  group_by(year, who_region)\n\n\n\n\n\n\nThe real power comes in where group_by() and summarize() are used together. First, write the group_by() statement. Then pipe the result to a call to summarize().\nLet’s summarize the mean incidence of tb for each year\n\ntb |&gt; \n  group_by(year) |&gt; \n  summarize(mean_inc = mean(incidence_100k, na.rm = TRUE))\n\n#sort the output by descending mean_inc\ntb |&gt; \n  group_by(year) |&gt; \n  summarize(mean_inc = mean(incidence_100k, na.rm = TRUE)) |&gt; \n  arrange(desc(mean_inc))\n\n\n\n\n\n\nMutate creates a new variable or modifies an existing one.\n\n\n\n\n\nLets create a column called ind_sub if the country is in the Indian Subcontinent.\n\n# use our vector indian_subcont that we created before\ntb |&gt; \n  mutate(indian_sub1 = if_else(country %in% indian_subcont, \n                              \"Indian Subcontinent\", \"Others\"))\n\nThe same thing can be done using case_when().\n\ntb |&gt; \n  mutate(indian_sub2 = case_when(country %in% \n                                   indian_subcont ~ \n                                   \"Indian Subcontinent\",\n                           TRUE ~ \"Other\")) \n\nLets do it again, but this time let us make it 1 and 0, 1 if it is a country in the Indian Subcontinent, 0 if otherwise.\n\ntb |&gt; \n  mutate(indian_sub3 = case_when(country %in% indian_subcont ~ 1,\n                           TRUE ~ 0))\n\n\n\n\n\n\nNote\nThe if_else() function may result in slightly shorter code if you only need to code for 2 options. For more options, nested if_else() statements become hard to read and could result in mismatched parentheses so case_when() will be a more elegant solution.\nAs a second example of case_when(), let’s say we wanted to create a new population variable that is low, medium, or high.\nSee the pop broken into 3 equally sized portions\n\nquantile(tb$pop, prob = c(.33, .66))\n\nNote\nSee the help file for quanile function or type ?quantile in the console.\nWe’ll say:\n\nlow pop = 2043237 or less\nmed pop = between 2043237 and 11379155\nhigh pop = above 11379155\n\n\ntb |&gt; \n  mutate(popcat = case_when(pop &lt;= 2043237 ~ \"low\",\n                            pop &gt; 2043237 & pop &lt;= 11379155 ~ \"med\",\n                            TRUE ~ \"high\"))\n\n\n\n\n\n\nTypically in a data science or data analysis project one would have to work with many sources of data. The researcher must be able to combine multiple datasets to answer the questions he or she is interested in. Collectively, these multiple tables of data area called relational data because more than the individual datasets, its the relations that are more important.\nAs with the other dplyr verbs, there are different families of verbs that are designed to work with relational data and one of the most commonly used family of verbs are the mutating joins.\n\n\n\nDifferent type of joins, represented by a series of Venn Diagram\n\n\nThese include:\n\nleft_join(x, y) which combines all columns in data frame x with those in data frame y but only retains rows from x.\nright_join(x, y) also keeps all columns but operates in the opposite direction, returning only rows from y.\nfull_join(x, y) combines all columns of x with all columns of y and retains all rows from both data frames.\ninner_join(x, y) combines all columns present in either x or y but only retains rows that are present in both data frames.\nanti_join(x, y) returns the columns from x only and retains rows of x that are not present in y.\nanti_join(y, x) returns the columns from y only and retains rows of y that are not present in x.\n\n\n\n\nVisual representation of the join() family of verbs\n\n\nApart from specifying the data frames to be joined, we also need to specify the key column(s) that is to be used for joining the data. Key columns are specified with the by argument, e.g. inner_join(x, y, by = \"subject_id\") adds columns of y to x for all rows where the values of the “subject_id” column (present in each data frame) match. If the name of the key column is different in both the dataframes, e.g. “subject_id” in x and “subj_id” in y, then you have to specify both names using by = c(\"subject_id\" = \"subj_id\").\n\n\n\n\n\nMost often, when working with our data we may have to reshape our data from long format to wide format and back. We can use the pivot family of functions to achieve this task. What we mean by “the shape of our data” is how the values are distributed across rows or columns. Here’s a visual representation of the same data in two different shapes:\n\n\n\nLong and Wide format of our data\n\n\n\n“Long” format is where we have a column for each of the types of things we measured or recorded in our data. In other words, each variable has its own column.\n“Wide” format occurs when we have data relating to the same measured thing in different columns. In this case, we have values related to our “metric” spread across multiple columns (a column each for a year).\n\nLet us now use the pivot functions to reshape the data in practice. The two pivot functions are:\n\npivot_wider(): from long to wide format.\npivot_longer(): from wide to long format.\n\n\n\n\n\n\n\n\n\n\n\n\nCheck out the Data Wrangling cheatsheet that covers dplyr and tidyr functions.(https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)\nReview the Tibbles chapter of the excellent, free R for Data Science book.(https://r4ds.had.co.nz/tibbles.html)\nCheck out the Transformations chapter to learn more about the dplyr package. Note that this chapter also uses the graphing package ggplot2 which we have covered yesterday.(https://r4ds.had.co.nz/transform.html)\nCheck out the Relational Data chapter to learn more about the joins.(https://r4ds.had.co.nz/relational-data.html)\n\n\n\n\n\n\n\n\nIn 2007, which 10 countries had the highest incidence_100k?\nWithin the South East Asia who_region, which countries have incidence_100K &gt; 300?\nHow many countries are in each who_region? Put the output in order from lowest to highest number of countries. Hint: use distinct() and arrange()\nWhich country in which year has the highest incidence of tuberculosis?\nExcluding missing values on hiv_incidence_100k, what is the correlation coefficient for the relationship between the tb incidence_100k and the hiv_incidence_100k for each region. Show the output with the highest correlations first. Hint: use cor()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskimr is designed to provide summary statistics about variables in data frames, tibbles, data tables and vectors. The core function of skimr is the skim() function, which is designed to work with (grouped) data frames, and will try coerce other objects to data frames if possible.\n\n\n\n\n\nGive skim() a try.\n\ntb |&gt; skimr::skim()\n\nCheck out the names of the output of skimr\n\ntb |&gt; skimr::skim() |&gt; names()\n\nAlso works with dplyr verbs\n\ntb |&gt; group_by(who_region) |&gt; skimr::skim()\n\n\ntb |&gt; skimr::skim() |&gt; \n  dplyr::select(skim_type, skim_variable, n_missing)\n\n\n\n\nThe DataExplorer package aims to automate most of data handling and visualization, so that users could focus on studying the data and extracting insights.1\n\n\n\n\n\nThe single most important function from the DataExplorer package is create_report()\nTry it for yourself.\n\nlibrary(DataExplorer)\ncreate_report(tb)\n\n\n\n\nCreate a report on Exploratory Data Analysis using RMarkdown\nUse DataExplorer to explore the given data"
  },
  {
    "objectID": "exploring_data.html#getting-started-with-the-data-exploration-pipeline",
    "href": "exploring_data.html#getting-started-with-the-data-exploration-pipeline",
    "title": "Exploring Data with R",
    "section": "",
    "text": "# Load the required packages\nlibrary(tidyverse) # required for tidy workflows\nlibrary(rio) # required for importing and exporting data\nlibrary(here) # required for managing file paths\n\nhere() starts at D:/websites/rforspatial_yenapoya\n\n\n\nNote\nThe shortcut for code commenting is Ctrl+Shift+C.\n\n\n\n\n\n\nThe dataset we will be working with has been cleaned (to an extent) for the purposes of this workshop. It is a dataset about tuberculosis that has been downloaded from the World Health Organization and cleaned up for our use. The raw dataset and many more such data are publicly available at: https://www.who.int/teams/global-tuberculosis-programme/data for download.\n\n\n# Check the file path\nhere::here(\"data\", \"who_tubercolosis_data.csv\")\n\n[1] \"D:/websites/rforspatial_yenapoya/data/who_tubercolosis_data.csv\"\n\n# Read Data\ntb &lt;- rio::import(\n  here::here(\"data\", \"who_tubercolosis_data.csv\"), setclass = 'tibble')\n\nTry the following functions using tb as the argument:\n\nglimpse()\nhead()\nnames()\n\nToday, we will be introducing you to three new packages:\n\ndplyr\nskimr\nDataExplorer"
  },
  {
    "objectID": "exploring_data.html#dplyr-package",
    "href": "exploring_data.html#dplyr-package",
    "title": "Exploring Data with R",
    "section": "",
    "text": "The dplyr is a powerful R-package to manipulate, clean and summarize unstructured data. In short, it makes data exploration and data manipulation easy and fast in R.\n\n\n\n\n\nThere are many verbs in dplyr that are useful, some of them are given here…\n\n\n\nImportant functions of the dplyr package to remember\n\n\n\n\n\nSyntax structure of the dplyr verb\n\n\n\n\n\n\n\n\n\nThe pipe operator in dplyr\n\n\nNote\nThe pipe |&gt; means THEN…\nThe pipe is an operator in R that allows you to chain together functions in dplyr.\nLet’s find the bottom 50 rows of tb without and with the pipe.\nTips The native pipe |&gt; is preferred.\n\n#without the pipe\ntail(tb, n = 50)\n\n#with the pipe\ntb |&gt; tail(n = 50)\n\nNow let’s see what the code looks like if we need 2 functions. Find the unique countries in the bottom 50 rows of tb.\n\n#without the pipe\nunique(tail(tb, n = 50)$country)\n\n# with the pipe\ntb |&gt; \n  tail(50) |&gt;\n  distinct(country)\n\nNote\nThe shortcut for the pipe is Ctrl+Shift+M\nYou will notice that we used different functions to complete our task. The code without the pipe uses functions from base R while the code with the pipe uses a mixture (tail() from base R and distinct() from dplyr). Not all functions work with the pipe, but we will usually opt for those that do when we have a choice.\n\n\n\n\n\nThe distinct() function will return the distinct values of a column, while count() provides both the distinct values of a column and then number of times each value shows up. The following example investigates the different regions (who_region) in the tb dataset:\n\ntb |&gt; \n  distinct(who_region) \n\ntb |&gt; \n  count(who_region)\n\nNotice that there is a new column produced by the count function called n.\n\n\n\n\n\nThe arrange() function does what it sounds like. It takes a data frame or tbl and arranges (or sorts) by column(s) of interest. The first argument is the data, and subsequent arguments are columns to sort on. Use the desc() function to arrange by descending.\nThe following code would get the number of times each region is in the dataset:\n\ntb |&gt; \n  count(who_region) |&gt; \n  arrange(n)\n\n# Since the default is ascending order, \n# we are not getting the results that are probably useful, \n# so let's use the desc() function\ntb |&gt; \n  count(who_region) |&gt; \n  arrange(desc(n))\n\n# shortcut for desc() is -\ntb |&gt; \n  count(who_region) |&gt; \n  arrange(-n)\n\n\n\n\n\n“How many countries are there in the tb dataset?\nWhich countries have fewer than 18 rows of data?\nWhich country in which year has the highest tb incidence per 100k?\nWhich country in which year has the highest tb incidence number?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to return rows of the data where some criteria are met, use the filter() function. This is how we subset in the tidyverse. (Base R function is subset())\n\n\n\n\n\nHere are the logical criteria in R:\n\n==: Equal to\n!=: Not equal to\n&gt;: Greater than\n&gt;=: Greater than or equal to\n&lt;: Less than\n&lt;=: Less than or equal to\n\nIf you want to satisfy all of multiple conditions, you can use the “and” operator, &.\nThe “or” operator | (the vertical pipe character, shift-backslash) will return a subset that meet any of the conditions.\nLet’s see all the data from 2015 or more recent\n\ntb |&gt; \n  filter(year &gt;= 2015)\n\nLet’s just see data from India\n\ntb |&gt; \n  filter(country == \"India\")\n\nBoth India and 2015 or more recent\n\ntb |&gt; \n  filter(year &gt;= 2015 & country == \"India\")\n\nWhich countries have incidence_100k below 5?\n\ntb |&gt; \n  filter(incidence_100k &lt; 5) |&gt; \n  distinct(country)\n\n# see them all\ntb |&gt; \n  filter(incidence_100k &lt; 5) |&gt; \n  distinct(country) |&gt;\n  print(n = Inf)\n\n\n\n\n\n\nTo filter() a categorical variable for only certain levels, we can use the %in% operator.\nLet’s see data from India, Nepal, Pakistan and Bangladesh First we will have to figure out how those are spelled in this dataset. Open the spreadsheet viewer and find out. We’ll see a way to find them in code later on in the course.\nOk, so we figured out that they are spelled:\n\n“India”\n“Nepal”\n“Pakistan”\n“Bangladesh”\n\nNow we’ll create a vector of countries we are interested in\n\nindian_subcont &lt;- c(\"India\",\n           \"Nepal\",\n           \"Pakistan\",\n           \"Bangladesh\")\n\nAnd use that vector to filter() tb for countries %in% indian_subcont\n\ntb |&gt; filter(country %in% indian_subcont)\n\nYou can also save the results of a pipeline. Notice that the rows belonging to Indian Subcontinent are returned in the console. If we wanted to do something with those rows, it might be helpful to save them as their own dataset. To create a new object, we use the &lt;- operator.\n\nindian_subcont_tb &lt;- tb |&gt; filter(country %in% indian_subcont)\n\n\n\n\n\n\nThe drop_na() function is extremely useful for when we need to subset a variable to remove missing values.\nReturn the tb dataset without rows that were missing on the hiv_incidence_100k variable\n\ntb |&gt; drop_na(hiv_incidence_100k)\n\nReturn the tb dataset without any rows that had an NA in any column. *Use with caution because this will remove a lot of data\n\ntb |&gt; drop_na()\n\n\n\n\n\n\nWhereas the filter() function allows you to return only certain rows matching a condition, the select() function returns only certain columns. The first argument is the data, and subsequent arguments are the columns you want.\nSee just the country, year, incidence_100k columns\n\n# list the column names you want to see separated by a comma\ntb |&gt; \n  select(country, year, incidence_100k)\n\nUse the - sign to drop these same columns\n\ntb |&gt; \n  select(-country, -year, -incidence_100k)\n\n\n\n\n\n\nThe starts_with(), ends_with() and contains() functions provide very useful tools for dropping/keeping several variables at once without having to list each and every column you want to keep. The function will return columns that either start with a specific string of text, ends with a certain string of text, or contain a certain string of text.\n\n# these functions are all case sensitive\ntb |&gt;  \n  select(starts_with(\"percent\"))\n\ntb |&gt; \n  select(ends_with(\"r\"))\n\ntb |&gt; \n  select(contains(\"_\"))\n\n# columns that do not contain -\ntb |&gt; \n  select(-contains(\"_\"))\n\n\n\n\n\n\nThe summarize() function summarizes multiple values to a single value. On its own the summarize() function doesn’t seem to be all that useful. The dplyr package provides a few convenience functions called n() and n_distinct() that tell you the number of observations or the number of distinct values of a particular variable.\nNote summarize() is the same as summarise()\nNotice that summarize takes a data frame and returns a data frame. In this case it’s a 1x1 data frame with a single row and a single column.\n\ntb |&gt; \n  summarize(mean(hiv_percent))\n\n# watch out for nas. Use na.rm = TRUE to run the calculation after excluding nas.\ntb |&gt; \n  summarize(mean(hiv_percent, na.rm = TRUE))\n\nThe name of the column is the expression used to summarize the data. This usually isn’t pretty, and if we wanted to work with this resulting data frame later on, we’d want to name that returned value something better.\n\ntb |&gt; \n  summarize(hiv_percent = mean(hiv_percent, na.rm = TRUE))\n\n\n\n\n\n\nWe saw that summarize() isn’t that useful on its own. Neither is group_by(). All this does is takes an existing data frame and converts it into a grouped data frame where operations are performed by group.\n\ntb |&gt; \n  group_by(year)\n\ntb |&gt; \n  group_by(year, who_region)\n\n\n\n\n\n\nThe real power comes in where group_by() and summarize() are used together. First, write the group_by() statement. Then pipe the result to a call to summarize().\nLet’s summarize the mean incidence of tb for each year\n\ntb |&gt; \n  group_by(year) |&gt; \n  summarize(mean_inc = mean(incidence_100k, na.rm = TRUE))\n\n#sort the output by descending mean_inc\ntb |&gt; \n  group_by(year) |&gt; \n  summarize(mean_inc = mean(incidence_100k, na.rm = TRUE)) |&gt; \n  arrange(desc(mean_inc))\n\n\n\n\n\n\nMutate creates a new variable or modifies an existing one.\n\n\n\n\n\nLets create a column called ind_sub if the country is in the Indian Subcontinent.\n\n# use our vector indian_subcont that we created before\ntb |&gt; \n  mutate(indian_sub1 = if_else(country %in% indian_subcont, \n                              \"Indian Subcontinent\", \"Others\"))\n\nThe same thing can be done using case_when().\n\ntb |&gt; \n  mutate(indian_sub2 = case_when(country %in% \n                                   indian_subcont ~ \n                                   \"Indian Subcontinent\",\n                           TRUE ~ \"Other\")) \n\nLets do it again, but this time let us make it 1 and 0, 1 if it is a country in the Indian Subcontinent, 0 if otherwise.\n\ntb |&gt; \n  mutate(indian_sub3 = case_when(country %in% indian_subcont ~ 1,\n                           TRUE ~ 0))\n\n\n\n\n\n\nNote\nThe if_else() function may result in slightly shorter code if you only need to code for 2 options. For more options, nested if_else() statements become hard to read and could result in mismatched parentheses so case_when() will be a more elegant solution.\nAs a second example of case_when(), let’s say we wanted to create a new population variable that is low, medium, or high.\nSee the pop broken into 3 equally sized portions\n\nquantile(tb$pop, prob = c(.33, .66))\n\nNote\nSee the help file for quanile function or type ?quantile in the console.\nWe’ll say:\n\nlow pop = 2043237 or less\nmed pop = between 2043237 and 11379155\nhigh pop = above 11379155\n\n\ntb |&gt; \n  mutate(popcat = case_when(pop &lt;= 2043237 ~ \"low\",\n                            pop &gt; 2043237 & pop &lt;= 11379155 ~ \"med\",\n                            TRUE ~ \"high\"))\n\n\n\n\n\n\nTypically in a data science or data analysis project one would have to work with many sources of data. The researcher must be able to combine multiple datasets to answer the questions he or she is interested in. Collectively, these multiple tables of data area called relational data because more than the individual datasets, its the relations that are more important.\nAs with the other dplyr verbs, there are different families of verbs that are designed to work with relational data and one of the most commonly used family of verbs are the mutating joins.\n\n\n\nDifferent type of joins, represented by a series of Venn Diagram\n\n\nThese include:\n\nleft_join(x, y) which combines all columns in data frame x with those in data frame y but only retains rows from x.\nright_join(x, y) also keeps all columns but operates in the opposite direction, returning only rows from y.\nfull_join(x, y) combines all columns of x with all columns of y and retains all rows from both data frames.\ninner_join(x, y) combines all columns present in either x or y but only retains rows that are present in both data frames.\nanti_join(x, y) returns the columns from x only and retains rows of x that are not present in y.\nanti_join(y, x) returns the columns from y only and retains rows of y that are not present in x.\n\n\n\n\nVisual representation of the join() family of verbs\n\n\nApart from specifying the data frames to be joined, we also need to specify the key column(s) that is to be used for joining the data. Key columns are specified with the by argument, e.g. inner_join(x, y, by = \"subject_id\") adds columns of y to x for all rows where the values of the “subject_id” column (present in each data frame) match. If the name of the key column is different in both the dataframes, e.g. “subject_id” in x and “subj_id” in y, then you have to specify both names using by = c(\"subject_id\" = \"subj_id\").\n\n\n\n\n\nMost often, when working with our data we may have to reshape our data from long format to wide format and back. We can use the pivot family of functions to achieve this task. What we mean by “the shape of our data” is how the values are distributed across rows or columns. Here’s a visual representation of the same data in two different shapes:\n\n\n\nLong and Wide format of our data\n\n\n\n“Long” format is where we have a column for each of the types of things we measured or recorded in our data. In other words, each variable has its own column.\n“Wide” format occurs when we have data relating to the same measured thing in different columns. In this case, we have values related to our “metric” spread across multiple columns (a column each for a year).\n\nLet us now use the pivot functions to reshape the data in practice. The two pivot functions are:\n\npivot_wider(): from long to wide format.\npivot_longer(): from wide to long format.\n\n\n\n\n\n\n\n\n\n\n\n\nCheck out the Data Wrangling cheatsheet that covers dplyr and tidyr functions.(https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)\nReview the Tibbles chapter of the excellent, free R for Data Science book.(https://r4ds.had.co.nz/tibbles.html)\nCheck out the Transformations chapter to learn more about the dplyr package. Note that this chapter also uses the graphing package ggplot2 which we have covered yesterday.(https://r4ds.had.co.nz/transform.html)\nCheck out the Relational Data chapter to learn more about the joins.(https://r4ds.had.co.nz/relational-data.html)\n\n\n\n\n\n\n\n\nIn 2007, which 10 countries had the highest incidence_100k?\nWithin the South East Asia who_region, which countries have incidence_100K &gt; 300?\nHow many countries are in each who_region? Put the output in order from lowest to highest number of countries. Hint: use distinct() and arrange()\nWhich country in which year has the highest incidence of tuberculosis?\nExcluding missing values on hiv_incidence_100k, what is the correlation coefficient for the relationship between the tb incidence_100k and the hiv_incidence_100k for each region. Show the output with the highest correlations first. Hint: use cor()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskimr is designed to provide summary statistics about variables in data frames, tibbles, data tables and vectors. The core function of skimr is the skim() function, which is designed to work with (grouped) data frames, and will try coerce other objects to data frames if possible.\n\n\n\n\n\nGive skim() a try.\n\ntb |&gt; skimr::skim()\n\nCheck out the names of the output of skimr\n\ntb |&gt; skimr::skim() |&gt; names()\n\nAlso works with dplyr verbs\n\ntb |&gt; group_by(who_region) |&gt; skimr::skim()\n\n\ntb |&gt; skimr::skim() |&gt; \n  dplyr::select(skim_type, skim_variable, n_missing)\n\n\n\n\nThe DataExplorer package aims to automate most of data handling and visualization, so that users could focus on studying the data and extracting insights.1\n\n\n\n\n\nThe single most important function from the DataExplorer package is create_report()\nTry it for yourself.\n\nlibrary(DataExplorer)\ncreate_report(tb)\n\n\n\n\nCreate a report on Exploratory Data Analysis using RMarkdown\nUse DataExplorer to explore the given data"
  },
  {
    "objectID": "exploring_data.html#skimr-package",
    "href": "exploring_data.html#skimr-package",
    "title": "Exploring Data with R",
    "section": "",
    "text": "skimr is designed to provide summary statistics about variables in data frames, tibbles, data tables and vectors. The core function of skimr is the skim() function, which is designed to work with (grouped) data frames, and will try coerce other objects to data frames if possible.\n\n\n\n\n\nGive skim() a try.\n\ntb |&gt; skimr::skim()\n\nCheck out the names of the output of skimr\n\ntb |&gt; skimr::skim() |&gt; names()\n\nAlso works with dplyr verbs\n\ntb |&gt; group_by(who_region) |&gt; skimr::skim()\n\n\ntb |&gt; skimr::skim() |&gt; \n  dplyr::select(skim_type, skim_variable, n_missing)"
  },
  {
    "objectID": "exploring_data.html#dataexplorer-package",
    "href": "exploring_data.html#dataexplorer-package",
    "title": "Exploring Data with R",
    "section": "",
    "text": "The DataExplorer package aims to automate most of data handling and visualization, so that users could focus on studying the data and extracting insights.1\n\n\n\n\n\nThe single most important function from the DataExplorer package is create_report()\nTry it for yourself.\n\nlibrary(DataExplorer)\ncreate_report(tb)\n\n\n\n\nCreate a report on Exploratory Data Analysis using RMarkdown\nUse DataExplorer to explore the given data"
  },
  {
    "objectID": "exploring_data.html#footnotes",
    "href": "exploring_data.html#footnotes",
    "title": "Exploring Data with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDataExplorer Package↩︎"
  },
  {
    "objectID": "gtsummary.html",
    "href": "gtsummary.html",
    "title": "Summary Tables",
    "section": "",
    "text": "Measures of central tendency\nMean\nThe function mean() is used to calculate this in R. The basic syntax for calculating mean in R is mean(x, trim = 0, na.rm = FALSE, ...). Following is the description of the parameters used:\nx is the input vector. trim is used to drop some observations from both end of the sorted vector. na.rm is used to remove the missing values from the input vector.\nMedian\nThe median() function is used in R to calculate this value. The basic syntax for calculating median in R is median(x, na.rm = FALSE). Following is the description of the parameters used\nx is the input vector. na.rm is used to remove the missing values from the input vector.\nThe summarise() creates a new data frame. It will have one (or more) rows for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarizing all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified.\nIllustrative example. Let us calculate mean and median birth weight of all infants in birthwt data.\n\ndf %&gt;% \n  summarize(\n    mean_bwt = mean(bwt, na.rm = F),\n    median_bwt = median(bwt)\n  ) \n\n  mean_bwt median_bwt\n1 2944.587       2977\n\n\nIllustrative example: Mean of two or more groups. Mean Birth weight among those with history of maternal smoking and those without history of maternal smoking\nMost data operations are done on groups defined by variables. group_by() takes an existing data set and converts it into a grouped data set where operations are performed “by group”.ungroup() removes grouping.\n\ndf %&gt;% \n  group_by(smoke) %&gt;% \n  summarise(mean_bwt = mean(bwt),\n            .groups = \"keep\") \n\n# A tibble: 2 × 2\n# Groups:   smoke [2]\n  smoke      mean_bwt\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Non Smoker    3056.\n2 Smoker        2772.\n\n\nMeasures of dispersion\nStandard deviation\nsd function computes the standard deviation of the values in x. Its syntax is sd(x, na.rm = FALSE). If na.rm is TRUE then missing values are removed before computation proceeds.\n\ndf %&gt;% \n  group_by(smoke) %&gt;% \n  summarise(mean_bwt = mean(bwt),\n            sd_bwt = sd(bwt),\n            .groups = \"keep\") \n\n# A tibble: 2 × 3\n# Groups:   smoke [2]\n  smoke      mean_bwt sd_bwt\n  &lt;fct&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n1 Non Smoker    3056.   753.\n2 Smoker        2772.   660.\n\n\nInterquartile range\nIQR computes interquartile range of the x values.\n\ndf %&gt;% \n  group_by(smoke) %&gt;% \n  summarise(median_bwt = median(bwt),\n            iqr_bwt = IQR(bwt),\n            .groups = \"keep\") \n\n# A tibble: 2 × 3\n# Groups:   smoke [2]\n  smoke      median_bwt iqr_bwt\n  &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1 Non Smoker      3100    1112.\n2 Smoker          2776.    875.\n\n\n\n\n\n\n\n\nIn routine, it is very time consuming, frustrating, and error prone to write again the results/ outputs obtained from statistical software into the writing and communication documents, be it an article/ manuscript or a dissertation or a thesis.\nFurther, Most courses and tutorials on Data Analytics using R teach a bunch of R functions but do not lead us to the outcome, which is to produce analyzed tables.\nIn R, there are certain packages which enable you to create publication ready tables which can be incorporated into research dissemination documents directly or with minor modifications. This saves a lot of mundane and unnecessary work and provides more time for interpretation and domain expertise related work.\nWe shall be using the gtsummary package which is compatible with tidy principles of working and creates presentation-ready tables, regression models, and more. The code to create the tables is concise and highly customizable.\n\n\n\n\n\n\n\n\n\nThe tbl_summary() function calculates descriptive statistics for continuous, categorical, and dichotomous variables in R, and presents the results in a beautiful, customizable summary table ready for publication. To introduce tbl_summary() we will show the most basic behaviour first, which actually produces a large and beautiful table. Then, we will examine in detail how to make adjustments and more tailored tables.The default behavior of tbl_summary() is quite incredible - it takes the columns you provide and creates a summary table in one command. The function prints statistics appropriate to the column class: median and inter-quartile range (IQR) for numeric columns, and counts (%) for categorical columns. Missing values are converted to ‘Unknown’.\nIllustrative example: A researcher is interested to know the basic descriptive analysis of the first five variables in low birth weight data.\n\ndf |&gt;  \n  select(low, bwt) |&gt;\n  tbl_summary() |&gt;  \n  as_hux_table()\n\n\n\nCharacteristic\nN = 189\n\n\nlow\n\nNormal130 (69%)\n\nLow Birth Weight59 (31%)\n\nbwt2,977 (2,414, 3,487)\n\nn (%); Median (IQR)\n\n\n\n\nNote\nThe sensible defaults with this basic usage: each of the defaults may be customized. Variable types are automatically detected so that appropriate descriptive statistics are calculated. Label attributes from the data set are automatically printed. Missing values are listed as “Unknown” in the table. Variable levels are indented and footnotes are added.\n\n\n\n\n\nYou can stratify your table by a column (e.g. by outcome), creating a 2-way table by using by = argument in the tbl_summary() function.\n\ndf |&gt;  \n  select(smoke, low) |&gt; \n  tbl_summary(by = low) |&gt;  as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nsmoke\n\nNon Smoker86 (66%)29 (49%)\n\nSmoker44 (34%)30 (51%)\n\nn (%)\n\n\n\n\n\n\n\nUse an equations to specify which statistics to show and how to display them. There are two sides to the equation, separated by a tilde ~. On the right side, in quotes, is the statistical display desired, and on the left are the columns to which that display will apply.\n\ndf %&gt;% \n  select(bwt, low) %&gt;%\n  tbl_summary(\n    by = low,\n    statistic = bwt~\"{mean}\"\n  ) %&gt;% \n  as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nbwt3,3292,097\n\nMean\n\n\n\n\n\ndf %&gt;% \n  select(bwt, low) %&gt;%\n  tbl_summary(\n    by = low,\n    statistic = bwt~\"{mean}, {sd}\"\n  ) %&gt;% \n  as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nbwt3,329, 4782,097, 391\n\nMean, SD\n\n\n\n\n\n\n\nAdjust how the column name should be displayed. Provide the column name and its desired label separated by a tilde. The default is the column name. This is done with help of argument label = in tbl_summary function.\n\ndf %&gt;% \n  select(bwt, low) %&gt;%\n  tbl_summary(\n    by = low,\n    statistic = bwt~\"{mean}, {sd}\",\n    label = bwt ~ \"Birth Weight\"\n  ) %&gt;% as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nBirth Weight3,329, 4782,097, 391\n\nMean, SD\n\n\n\n\n\n\n\nYou can change labels of multiple variables by providing the labels as a list to the label argument.\n\ndf %&gt;% \n  select(bwt, low, smoke) %&gt;%\n  tbl_summary(\n    by = low,\n    statistic = bwt~\"{mean}, {sd}\",\n    label = list(bwt ~ \"Birth Weight\",\n                 smoke ~ \"Smoking history\")) %&gt;% as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nBirth Weight3,329, 4782,097, 391\n\nSmoking history\n\nNon Smoker86 (66%)29 (49%)\n\nSmoker44 (34%)30 (51%)\n\nMean, SD; n (%)\n\n\n\n\nCan we provide a list to the statistic argument also for customizing statistical output? Try it!\n\n\n\nIf you want to print multiple lines of statistics for variables, you can indicate this by setting the type = to “continuous2”. You can combine all of the previously shown elements in one table by choosing which statistics you want to show. To do this you need to tell the function that you want to get a table back by entering the type as continuous2.\n\ndf %&gt;% \n  select(bwt, low, smoke) %&gt;%\n  tbl_summary(\n    by = low,\n    type = bwt ~ \"continuous2\",\n    statistic = bwt~c(\n      \"{mean}, {sd}\",\n      \"{median}, ({p25}, {p75})\"),\n    label = list(bwt ~ \"Birth Weight\",\n                 smoke ~ \"Smoking history\")) %&gt;% as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nBirth Weight\n\nMean, SD3,329, 4782,097, 391\n\nMedian, (IQR)3,267, (2,948, 3,651)2,211, (1,928, 2,396)\n\nSmoking history\n\nNon Smoker86 (66%)29 (49%)\n\nSmoker44 (34%)30 (51%)\n\nn (%)\n\n\n\n\n\n\n\nIf you wish to print multiline output for all continuous variables, instead of providing “continuous2” argument specified by name of the variable, use continous() in type and statisticarguments.\n\ndf %&gt;% \n  select(bwt, low, smoke, lwt) %&gt;%\n  tbl_summary(\n    by = low,\n    type = all_continuous() ~ \"continuous2\",\n    statistic = all_continuous()~c(\n      \"{mean}, {sd}\",\n      \"{median}, ({p25}, {p75})\"),\n    label = list(bwt ~ \"Birth Weight\",\n                 smoke ~ \"Smoking history\")) %&gt;% as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nBirth Weight\n\nMean, SD3,329, 4782,097, 391\n\nMedian, (IQR)3,267, (2,948, 3,651)2,211, (1,928, 2,396)\n\nSmoking history\n\nNon Smoker86 (66%)29 (49%)\n\nSmoker44 (34%)30 (51%)\n\nlwt\n\nMean, SD133, 32122, 27\n\nMedian, (IQR)124, (113, 147)120, (104, 130)\n\nn (%)\n\n\n\n\n\n\n\nThe type argument in tbl_summary function is an optional argument which includes details for the customized outputs according to the type of variables.\n\ndf %&gt;% \n  select(bwt, low, smoke) %&gt;% \n  tbl_summary(type = all_continuous() ~ \"continuous2\", \n              statistic = list(all_continuous() ~ c(\n                \"{mean} ({sd})\", \n                \"{median} ({p25}, {p75})\"), \n      all_categorical() ~ \"{n} ({p}%)\"),   \n      digits = all_continuous() ~ 1) %&gt;% # setting for decimal points\n  as_hux_table()\n\n\n\nCharacteristic\nN = 189\n\n\nbwt\n\nMean (SD)2,944.6 (729.2)\n\nMedian (IQR)2,977.0 (2,414.0, 3,487.0)\n\nlow\n\nNormal130 (69%)\n\nLow Birth Weight59 (31%)\n\nsmoke\n\nNon Smoker115 (61%)\n\nSmoker74 (39%)\n\nn (%)\n\n\n\n\nTip\nHaving a reproducible code works wonders! Try writing this reproducible code for your own tidy dataset. Voila! You will have publication ready tables.\n\n\n\n\nCompare the difference in means for a continuous variable in two groups. add_p()function from gtsummarypackage adds p-values to gtsummary table\nIllustrative example: t test\n\ndf |&gt;  \n  select(bwt, smoke) |&gt;  \n  tbl_summary(by = smoke) |&gt;  \n  add_p(bwt ~ \"t.test\") |&gt;  \n  as_hux_table()\n\n\n\nCharacteristic\nNon Smoker, N = 115\nSmoker, N = 74\np-value\n\n\nbwt3,100 (2,509, 3,622)2,776 (2,371, 3,246)0.007\n\nMedian (IQR)\n\nWelch Two Sample t-test\n\n\n\n\nWhat happens if we do not pass any argument to function? Try it!\n\n\nTo find the list of tests available internally within gtsummary, type ?gtsummary::tests in your console. What do you see? There are tbl_summary() variants as well as add_difference variant. Refer to gtsummary vignettes available at https://cran.r-project.org/web/packages/gtsummary/index.html for more details.\n\n\n\nIllustrative example. A researcher is interested to know whether there is a significant difference in mean birth weight as well as proportion of low birth weight babies among mothers with history of smoking during pregnancy as compared to those without history of smoking during pregnancy.\nTo answer the question for this study, the summary statistics should be grouped by smoking history group, which can be done by using the by= argument. To compare two or more groups, include add_p() with the function, which detects variable type and uses an appropriate statistical test.\n\ndf |&gt; \n  select(smoke, bwt, low) |&gt;  \n  tbl_summary(by = smoke) |&gt;  \n  add_p() |&gt;  \n  as_hux_table()\n\n\n\nCharacteristic\nNon Smoker, N = 115\nSmoker, N = 74\np-value\n\n\nbwt3,100 (2,509, 3,622)2,776 (2,371, 3,246)0.007\n\nlow0.026\n\nNormal86 (75%)44 (59%)\n\nLow Birth Weight29 (25%)30 (41%)\n\nMedian (IQR); n (%)\n\nWilcoxon rank sum test; Pearson's Chi-squared test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe report package core objective is to streamline the process of presenting models and data frames in adherence to established formatting standards such as APA style. By automating the report generation process, “report” ensures consistency and quality in the presentation of results, facilitating the dissemination of research findings while adhering to best practices guidelines.\nWe haven’t explored the details of the ‘report’ package yet, but it’s an exciting tool. It connects R’s analytical power with the polished presentation needed for scholarly papers. Taking a closer look at its features could offer valuable insights and benefits.\n\n\n\nWe have introduced you to one of the most powerful package currently used to develop publication ready tables. It might seem that there is a lot to learn for making publication ready tables using R. Initially, one might feel unnecessary to learn these syntax. However, if you seriously are into creating tables in your professional career, it is recommended and worth investing time to learn these syntax. We are confident your initial effort will save a lot of time subsequently and make you more efficient and accurate in future. Best wishes!"
  },
  {
    "objectID": "gtsummary.html#basic-statistical-analysis-using-r",
    "href": "gtsummary.html#basic-statistical-analysis-using-r",
    "title": "Summary Tables",
    "section": "",
    "text": "Measures of central tendency\nMean\nThe function mean() is used to calculate this in R. The basic syntax for calculating mean in R is mean(x, trim = 0, na.rm = FALSE, ...). Following is the description of the parameters used:\nx is the input vector. trim is used to drop some observations from both end of the sorted vector. na.rm is used to remove the missing values from the input vector.\nMedian\nThe median() function is used in R to calculate this value. The basic syntax for calculating median in R is median(x, na.rm = FALSE). Following is the description of the parameters used\nx is the input vector. na.rm is used to remove the missing values from the input vector.\nThe summarise() creates a new data frame. It will have one (or more) rows for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarizing all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified.\nIllustrative example. Let us calculate mean and median birth weight of all infants in birthwt data.\n\ndf %&gt;% \n  summarize(\n    mean_bwt = mean(bwt, na.rm = F),\n    median_bwt = median(bwt)\n  ) \n\n  mean_bwt median_bwt\n1 2944.587       2977\n\n\nIllustrative example: Mean of two or more groups. Mean Birth weight among those with history of maternal smoking and those without history of maternal smoking\nMost data operations are done on groups defined by variables. group_by() takes an existing data set and converts it into a grouped data set where operations are performed “by group”.ungroup() removes grouping.\n\ndf %&gt;% \n  group_by(smoke) %&gt;% \n  summarise(mean_bwt = mean(bwt),\n            .groups = \"keep\") \n\n# A tibble: 2 × 2\n# Groups:   smoke [2]\n  smoke      mean_bwt\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Non Smoker    3056.\n2 Smoker        2772.\n\n\nMeasures of dispersion\nStandard deviation\nsd function computes the standard deviation of the values in x. Its syntax is sd(x, na.rm = FALSE). If na.rm is TRUE then missing values are removed before computation proceeds.\n\ndf %&gt;% \n  group_by(smoke) %&gt;% \n  summarise(mean_bwt = mean(bwt),\n            sd_bwt = sd(bwt),\n            .groups = \"keep\") \n\n# A tibble: 2 × 3\n# Groups:   smoke [2]\n  smoke      mean_bwt sd_bwt\n  &lt;fct&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n1 Non Smoker    3056.   753.\n2 Smoker        2772.   660.\n\n\nInterquartile range\nIQR computes interquartile range of the x values.\n\ndf %&gt;% \n  group_by(smoke) %&gt;% \n  summarise(median_bwt = median(bwt),\n            iqr_bwt = IQR(bwt),\n            .groups = \"keep\") \n\n# A tibble: 2 × 3\n# Groups:   smoke [2]\n  smoke      median_bwt iqr_bwt\n  &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1 Non Smoker      3100    1112.\n2 Smoker          2776.    875."
  },
  {
    "objectID": "gtsummary.html#publicaton-ready-summary-tables",
    "href": "gtsummary.html#publicaton-ready-summary-tables",
    "title": "Summary Tables",
    "section": "",
    "text": "In routine, it is very time consuming, frustrating, and error prone to write again the results/ outputs obtained from statistical software into the writing and communication documents, be it an article/ manuscript or a dissertation or a thesis.\nFurther, Most courses and tutorials on Data Analytics using R teach a bunch of R functions but do not lead us to the outcome, which is to produce analyzed tables.\nIn R, there are certain packages which enable you to create publication ready tables which can be incorporated into research dissemination documents directly or with minor modifications. This saves a lot of mundane and unnecessary work and provides more time for interpretation and domain expertise related work.\nWe shall be using the gtsummary package which is compatible with tidy principles of working and creates presentation-ready tables, regression models, and more. The code to create the tables is concise and highly customizable."
  },
  {
    "objectID": "gtsummary.html#introduction-to-publication-ready-tables",
    "href": "gtsummary.html#introduction-to-publication-ready-tables",
    "title": "Summary Tables",
    "section": "",
    "text": "The tbl_summary() function calculates descriptive statistics for continuous, categorical, and dichotomous variables in R, and presents the results in a beautiful, customizable summary table ready for publication. To introduce tbl_summary() we will show the most basic behaviour first, which actually produces a large and beautiful table. Then, we will examine in detail how to make adjustments and more tailored tables.The default behavior of tbl_summary() is quite incredible - it takes the columns you provide and creates a summary table in one command. The function prints statistics appropriate to the column class: median and inter-quartile range (IQR) for numeric columns, and counts (%) for categorical columns. Missing values are converted to ‘Unknown’.\nIllustrative example: A researcher is interested to know the basic descriptive analysis of the first five variables in low birth weight data.\n\ndf |&gt;  \n  select(low, bwt) |&gt;\n  tbl_summary() |&gt;  \n  as_hux_table()\n\n\n\nCharacteristic\nN = 189\n\n\nlow\n\nNormal130 (69%)\n\nLow Birth Weight59 (31%)\n\nbwt2,977 (2,414, 3,487)\n\nn (%); Median (IQR)\n\n\n\n\nNote\nThe sensible defaults with this basic usage: each of the defaults may be customized. Variable types are automatically detected so that appropriate descriptive statistics are calculated. Label attributes from the data set are automatically printed. Missing values are listed as “Unknown” in the table. Variable levels are indented and footnotes are added."
  },
  {
    "objectID": "gtsummary.html#adjustments",
    "href": "gtsummary.html#adjustments",
    "title": "Summary Tables",
    "section": "",
    "text": "You can stratify your table by a column (e.g. by outcome), creating a 2-way table by using by = argument in the tbl_summary() function.\n\ndf |&gt;  \n  select(smoke, low) |&gt; \n  tbl_summary(by = low) |&gt;  as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nsmoke\n\nNon Smoker86 (66%)29 (49%)\n\nSmoker44 (34%)30 (51%)\n\nn (%)\n\n\n\n\n\n\n\nUse an equations to specify which statistics to show and how to display them. There are two sides to the equation, separated by a tilde ~. On the right side, in quotes, is the statistical display desired, and on the left are the columns to which that display will apply.\n\ndf %&gt;% \n  select(bwt, low) %&gt;%\n  tbl_summary(\n    by = low,\n    statistic = bwt~\"{mean}\"\n  ) %&gt;% \n  as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nbwt3,3292,097\n\nMean\n\n\n\n\n\ndf %&gt;% \n  select(bwt, low) %&gt;%\n  tbl_summary(\n    by = low,\n    statistic = bwt~\"{mean}, {sd}\"\n  ) %&gt;% \n  as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nbwt3,329, 4782,097, 391\n\nMean, SD\n\n\n\n\n\n\n\nAdjust how the column name should be displayed. Provide the column name and its desired label separated by a tilde. The default is the column name. This is done with help of argument label = in tbl_summary function.\n\ndf %&gt;% \n  select(bwt, low) %&gt;%\n  tbl_summary(\n    by = low,\n    statistic = bwt~\"{mean}, {sd}\",\n    label = bwt ~ \"Birth Weight\"\n  ) %&gt;% as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nBirth Weight3,329, 4782,097, 391\n\nMean, SD\n\n\n\n\n\n\n\nYou can change labels of multiple variables by providing the labels as a list to the label argument.\n\ndf %&gt;% \n  select(bwt, low, smoke) %&gt;%\n  tbl_summary(\n    by = low,\n    statistic = bwt~\"{mean}, {sd}\",\n    label = list(bwt ~ \"Birth Weight\",\n                 smoke ~ \"Smoking history\")) %&gt;% as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nBirth Weight3,329, 4782,097, 391\n\nSmoking history\n\nNon Smoker86 (66%)29 (49%)\n\nSmoker44 (34%)30 (51%)\n\nMean, SD; n (%)\n\n\n\n\nCan we provide a list to the statistic argument also for customizing statistical output? Try it!\n\n\n\nIf you want to print multiple lines of statistics for variables, you can indicate this by setting the type = to “continuous2”. You can combine all of the previously shown elements in one table by choosing which statistics you want to show. To do this you need to tell the function that you want to get a table back by entering the type as continuous2.\n\ndf %&gt;% \n  select(bwt, low, smoke) %&gt;%\n  tbl_summary(\n    by = low,\n    type = bwt ~ \"continuous2\",\n    statistic = bwt~c(\n      \"{mean}, {sd}\",\n      \"{median}, ({p25}, {p75})\"),\n    label = list(bwt ~ \"Birth Weight\",\n                 smoke ~ \"Smoking history\")) %&gt;% as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nBirth Weight\n\nMean, SD3,329, 4782,097, 391\n\nMedian, (IQR)3,267, (2,948, 3,651)2,211, (1,928, 2,396)\n\nSmoking history\n\nNon Smoker86 (66%)29 (49%)\n\nSmoker44 (34%)30 (51%)\n\nn (%)\n\n\n\n\n\n\n\nIf you wish to print multiline output for all continuous variables, instead of providing “continuous2” argument specified by name of the variable, use continous() in type and statisticarguments.\n\ndf %&gt;% \n  select(bwt, low, smoke, lwt) %&gt;%\n  tbl_summary(\n    by = low,\n    type = all_continuous() ~ \"continuous2\",\n    statistic = all_continuous()~c(\n      \"{mean}, {sd}\",\n      \"{median}, ({p25}, {p75})\"),\n    label = list(bwt ~ \"Birth Weight\",\n                 smoke ~ \"Smoking history\")) %&gt;% as_hux_table()\n\n\n\nCharacteristic\nNormal, N = 130\nLow Birth Weight, N = 59\n\n\nBirth Weight\n\nMean, SD3,329, 4782,097, 391\n\nMedian, (IQR)3,267, (2,948, 3,651)2,211, (1,928, 2,396)\n\nSmoking history\n\nNon Smoker86 (66%)29 (49%)\n\nSmoker44 (34%)30 (51%)\n\nlwt\n\nMean, SD133, 32122, 27\n\nMedian, (IQR)124, (113, 147)120, (104, 130)\n\nn (%)\n\n\n\n\n\n\n\nThe type argument in tbl_summary function is an optional argument which includes details for the customized outputs according to the type of variables.\n\ndf %&gt;% \n  select(bwt, low, smoke) %&gt;% \n  tbl_summary(type = all_continuous() ~ \"continuous2\", \n              statistic = list(all_continuous() ~ c(\n                \"{mean} ({sd})\", \n                \"{median} ({p25}, {p75})\"), \n      all_categorical() ~ \"{n} ({p}%)\"),   \n      digits = all_continuous() ~ 1) %&gt;% # setting for decimal points\n  as_hux_table()\n\n\n\nCharacteristic\nN = 189\n\n\nbwt\n\nMean (SD)2,944.6 (729.2)\n\nMedian (IQR)2,977.0 (2,414.0, 3,487.0)\n\nlow\n\nNormal130 (69%)\n\nLow Birth Weight59 (31%)\n\nsmoke\n\nNon Smoker115 (61%)\n\nSmoker74 (39%)\n\nn (%)\n\n\n\n\nTip\nHaving a reproducible code works wonders! Try writing this reproducible code for your own tidy dataset. Voila! You will have publication ready tables."
  },
  {
    "objectID": "gtsummary.html#inferential-statistics-and-publication-ready-tables.",
    "href": "gtsummary.html#inferential-statistics-and-publication-ready-tables.",
    "title": "Summary Tables",
    "section": "",
    "text": "Compare the difference in means for a continuous variable in two groups. add_p()function from gtsummarypackage adds p-values to gtsummary table\nIllustrative example: t test\n\ndf |&gt;  \n  select(bwt, smoke) |&gt;  \n  tbl_summary(by = smoke) |&gt;  \n  add_p(bwt ~ \"t.test\") |&gt;  \n  as_hux_table()\n\n\n\nCharacteristic\nNon Smoker, N = 115\nSmoker, N = 74\np-value\n\n\nbwt3,100 (2,509, 3,622)2,776 (2,371, 3,246)0.007\n\nMedian (IQR)\n\nWelch Two Sample t-test\n\n\n\n\nWhat happens if we do not pass any argument to function? Try it!\n\n\nTo find the list of tests available internally within gtsummary, type ?gtsummary::tests in your console. What do you see? There are tbl_summary() variants as well as add_difference variant. Refer to gtsummary vignettes available at https://cran.r-project.org/web/packages/gtsummary/index.html for more details.\n\n\n\nIllustrative example. A researcher is interested to know whether there is a significant difference in mean birth weight as well as proportion of low birth weight babies among mothers with history of smoking during pregnancy as compared to those without history of smoking during pregnancy.\nTo answer the question for this study, the summary statistics should be grouped by smoking history group, which can be done by using the by= argument. To compare two or more groups, include add_p() with the function, which detects variable type and uses an appropriate statistical test.\n\ndf |&gt; \n  select(smoke, bwt, low) |&gt;  \n  tbl_summary(by = smoke) |&gt;  \n  add_p() |&gt;  \n  as_hux_table()\n\n\n\nCharacteristic\nNon Smoker, N = 115\nSmoker, N = 74\np-value\n\n\nbwt3,100 (2,509, 3,622)2,776 (2,371, 3,246)0.007\n\nlow0.026\n\nNormal86 (75%)44 (59%)\n\nLow Birth Weight29 (25%)30 (41%)\n\nMedian (IQR); n (%)\n\nWilcoxon rank sum test; Pearson's Chi-squared test"
  },
  {
    "objectID": "gtsummary.html#report-a-data",
    "href": "gtsummary.html#report-a-data",
    "title": "Summary Tables",
    "section": "",
    "text": "The report package core objective is to streamline the process of presenting models and data frames in adherence to established formatting standards such as APA style. By automating the report generation process, “report” ensures consistency and quality in the presentation of results, facilitating the dissemination of research findings while adhering to best practices guidelines.\nWe haven’t explored the details of the ‘report’ package yet, but it’s an exciting tool. It connects R’s analytical power with the polished presentation needed for scholarly papers. Taking a closer look at its features could offer valuable insights and benefits."
  },
  {
    "objectID": "gtsummary.html#way-forward",
    "href": "gtsummary.html#way-forward",
    "title": "Summary Tables",
    "section": "",
    "text": "We have introduced you to one of the most powerful package currently used to develop publication ready tables. It might seem that there is a lot to learn for making publication ready tables using R. Initially, one might feel unnecessary to learn these syntax. However, if you seriously are into creating tables in your professional career, it is recommended and worth investing time to learn these syntax. We are confident your initial effort will save a lot of time subsequently and make you more efficient and accurate in future. Best wishes!"
  },
  {
    "objectID": "intro_r.html",
    "href": "intro_r.html",
    "title": "Getting Comfortable with R and RStudio",
    "section": "",
    "text": "Go here: https://cran.rstudio.com/\nChoose the correct “Download R for. . .” option from the top (probably Windows or macOS), then…\n\n\n\nFor Windows users, choose “Install R for the first time” (next to the base subdirectory) and then “Download R 4.4.1 for Windows”\nFor macOS users, select the appropriate version for your operating system (e.g. the latest release is version 4.4.1, will look something like R-4.4.1-arm64.pkg), then choose to Save or Open\nOnce downloaded, save, open once downloaded, agree to license, and install like you would any other software.\n\n\n\n\n\nIf it installs, you should be able to find the R icon in your applications.\n\n\n\n\n\n\nRStudio is a user-friendly interface for working with R. That means you must have R already installed for RStudio to work. Make sure you’ve successfully installed R in Step 1, then. . .\n\nGo to https://www.rstudio.com/products/rstudio/download/ to download RStudio Desktop (Open Source License). You’ll know you’re clicking the right one because it says “FREE” right above the download button.\nClick download, which takes you just down the page to where you can select the correct version under Installers for Supported Platforms (almost everyone will choose one of the first two options, RStudio for Windows or macOS).\nClick on the correct installer version, save, open once downloaded, agree to license and install like you would any other software. The version should be at least RStudio 2023.06.2 “Mountain Hydrangea”, 2023.\n\n\n\n\n\nIf it installs, you should be able to find the RStudio icon in your applications.\n\n\n\n\n\n\n\n\nThe RStudio environment consist of multiple windows. Each window consist of certain Panels\nPanels in RStudio\n\nSource\nConsole\nEnvironment\nHistory\nFiles\nPlots\nConnections\nPackages\nHelp\nBuild\nTutorial\nViewer\n\nIt is important to understand that not all panels will be used by you in routine as well as by us during the workshop. The workshop focuses on using R for healthcare professionals as a database management, visualization, and communication tool. The most common panels which requires attention are the source, console, environment, history, files, packages, help, tutorial, and viewer panels.\n\n\n\n\n\nYou are requested to make your own notes during the workshop. Let us dive deep into understanding the environment further in the workshop.\n\n\n\n\n\n\nIt is important to understand that good workflows facilitate efficient database management. Lets discuss!\n\n\n\n\n\nThe most common used file types are\n\n.R : Script file\n.Rmd : RMarkdown file\n.qmd : Quarto file\n.rds : Single R database file\n.RData : Multiple files in a single R database file\n\n\n\n\n\n\nR is easiest to use when you know how the R language works. This section will teach you the implicit background knowledge that informs every piece of R code. You’ll learn about:\n\nFunctions and their arguments\nObjects\nR’s basic data types\nR’s basic data structures including vectors and lists\nR’s package system\n\n\n\n\n\nTo do anything in R, we call functions to work for us. Take for example, we want to compute square root of 5197. Now, we need to call a function sqrt() for the same.\n\nsqrt(5197)\n\n[1] 72.09022\n\n\nImportant things to know about functions include:\n\nCode body.\n\nTyping code body and running it enables us understand what a function does in background.\n\nsqrt\n\nfunction (x)  .Primitive(\"sqrt\")\n\n\n\nRun a function.\n\nTo run a function, we need to add a parenthesis () after the code body. Within the parenthesis we add the details such as number in the above example.\n\nHelp page.\n\nPlacing a question mark before the function takes you to the help page. This is an important aspect we need to understand. When calling help page parenthesis is not placed. This help page will enable you learn about new functions in your journey!\n\n?sqrt \n\n\nTip:\n\nAnnotations are meant for humans to read and not by machines. It enables us take notes as we write. As a result, next time when you open your code even after a long time, you will know what you did last summer :)\n\nArguments are inputs provided to the function. There are functions which take no arguments, some take a single argument and some take multiple arguments. When there are two or more arguments, the arguments are separated by a comma.\n\n# No argument\nSys.Date()\n\n[1] \"2024-10-02\"\n\n# One argument\nsqrt(5197)\n\n[1] 72.09022\n\n# Two arguments\nsum(2,3)\n\n[1] 5\n\n# Multiple arguments\nseq(from=1,\n    to = 10, \n    by  = 2)\n\n[1] 1 3 5 7 9\n\n\nMatching arguments: Some arguments are understood as such by the software. Take for example, generating a sequence includes three arguments viz: from, to, by. The right inputs are automatically matched to the right argument.\n\nseq(1,10,2)\n\n[1] 1 3 5 7 9\n\n\nCaution: The wrong inputs are also matched. Best practice is to be explicit at early stages. Use argument names!\n\nseq(2,10,1)\n\n[1]  2  3  4  5  6  7  8  9 10\n\nseq(by = 2,\n    to = 10,\n    from = 1)\n\n[1] 1 3 5 7 9\n\n\nOptional arguments: Some arguments are optional. They may be added or removed as per requirement. By default these optional arguments are taken by R as default values. Take for example, in sum() function, na.rm = FALSE is an optional argument. It ensures that the NA values are not removed by default and the sum is not returned when there are NA values. These optional arguments can be override by mentioning them explicitly.\n\nsum(2,3,NA)\n\n[1] NA\n\nsum(2,3,NA, na.rm = T)\n\n[1] 5\n\n\nIn contrast, the arguments which needs to be mentioned explicitly are mandatory! Without them, errors are returned as output.\n\n\nsqrt()\n\n\n\n\n\nIf we want to use the results in addition to viewing them in console, we need to store them as objects. To create an object, type the name of the object (Choose wisely, let it be explicit and self explanatory!), then provide an assignment operator. Everything to the right of the operator will be assigned to the object. You can save a single value or output of a function or multiple values or an entire data set in a single object.\n\n# Single value\nx &lt;- 3\nx\n\n[1] 3\n\n# Output from function\nx &lt;- seq(from=1,\n    to = 10, \n    by  = 2)\n# Better name:\nsequence_from_1_to_10 &lt;- seq(from=1,\n    to = 10, \n    by  = 2)\n\nCreating an object helps us in viewing its contents as well make it easier to apply additional functions\nTip. While typing functions/ object names, R prompts are provided. Choose from the prompts rather than typing the entire thing. It will ease out many things later!\n\n\nsequence_from_1_to_10\n\n[1] 1 3 5 7 9\n\nsum(sequence_from_1_to_10)\n\n[1] 25\n\n\n\n\n\n\nR stores values as a vector which is one dimensional array. Arrays can be two dimensional (similar to excel data/ tabular data), or multidimensional. Vectors are always one dimensional!\nVectors can be a single value or a combination of values. We can create our own vectors using c() function.\n\nsingle_number &lt;- 3\nsingle_number\n\n[1] 3\n\nnumber_vector &lt;- c(1,2,3)\nnumber_vector\n\n[1] 1 2 3\n\n\nCreating personalized vectors is powerful as a lot of functions in R takes vectors as inputs.\n\nmean(number_vector)\n\n[1] 2\n\n\nVectorized functions: The function is applied to each element of the vector:\n\nsqrt(number_vector)\n\n[1] 1.000000 1.414214 1.732051\n\n\nIf we have two vectors of similar lengths (such as columns of a research data), vectorised functions help us compute for new columns by applying the said function on each element of both the vectors and give a vector of the same length (Consider this as a new column in the research data)\n\nnumber_vector2 &lt;- c(3,-4,5.4)\nnumber_vector + number_vector2\n\n[1]  4.0 -2.0  8.4\n\n\n\n\n\n\n\nR recognizes different types of vectors based on the values in the vector.\nIf all values are numbers (positive numbers, negative numbers, decimals), R will consider that vector as numerical and allows you to carry out mathematical operations/ functions. You can find the class of the vector by using class() function.R labels these vectors as “double”, “numeric”, or “integers”.\n\nclass(number_vector)\n\n[1] \"numeric\"\n\nclass(number_vector2)\n\n[1] \"numeric\"\n\n\nIf the values are within quotation marks, it is character variable by default. It is equivalent to nominal variable.\n\nalphabets_vector &lt;- c(\"a\", \"b\", \"c\")\nclass(alphabets_vector)\n\n[1] \"character\"\n\ninteger_vector &lt;- c(1L,2L)\nclass(integer_vector)\n\n[1] \"integer\"\n\n\nLogical vectors contain TRUE and FALSE values\n\nlogical_vector &lt;- c(TRUE, FALSE)\nclass(logical_vector)\n\n[1] \"logical\"\n\n\nFactor vectors are categorical variables. Other variable types can be converted to factor type using functionfactor()\n\nfactor_vector &lt;- factor(number_vector)\nfactor_vector\n\n[1] 1 2 3\nLevels: 1 2 3\n\n\nWe can add labels to factor vectors using optional arguments\n\nfactor_vector &lt;- factor(number_vector,\n                        levels =c(1,2,3),\n                        labels = c(\"level1\", \n                                   \"level2\", \n                                   \"level3\"))\nfactor_vector\n\n[1] level1 level2 level3\nLevels: level1 level2 level3\n\n\nOne vector = One type. For example: When there is mix of numbers and characters, R will consider all as character.\n\nmix_vector &lt;- c(1,\"a\")\nclass(mix_vector)\n\n[1] \"character\"\n\n\nNote that the number 1 has been converted into character class.\n\nmix_vector[1]\n\n[1] \"1\"\n\nmix_vector[1] |&gt; class()\n\n[1] \"character\"\n\n\nDouble, character, integer, logical, complex, raw, dates, etc… There are many other data types and objects but for now, lets start with these. You will understand additional types as you will proceed in your R journey!\n\n\n\n\n\nIn addition to vectors, lists are another powerful objects. A list can be considered as a vector of vectors!! They enable you to store multiple types of vectors together. A list can be made using a list() function. It is similar to c() function but creates a list rather than a vector. It is a good practice to name the vectors in the list.\n\nexample_list &lt;- list(numbers = number_vector, \n                     alphabets = alphabets_vector)\nclass(example_list)\n\n[1] \"list\"\n\nexample_list\n\n$numbers\n[1] 1 2 3\n\n$alphabets\n[1] \"a\" \"b\" \"c\"\n\n\nThe elements of a named list/ a named vector can be called by using a $.\n\nexample_list$numbers\n\n[1] 1 2 3\n\n\n\n\n\n\n\nThere are thousands of functions in R. To be computationally efficient, R do not load all functions on start. It loads only base functions. As you want to use additional functions, we need to load the packages using library() function.\n\nThe additional packages are installed once but loaded everytime you start R sessions.\nWith these basics, lets deep dive into the workshop!! Are you ready?"
  },
  {
    "objectID": "intro_r.html#install-r",
    "href": "intro_r.html#install-r",
    "title": "Getting Comfortable with R and RStudio",
    "section": "",
    "text": "Go here: https://cran.rstudio.com/\nChoose the correct “Download R for. . .” option from the top (probably Windows or macOS), then…\n\n\n\nFor Windows users, choose “Install R for the first time” (next to the base subdirectory) and then “Download R 4.4.1 for Windows”\nFor macOS users, select the appropriate version for your operating system (e.g. the latest release is version 4.4.1, will look something like R-4.4.1-arm64.pkg), then choose to Save or Open\nOnce downloaded, save, open once downloaded, agree to license, and install like you would any other software.\n\n\n\n\n\nIf it installs, you should be able to find the R icon in your applications."
  },
  {
    "objectID": "intro_r.html#install-rstudio",
    "href": "intro_r.html#install-rstudio",
    "title": "Getting Comfortable with R and RStudio",
    "section": "",
    "text": "RStudio is a user-friendly interface for working with R. That means you must have R already installed for RStudio to work. Make sure you’ve successfully installed R in Step 1, then. . .\n\nGo to https://www.rstudio.com/products/rstudio/download/ to download RStudio Desktop (Open Source License). You’ll know you’re clicking the right one because it says “FREE” right above the download button.\nClick download, which takes you just down the page to where you can select the correct version under Installers for Supported Platforms (almost everyone will choose one of the first two options, RStudio for Windows or macOS).\nClick on the correct installer version, save, open once downloaded, agree to license and install like you would any other software. The version should be at least RStudio 2023.06.2 “Mountain Hydrangea”, 2023.\n\n\n\n\n\nIf it installs, you should be able to find the RStudio icon in your applications."
  },
  {
    "objectID": "intro_r.html#understanding-the-rstudio-environment",
    "href": "intro_r.html#understanding-the-rstudio-environment",
    "title": "Getting Comfortable with R and RStudio",
    "section": "",
    "text": "The RStudio environment consist of multiple windows. Each window consist of certain Panels\nPanels in RStudio\n\nSource\nConsole\nEnvironment\nHistory\nFiles\nPlots\nConnections\nPackages\nHelp\nBuild\nTutorial\nViewer\n\nIt is important to understand that not all panels will be used by you in routine as well as by us during the workshop. The workshop focuses on using R for healthcare professionals as a database management, visualization, and communication tool. The most common panels which requires attention are the source, console, environment, history, files, packages, help, tutorial, and viewer panels.\n\n\n\n\n\nYou are requested to make your own notes during the workshop. Let us dive deep into understanding the environment further in the workshop."
  },
  {
    "objectID": "intro_r.html#creating-a-project.",
    "href": "intro_r.html#creating-a-project.",
    "title": "Getting Comfortable with R and RStudio",
    "section": "",
    "text": "It is important to understand that good workflows facilitate efficient database management. Lets discuss!"
  },
  {
    "objectID": "intro_r.html#file-types-in-r",
    "href": "intro_r.html#file-types-in-r",
    "title": "Getting Comfortable with R and RStudio",
    "section": "",
    "text": "The most common used file types are\n\n.R : Script file\n.Rmd : RMarkdown file\n.qmd : Quarto file\n.rds : Single R database file\n.RData : Multiple files in a single R database file"
  },
  {
    "objectID": "intro_r.html#programming-basics.",
    "href": "intro_r.html#programming-basics.",
    "title": "Getting Comfortable with R and RStudio",
    "section": "",
    "text": "R is easiest to use when you know how the R language works. This section will teach you the implicit background knowledge that informs every piece of R code. You’ll learn about:\n\nFunctions and their arguments\nObjects\nR’s basic data types\nR’s basic data structures including vectors and lists\nR’s package system\n\n\n\n\n\nTo do anything in R, we call functions to work for us. Take for example, we want to compute square root of 5197. Now, we need to call a function sqrt() for the same.\n\nsqrt(5197)\n\n[1] 72.09022\n\n\nImportant things to know about functions include:\n\nCode body.\n\nTyping code body and running it enables us understand what a function does in background.\n\nsqrt\n\nfunction (x)  .Primitive(\"sqrt\")\n\n\n\nRun a function.\n\nTo run a function, we need to add a parenthesis () after the code body. Within the parenthesis we add the details such as number in the above example.\n\nHelp page.\n\nPlacing a question mark before the function takes you to the help page. This is an important aspect we need to understand. When calling help page parenthesis is not placed. This help page will enable you learn about new functions in your journey!\n\n?sqrt \n\n\nTip:\n\nAnnotations are meant for humans to read and not by machines. It enables us take notes as we write. As a result, next time when you open your code even after a long time, you will know what you did last summer :)\n\nArguments are inputs provided to the function. There are functions which take no arguments, some take a single argument and some take multiple arguments. When there are two or more arguments, the arguments are separated by a comma.\n\n# No argument\nSys.Date()\n\n[1] \"2024-10-02\"\n\n# One argument\nsqrt(5197)\n\n[1] 72.09022\n\n# Two arguments\nsum(2,3)\n\n[1] 5\n\n# Multiple arguments\nseq(from=1,\n    to = 10, \n    by  = 2)\n\n[1] 1 3 5 7 9\n\n\nMatching arguments: Some arguments are understood as such by the software. Take for example, generating a sequence includes three arguments viz: from, to, by. The right inputs are automatically matched to the right argument.\n\nseq(1,10,2)\n\n[1] 1 3 5 7 9\n\n\nCaution: The wrong inputs are also matched. Best practice is to be explicit at early stages. Use argument names!\n\nseq(2,10,1)\n\n[1]  2  3  4  5  6  7  8  9 10\n\nseq(by = 2,\n    to = 10,\n    from = 1)\n\n[1] 1 3 5 7 9\n\n\nOptional arguments: Some arguments are optional. They may be added or removed as per requirement. By default these optional arguments are taken by R as default values. Take for example, in sum() function, na.rm = FALSE is an optional argument. It ensures that the NA values are not removed by default and the sum is not returned when there are NA values. These optional arguments can be override by mentioning them explicitly.\n\nsum(2,3,NA)\n\n[1] NA\n\nsum(2,3,NA, na.rm = T)\n\n[1] 5\n\n\nIn contrast, the arguments which needs to be mentioned explicitly are mandatory! Without them, errors are returned as output.\n\n\nsqrt()\n\n\n\n\n\nIf we want to use the results in addition to viewing them in console, we need to store them as objects. To create an object, type the name of the object (Choose wisely, let it be explicit and self explanatory!), then provide an assignment operator. Everything to the right of the operator will be assigned to the object. You can save a single value or output of a function or multiple values or an entire data set in a single object.\n\n# Single value\nx &lt;- 3\nx\n\n[1] 3\n\n# Output from function\nx &lt;- seq(from=1,\n    to = 10, \n    by  = 2)\n# Better name:\nsequence_from_1_to_10 &lt;- seq(from=1,\n    to = 10, \n    by  = 2)\n\nCreating an object helps us in viewing its contents as well make it easier to apply additional functions\nTip. While typing functions/ object names, R prompts are provided. Choose from the prompts rather than typing the entire thing. It will ease out many things later!\n\n\nsequence_from_1_to_10\n\n[1] 1 3 5 7 9\n\nsum(sequence_from_1_to_10)\n\n[1] 25\n\n\n\n\n\n\nR stores values as a vector which is one dimensional array. Arrays can be two dimensional (similar to excel data/ tabular data), or multidimensional. Vectors are always one dimensional!\nVectors can be a single value or a combination of values. We can create our own vectors using c() function.\n\nsingle_number &lt;- 3\nsingle_number\n\n[1] 3\n\nnumber_vector &lt;- c(1,2,3)\nnumber_vector\n\n[1] 1 2 3\n\n\nCreating personalized vectors is powerful as a lot of functions in R takes vectors as inputs.\n\nmean(number_vector)\n\n[1] 2\n\n\nVectorized functions: The function is applied to each element of the vector:\n\nsqrt(number_vector)\n\n[1] 1.000000 1.414214 1.732051\n\n\nIf we have two vectors of similar lengths (such as columns of a research data), vectorised functions help us compute for new columns by applying the said function on each element of both the vectors and give a vector of the same length (Consider this as a new column in the research data)\n\nnumber_vector2 &lt;- c(3,-4,5.4)\nnumber_vector + number_vector2\n\n[1]  4.0 -2.0  8.4\n\n\n\n\n\n\n\nR recognizes different types of vectors based on the values in the vector.\nIf all values are numbers (positive numbers, negative numbers, decimals), R will consider that vector as numerical and allows you to carry out mathematical operations/ functions. You can find the class of the vector by using class() function.R labels these vectors as “double”, “numeric”, or “integers”.\n\nclass(number_vector)\n\n[1] \"numeric\"\n\nclass(number_vector2)\n\n[1] \"numeric\"\n\n\nIf the values are within quotation marks, it is character variable by default. It is equivalent to nominal variable.\n\nalphabets_vector &lt;- c(\"a\", \"b\", \"c\")\nclass(alphabets_vector)\n\n[1] \"character\"\n\ninteger_vector &lt;- c(1L,2L)\nclass(integer_vector)\n\n[1] \"integer\"\n\n\nLogical vectors contain TRUE and FALSE values\n\nlogical_vector &lt;- c(TRUE, FALSE)\nclass(logical_vector)\n\n[1] \"logical\"\n\n\nFactor vectors are categorical variables. Other variable types can be converted to factor type using functionfactor()\n\nfactor_vector &lt;- factor(number_vector)\nfactor_vector\n\n[1] 1 2 3\nLevels: 1 2 3\n\n\nWe can add labels to factor vectors using optional arguments\n\nfactor_vector &lt;- factor(number_vector,\n                        levels =c(1,2,3),\n                        labels = c(\"level1\", \n                                   \"level2\", \n                                   \"level3\"))\nfactor_vector\n\n[1] level1 level2 level3\nLevels: level1 level2 level3\n\n\nOne vector = One type. For example: When there is mix of numbers and characters, R will consider all as character.\n\nmix_vector &lt;- c(1,\"a\")\nclass(mix_vector)\n\n[1] \"character\"\n\n\nNote that the number 1 has been converted into character class.\n\nmix_vector[1]\n\n[1] \"1\"\n\nmix_vector[1] |&gt; class()\n\n[1] \"character\"\n\n\nDouble, character, integer, logical, complex, raw, dates, etc… There are many other data types and objects but for now, lets start with these. You will understand additional types as you will proceed in your R journey!\n\n\n\n\n\nIn addition to vectors, lists are another powerful objects. A list can be considered as a vector of vectors!! They enable you to store multiple types of vectors together. A list can be made using a list() function. It is similar to c() function but creates a list rather than a vector. It is a good practice to name the vectors in the list.\n\nexample_list &lt;- list(numbers = number_vector, \n                     alphabets = alphabets_vector)\nclass(example_list)\n\n[1] \"list\"\n\nexample_list\n\n$numbers\n[1] 1 2 3\n\n$alphabets\n[1] \"a\" \"b\" \"c\"\n\n\nThe elements of a named list/ a named vector can be called by using a $.\n\nexample_list$numbers\n\n[1] 1 2 3\n\n\n\n\n\n\n\nThere are thousands of functions in R. To be computationally efficient, R do not load all functions on start. It loads only base functions. As you want to use additional functions, we need to load the packages using library() function.\n\nThe additional packages are installed once but loaded everytime you start R sessions.\nWith these basics, lets deep dive into the workshop!! Are you ready?"
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "Communicating Research with Quarto",
    "section": "",
    "text": "Quarto provides a unified authoring framework for data science, combining your code, its results, and your prose. Quarto documents are fully reproducible and support dozens of output formats, like PDFs, Word files, presentations, and more.\n\n\n\n\n\nQuarto files are designed to be used in three ways:\n\nFor communicating to decision-makers, who want to focus on the conclusions, not the code behind the analysis.\nFor collaborating with other data scientists (including future you!), who are interested in both your conclusions, and how you reached them (i.e. the code).\nAs an environment in which to do data science, as a modern-day lab notebook where you can capture not only what you did, but also what you were thinking.\n\nQuarto is a command line interface tool, not an R package. This means that help is, by-and-large, not available through ?. Instead, as you work through this chapter, and use Quarto in the future, you should refer to the Quarto documentation (https://quarto.org/).\n\n\n\n\n\nNote\nQuarto documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more.\nNeed some help?\n\nDownload Quarto: https://quarto.org/docs/get-started/\nQuarto Guide: https://quarto.org/docs/guide/\nMarkdown Reference Sheet: Help &gt; Markdown Quick Reference\n\nYou’ll need the Quarto Command Line Interface but it is automatically done by RStudio for you.\nLet us create one from RStudio now.\nTo create a new Quarto document (.qmd), select File -&gt; New File -&gt; Quarto Document in RStudio, then choose the file type you want to create. For now we will focus on a .html Document, which can be easily converted to other file types later.\nGo ahead and give a title.\nThe newly created .qmd file comes with basic instructions, let us go through it now.\nIt contains three important types of content:\n\nAn (optional) YAML header surrounded by ---\nChunks of R code surrounded by ```\nText mixed with formatting like ## headings and simple text.\n\nYAML stands for yet another markup language or YAML ain’t markup language (a recursive acronym), which emphasizes that YAML is for data, not documents.\nIn any case, it holds the metadata of the document and can be really helpful.\n\n\n\nWhen you render a Quarto document, first knitr executes all of the code chunks and creates a new markdown (.md) document, which includes the code and its output. The markdown file generated is then processed by pandoc, which creates the finished format. The Render button encapsulates these actions and executes them in the right order for you.\n\n\n\n\n\n\n\n\nLearn more about Markdown from the Guide: https://quarto.org/docs/authoring/markdown-basics.html\nWhen you open an .qmd, you get a notebook interface where code and output are interleaved. You can run each code chunk by clicking the Run icon (it looks like a play button at the top of the chunk), or by pressing Ctrl + Shift + Enter.\nRStudio executes the code and displays the results inline with the code by default. However, you can change it to display in the console instead by clicking on the gear icon and changing the Chunk Output in Console option.\n\n\n\nBasic Markdown Syntax and its output\n\n\n\n\n\n\n\nYou can render the entire document with a single click of a button.\nGo ahead and give it a try. RStudio might prompt you to save the document first, save it in your working directory by giving it a suitable title.\nYou should now see some output like this:\n\n\n\nHTML output of the QMD file\n\n\n\n\n\nThe knitr package extends the basic markdown syntax to include chunks of executable R code.\nWhen you render the report, knitr will run the code and add the results to the output file. You can have the output display just the code, just the results, or both.\nTo embed a chunk of R code into your report, surround the code with two lines that each contain three back ticks. After the first set of backticks, include {r}, which alerts knitr that you have included a chunk of R code. The result will look like this:\n\n\n\nR Code Chunk\n\n\nTo omit the results from your final report (and not run the code) add the argument eval = FALSE inside the brackets and after r. This will place a copy of your code into the report.\n\n\n\nR Code Chunk with `eval` set to FALSE\n\n\nTo omit the code from the final report (while including the results) add the argument echo = FALSE. This is very handy for adding plots to a report, since you usually do not want to see the code that generates the plot.\n\n\n\nR Code Chunk with `echo` set to FALSE\n\n\nRead more about R Code Chunks at https://rmarkdown.rstudio.com/articles_intro.html. You can also change this from the gear icon on the right of the code chunk\n\n\nYou can also evaluate R expressions inline by enclosing the expression within a single back-tick qualified with r.\nknitr will replace the inline code with its result in your final document (inline code is always replaced by its result). The result will appear as if it were part of the original text. For example, the snippet above will appear like this:\n\n\n\nInline R code in RMarkdown documents\n\n\n\n\n\nHTML output of the QMD file\n\n\nNow let us try building our own .qmd document and add our own analysis. Let us use a new dataset for this purpose. So go ahead and delete everything below the YAML header.\n\nThe data we are going to use today is the data of deaths due to COVID-19 in Kerala state. This information is available from the Government of Kerala COVID-19 Dashboard https://dashboard.kerala.gov.in/covid/\n\nLets begin!\nWorkflow with Quarto\nCreate a new project and open a new .qmd file in the project.\nLoad Packages\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(rio)\n\nLoad the Data\n\nmortality_df &lt;- rio::import(\n  here(\"data\", \n    \"GoK Dashboard  Official Kerala COVID-19 Statistics.xlsx\"),\n                       skip = 1)\n\nCheck the dimensions of the data\n\nmortality_df |&gt; dim()\n\n[1] 21820     9\n\n\nYou can alternatively use nrow() and ncol().\n\nmortality_df |&gt; nrow()\n\n[1] 21820\n\nmortality_df |&gt; ncol()\n\n[1] 9\n\n\nNow try to use them in the R inline code.\nHint: Use `r ` for inline code chunk like we discussed earlier. Inline R code chunks can be very useful when you are working with data.\nText in Quarto:\n\nThere are `r mortality_df |&gt; nrow()` rows in the data \nand `r mortality_df |&gt; ncol()` columns.\n\nOutput:\nThere are 21820 rows in the data and 9 columns.\nCheck the variable names and clean them\nA good practice is to first check all the variable names and clean them using the clean_names() function from the janitor package\n\nmortality_df |&gt; names()\n\n[1] \"SL No.\"                      \"Date Reported\"              \n[3] \"District\"                    \"Name\"                       \n[5] \"Place\"                       \"Age\"                        \n[7] \"Sex\"                         \"Date of death\"              \n[9] \"History(Traveler / contact)\"\n\n\nLook at the difference in the names() of the dataset once it has been cleaned by janitor\n\nmortality_df |&gt; janitor::clean_names() |&gt;  names()\n\n[1] \"sl_no\"                    \"date_reported\"           \n[3] \"district\"                 \"name\"                    \n[5] \"place\"                    \"age\"                     \n[7] \"sex\"                      \"date_of_death\"           \n[9] \"history_traveler_contact\"\n\n\n\nskimr::skim(mortality_df)\n\nThe skim() function shows that date_reported, date_death, and sex are character variables which might not be ideal. Let us transform them into the data types date and factor. also that history_traveler_contact are mostly NA.\nLet us drop the column history_traveler_contact, name and place from our analysis\n\nmortality_df &lt;- mortality_df |&gt; \n  select(-c(history_traveler_contact, name, place))\n\nLets check the class of date_reported.\n\nmortality_df |&gt; pull(date_reported) |&gt; class()\n\n[1] \"character\"\n\n\nLets do some more cleaning of the variables\nWhen working with dates, the lubridate package is ideal.\n\nlibrary(lubridate)\nmortality_df &lt;- mortality_df |&gt; \n  mutate(date_reported = lubridate::dmy(date_reported))\n\nLets check the class of date_reported now\n\nmortality_df |&gt; \n  pull(date_reported) |&gt; \n  class()\n\n[1] \"Date\"\n\n\npull() is an excellent funtion that lets you pull a single varible from a dataset and perform operations. Read more about pull() in the Help menu.\nLet us now look at the sex variable.\n\nmortality_df |&gt; \n  pull(sex) |&gt;\n  unique()\n\n[1] \"Male\"   \"Female\" \"Others\" \"-\"      \"gf\"     \"male\"   NA      \n\n\nAfter some mutate() magic…\n\nmortality_df |&gt; \n  mutate(sex = fct_collapse(sex, Male = \"male\")) |&gt; \n  pull(sex) |&gt; \n  unique()\n\n[1] Male   Female Others -      gf     &lt;NA&gt;  \nLevels: - Female gf Male Others\n\n\nWe can pipe multiple mutate() functions too..\n\nmortality_df &lt;- mortality_df |&gt; \n  mutate(date_of_death = lubridate::dmy(date_of_death)) |&gt; \n  mutate(sex = fct_collapse(sex, Male = \"male\")) |&gt; \n  mutate(sex = factor(sex, levels = c(\"Male\", \"Female\")))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `date_of_death = lubridate::dmy(date_of_death)`.\nCaused by warning:\n!  47 failed to parse.\n\n\nLet us drop_na() for now\n\nmortality_df &lt;- mortality_df |&gt; drop_na()\n\nLet us look at the number of rows now\n\n mortality_df |&gt; nrow()\n\n[1] 21609\n\n\nLet us look at the Districts\n\nmortality_df |&gt; pull(district) |&gt; unique() \n\n [1] \"Thiruvananthapuram\"   \"Kollam\"               \"Pathanamthitta\"      \n [4] \"Kottayam\"             \"Idukki\"               \"Ernakulam\"           \n [7] \"Thrissur\"             \"Palakkad\"             \"Malappuram\"          \n[10] \"Kozhikode\"            \"Wayanad\"              \"Kasaragod\"           \n[13] \"Alappuzha\"            \"Kannur\"               \"Thiruvananthapura m\" \n[16] \"THIRUVANANTHAPURAM\"   \"KOLLAM\"               \"PATHANAMTHITTA\"      \n[19] \"ALAPPUZHA\"            \"KOTTAYAM\"             \"IDUKKI\"              \n[22] \"ERNAKULAM\"            \"THRISSUR\"             \"PALAKKAD\"            \n[25] \"MALAPPURAM\"           \"KOZHIKODE\"            \"WAYANAD\"             \n[28] \"KANNUR\"               \"KASARAGOD\"            \"Kasargod\"            \n[31] \"Thiruvananthapuram?K\" \"Alappuzha?Kannur\"     \"Kollam?Thiruvanantha\"\n[34] \"Malappuaram\"          \"Kozhikode?Thiruvanan\" \"Kozhikode?Ernakulam\" \n[37] \"Eranakulam\"          \n\n\nLet us clean it\n\nmortality_df &lt;- mortality_df |&gt; \n mutate(district = str_to_sentence(district)) |&gt; \n  mutate(district = fct_collapse(district,\n                                 Thiruvananthapuram = c(\n                                   \"Thiruvananthapura m\",\n                                   \"Thiruvananthapuram?K\"))) |&gt; \n  mutate(district = fct_collapse(district, \n                                 Kollam = c(\n                                   \"Kollam?Thiruvanantha\"))) |&gt; \n  mutate(district = fct_collapse(district, \n                                 Ernakulam = c(\n                                   \"Eranakulam\"))) |&gt; \n  mutate(district = fct_collapse(district, \n                                 Kasaragod = c(\n                                   \"Kasargod\"))) |&gt; \n  mutate(district = fct_collapse(district, \n                                 Kozhikode = c(\n                                   \"Kozhikode?Ernakulam\", \n                                   \"Kozhikode?Thiruvanan\"))) |&gt; \n  mutate(district = fct_collapse(district, \n                                 Alappuzha = c(\n                                   \"Alappuzha?Kannur\"))) |&gt; \n  mutate(district = fct_collapse(district, \n                                 Malappuram = c(\n                                   \"Malappuaram\"))) \n\nLet us look at the number of Districts now\n\nmortality_df |&gt; pull(district) |&gt; unique() |&gt; length()\n\n[1] 14\n\n\nLet us now create a new variable called wave. This will tell us if the death has happened in the first wave or second wave of COVID-19.\nFor the workshop’s sake, let us consider April, 2021 as the cut off date for the first wave and second waves of COVID-19 in Kerala.\n\nmortality_df &lt;- mortality_df |&gt; \n   mutate(wave = if_else(date_of_death &lt;= \"2021-04-01\", \n                        \"First Wave\", \n                        \"Second Wave\"))  \n\nLet us create age_group variable\n\nmortality_df &lt;- mortality_df |&gt; mutate(age_group = case_when(\n  age &lt; 60 ~ \"&lt;60 Years\", TRUE ~ \"&gt;60 Years\"))\n\nDistribution of Age and Gender\nLets look at the distribution age and sex among COVID-19 deaths in Kerala\n\nmortality_df |&gt; pull(age) |&gt; summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0      59      68      67      76     121 \n\nmortality_df |&gt; pull(sex) |&gt; factor() |&gt; summary()\n\n  Male Female \n 12777   8832 \n\nmortality_df |&gt; group_by(sex) |&gt; summarize(mean(age), sd(age))\n\n# A tibble: 2 × 3\n  sex    `mean(age)` `sd(age)`\n  &lt;fct&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 Male          66.3      13.6\n2 Female        68.0      14.2\n\n\nUsing gtsummary\nlibrary(gtsummary)\nage_sex_table &lt;- mortality_df |&gt;\n  dplyr::select(age, sex) |&gt;  \n  tbl_summary(by = sex) |&gt; \n  add_p()\n\n# using the {gt} package\nas_gt(age_sex_table) |&gt; gt::as_latex()\nUsing the inline R code you can:\n\nThe median (IQR) age (in years) among males and females are \n`r inline_text(age_sex_table, variable = age, \ncolumn = \"Male\")` \nand `r inline_text(age_sex_table, \nvariable = age, column = \"Female\"), respectively.\n\nOutput:\nThe median (IQR) age (in years) among males and females are 68 (58, 75) and 70 (60, 78) , respectively.\nVisualize using ggplot2\n\nmortality_df |&gt; \n  ggplot(aes(x = sex, y = age)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nDistribution of Age groups and Waves\nage_group_wave_table &lt;- mortality_df |&gt; \n  dplyr::select(age_group, wave) |&gt;  \n  tbl_summary(by = wave) |&gt; \n  add_p()\n\n# using the {gt} package\nas_gt(age_group_wave_table) |&gt; gt::as_latex()\nUsing the inline R code you can:\n\nThe number of deaths in the First wave and Second wave of COVID-19 are  \n`r inline_text(age_group_wave_table, variable = age_group, \nlevel = \"&lt;60 Years\", column = \"First Wave\")` and \n`r inline_text(age_group_wave_table, variable = age_group, \nlevel = \"&gt;60 Years\", column = \"Second Wave\")` , respectively.\n\nOutput:\nThe number of deaths in the First wave and Second wave of COVID-19 are 1,079 (24%) and 12,467 (73%) , respectively.\nVisualize using ggplot2\n\nmortality_df |&gt; \n  ggplot(aes(x = wave, fill = age_group)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\nLets make more sense from this plot with some mutate() magic again..\n\ndf &lt;- mortality_df |&gt; \n  count(wave, age_group) |&gt; \n  na.omit() |&gt; \n  group_by(wave) |&gt; \n  mutate(prop = (n / sum(n))*100) |&gt; \n  ungroup()\n\n\ndf |&gt; \n  ggplot(aes(x = wave, y = prop,  fill = age_group)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")\n\n\n\n\n\n\n\n\nNow let us render this!\n\n\n\n\n\nQuarto is awesome.\n\nThe ratio of markup to content is excellent.\nFor exploratory analyses, blog posts, and interactive documents\nFor journal articles, though knowledge on will be helpful.\n\nThe RStudio team have made the whole process very user friendly.\n\nRStudio provides useful short cut keys for compiling to HTML, and running code chunks.\nThese shortcut keys are presented in a clear way.\nCode completion on R code chunk options is really helpful. See also chunk options documentation on the knitr website."
  },
  {
    "objectID": "quarto.html#introduction",
    "href": "quarto.html#introduction",
    "title": "Communicating Research with Quarto",
    "section": "",
    "text": "Quarto provides a unified authoring framework for data science, combining your code, its results, and your prose. Quarto documents are fully reproducible and support dozens of output formats, like PDFs, Word files, presentations, and more.\n\n\n\n\n\nQuarto files are designed to be used in three ways:\n\nFor communicating to decision-makers, who want to focus on the conclusions, not the code behind the analysis.\nFor collaborating with other data scientists (including future you!), who are interested in both your conclusions, and how you reached them (i.e. the code).\nAs an environment in which to do data science, as a modern-day lab notebook where you can capture not only what you did, but also what you were thinking.\n\nQuarto is a command line interface tool, not an R package. This means that help is, by-and-large, not available through ?. Instead, as you work through this chapter, and use Quarto in the future, you should refer to the Quarto documentation (https://quarto.org/).\n\n\n\n\n\nNote\nQuarto documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more.\nNeed some help?\n\nDownload Quarto: https://quarto.org/docs/get-started/\nQuarto Guide: https://quarto.org/docs/guide/\nMarkdown Reference Sheet: Help &gt; Markdown Quick Reference\n\nYou’ll need the Quarto Command Line Interface but it is automatically done by RStudio for you.\nLet us create one from RStudio now.\nTo create a new Quarto document (.qmd), select File -&gt; New File -&gt; Quarto Document in RStudio, then choose the file type you want to create. For now we will focus on a .html Document, which can be easily converted to other file types later.\nGo ahead and give a title.\nThe newly created .qmd file comes with basic instructions, let us go through it now.\nIt contains three important types of content:\n\nAn (optional) YAML header surrounded by ---\nChunks of R code surrounded by ```\nText mixed with formatting like ## headings and simple text.\n\nYAML stands for yet another markup language or YAML ain’t markup language (a recursive acronym), which emphasizes that YAML is for data, not documents.\nIn any case, it holds the metadata of the document and can be really helpful."
  },
  {
    "objectID": "quarto.html#how-does-quarto-work",
    "href": "quarto.html#how-does-quarto-work",
    "title": "Communicating Research with Quarto",
    "section": "",
    "text": "When you render a Quarto document, first knitr executes all of the code chunks and creates a new markdown (.md) document, which includes the code and its output. The markdown file generated is then processed by pandoc, which creates the finished format. The Render button encapsulates these actions and executes them in the right order for you."
  },
  {
    "objectID": "quarto.html#some-basics-of-the-markdown-syntax",
    "href": "quarto.html#some-basics-of-the-markdown-syntax",
    "title": "Communicating Research with Quarto",
    "section": "",
    "text": "Learn more about Markdown from the Guide: https://quarto.org/docs/authoring/markdown-basics.html\nWhen you open an .qmd, you get a notebook interface where code and output are interleaved. You can run each code chunk by clicking the Run icon (it looks like a play button at the top of the chunk), or by pressing Ctrl + Shift + Enter.\nRStudio executes the code and displays the results inline with the code by default. However, you can change it to display in the console instead by clicking on the gear icon and changing the Chunk Output in Console option.\n\n\n\nBasic Markdown Syntax and its output\n\n\n\n\n\n\n\nYou can render the entire document with a single click of a button.\nGo ahead and give it a try. RStudio might prompt you to save the document first, save it in your working directory by giving it a suitable title.\nYou should now see some output like this:\n\n\n\nHTML output of the QMD file"
  },
  {
    "objectID": "quarto.html#code-chunks",
    "href": "quarto.html#code-chunks",
    "title": "Communicating Research with Quarto",
    "section": "",
    "text": "The knitr package extends the basic markdown syntax to include chunks of executable R code.\nWhen you render the report, knitr will run the code and add the results to the output file. You can have the output display just the code, just the results, or both.\nTo embed a chunk of R code into your report, surround the code with two lines that each contain three back ticks. After the first set of backticks, include {r}, which alerts knitr that you have included a chunk of R code. The result will look like this:\n\n\n\nR Code Chunk\n\n\nTo omit the results from your final report (and not run the code) add the argument eval = FALSE inside the brackets and after r. This will place a copy of your code into the report.\n\n\n\nR Code Chunk with `eval` set to FALSE\n\n\nTo omit the code from the final report (while including the results) add the argument echo = FALSE. This is very handy for adding plots to a report, since you usually do not want to see the code that generates the plot.\n\n\n\nR Code Chunk with `echo` set to FALSE\n\n\nRead more about R Code Chunks at https://rmarkdown.rstudio.com/articles_intro.html. You can also change this from the gear icon on the right of the code chunk\n\n\nYou can also evaluate R expressions inline by enclosing the expression within a single back-tick qualified with r.\nknitr will replace the inline code with its result in your final document (inline code is always replaced by its result). The result will appear as if it were part of the original text. For example, the snippet above will appear like this:\n\n\n\nInline R code in RMarkdown documents\n\n\n\n\n\nHTML output of the QMD file\n\n\nNow let us try building our own .qmd document and add our own analysis. Let us use a new dataset for this purpose. So go ahead and delete everything below the YAML header.\n\nThe data we are going to use today is the data of deaths due to COVID-19 in Kerala state. This information is available from the Government of Kerala COVID-19 Dashboard https://dashboard.kerala.gov.in/covid/\n\nLets begin!\nWorkflow with Quarto\nCreate a new project and open a new .qmd file in the project.\nLoad Packages\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(rio)\n\nLoad the Data\n\nmortality_df &lt;- rio::import(\n  here(\"data\", \n    \"GoK Dashboard  Official Kerala COVID-19 Statistics.xlsx\"),\n                       skip = 1)\n\nCheck the dimensions of the data\n\nmortality_df |&gt; dim()\n\n[1] 21820     9\n\n\nYou can alternatively use nrow() and ncol().\n\nmortality_df |&gt; nrow()\n\n[1] 21820\n\nmortality_df |&gt; ncol()\n\n[1] 9\n\n\nNow try to use them in the R inline code.\nHint: Use `r ` for inline code chunk like we discussed earlier. Inline R code chunks can be very useful when you are working with data.\nText in Quarto:\n\nThere are `r mortality_df |&gt; nrow()` rows in the data \nand `r mortality_df |&gt; ncol()` columns.\n\nOutput:\nThere are 21820 rows in the data and 9 columns.\nCheck the variable names and clean them\nA good practice is to first check all the variable names and clean them using the clean_names() function from the janitor package\n\nmortality_df |&gt; names()\n\n[1] \"SL No.\"                      \"Date Reported\"              \n[3] \"District\"                    \"Name\"                       \n[5] \"Place\"                       \"Age\"                        \n[7] \"Sex\"                         \"Date of death\"              \n[9] \"History(Traveler / contact)\"\n\n\nLook at the difference in the names() of the dataset once it has been cleaned by janitor\n\nmortality_df |&gt; janitor::clean_names() |&gt;  names()\n\n[1] \"sl_no\"                    \"date_reported\"           \n[3] \"district\"                 \"name\"                    \n[5] \"place\"                    \"age\"                     \n[7] \"sex\"                      \"date_of_death\"           \n[9] \"history_traveler_contact\"\n\n\n\nskimr::skim(mortality_df)\n\nThe skim() function shows that date_reported, date_death, and sex are character variables which might not be ideal. Let us transform them into the data types date and factor. also that history_traveler_contact are mostly NA.\nLet us drop the column history_traveler_contact, name and place from our analysis\n\nmortality_df &lt;- mortality_df |&gt; \n  select(-c(history_traveler_contact, name, place))\n\nLets check the class of date_reported.\n\nmortality_df |&gt; pull(date_reported) |&gt; class()\n\n[1] \"character\"\n\n\nLets do some more cleaning of the variables\nWhen working with dates, the lubridate package is ideal.\n\nlibrary(lubridate)\nmortality_df &lt;- mortality_df |&gt; \n  mutate(date_reported = lubridate::dmy(date_reported))\n\nLets check the class of date_reported now\n\nmortality_df |&gt; \n  pull(date_reported) |&gt; \n  class()\n\n[1] \"Date\"\n\n\npull() is an excellent funtion that lets you pull a single varible from a dataset and perform operations. Read more about pull() in the Help menu.\nLet us now look at the sex variable.\n\nmortality_df |&gt; \n  pull(sex) |&gt;\n  unique()\n\n[1] \"Male\"   \"Female\" \"Others\" \"-\"      \"gf\"     \"male\"   NA      \n\n\nAfter some mutate() magic…\n\nmortality_df |&gt; \n  mutate(sex = fct_collapse(sex, Male = \"male\")) |&gt; \n  pull(sex) |&gt; \n  unique()\n\n[1] Male   Female Others -      gf     &lt;NA&gt;  \nLevels: - Female gf Male Others\n\n\nWe can pipe multiple mutate() functions too..\n\nmortality_df &lt;- mortality_df |&gt; \n  mutate(date_of_death = lubridate::dmy(date_of_death)) |&gt; \n  mutate(sex = fct_collapse(sex, Male = \"male\")) |&gt; \n  mutate(sex = factor(sex, levels = c(\"Male\", \"Female\")))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `date_of_death = lubridate::dmy(date_of_death)`.\nCaused by warning:\n!  47 failed to parse.\n\n\nLet us drop_na() for now\n\nmortality_df &lt;- mortality_df |&gt; drop_na()\n\nLet us look at the number of rows now\n\n mortality_df |&gt; nrow()\n\n[1] 21609\n\n\nLet us look at the Districts\n\nmortality_df |&gt; pull(district) |&gt; unique() \n\n [1] \"Thiruvananthapuram\"   \"Kollam\"               \"Pathanamthitta\"      \n [4] \"Kottayam\"             \"Idukki\"               \"Ernakulam\"           \n [7] \"Thrissur\"             \"Palakkad\"             \"Malappuram\"          \n[10] \"Kozhikode\"            \"Wayanad\"              \"Kasaragod\"           \n[13] \"Alappuzha\"            \"Kannur\"               \"Thiruvananthapura m\" \n[16] \"THIRUVANANTHAPURAM\"   \"KOLLAM\"               \"PATHANAMTHITTA\"      \n[19] \"ALAPPUZHA\"            \"KOTTAYAM\"             \"IDUKKI\"              \n[22] \"ERNAKULAM\"            \"THRISSUR\"             \"PALAKKAD\"            \n[25] \"MALAPPURAM\"           \"KOZHIKODE\"            \"WAYANAD\"             \n[28] \"KANNUR\"               \"KASARAGOD\"            \"Kasargod\"            \n[31] \"Thiruvananthapuram?K\" \"Alappuzha?Kannur\"     \"Kollam?Thiruvanantha\"\n[34] \"Malappuaram\"          \"Kozhikode?Thiruvanan\" \"Kozhikode?Ernakulam\" \n[37] \"Eranakulam\"          \n\n\nLet us clean it\n\nmortality_df &lt;- mortality_df |&gt; \n mutate(district = str_to_sentence(district)) |&gt; \n  mutate(district = fct_collapse(district,\n                                 Thiruvananthapuram = c(\n                                   \"Thiruvananthapura m\",\n                                   \"Thiruvananthapuram?K\"))) |&gt; \n  mutate(district = fct_collapse(district, \n                                 Kollam = c(\n                                   \"Kollam?Thiruvanantha\"))) |&gt; \n  mutate(district = fct_collapse(district, \n                                 Ernakulam = c(\n                                   \"Eranakulam\"))) |&gt; \n  mutate(district = fct_collapse(district, \n                                 Kasaragod = c(\n                                   \"Kasargod\"))) |&gt; \n  mutate(district = fct_collapse(district, \n                                 Kozhikode = c(\n                                   \"Kozhikode?Ernakulam\", \n                                   \"Kozhikode?Thiruvanan\"))) |&gt; \n  mutate(district = fct_collapse(district, \n                                 Alappuzha = c(\n                                   \"Alappuzha?Kannur\"))) |&gt; \n  mutate(district = fct_collapse(district, \n                                 Malappuram = c(\n                                   \"Malappuaram\"))) \n\nLet us look at the number of Districts now\n\nmortality_df |&gt; pull(district) |&gt; unique() |&gt; length()\n\n[1] 14\n\n\nLet us now create a new variable called wave. This will tell us if the death has happened in the first wave or second wave of COVID-19.\nFor the workshop’s sake, let us consider April, 2021 as the cut off date for the first wave and second waves of COVID-19 in Kerala.\n\nmortality_df &lt;- mortality_df |&gt; \n   mutate(wave = if_else(date_of_death &lt;= \"2021-04-01\", \n                        \"First Wave\", \n                        \"Second Wave\"))  \n\nLet us create age_group variable\n\nmortality_df &lt;- mortality_df |&gt; mutate(age_group = case_when(\n  age &lt; 60 ~ \"&lt;60 Years\", TRUE ~ \"&gt;60 Years\"))\n\nDistribution of Age and Gender\nLets look at the distribution age and sex among COVID-19 deaths in Kerala\n\nmortality_df |&gt; pull(age) |&gt; summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0      59      68      67      76     121 \n\nmortality_df |&gt; pull(sex) |&gt; factor() |&gt; summary()\n\n  Male Female \n 12777   8832 \n\nmortality_df |&gt; group_by(sex) |&gt; summarize(mean(age), sd(age))\n\n# A tibble: 2 × 3\n  sex    `mean(age)` `sd(age)`\n  &lt;fct&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 Male          66.3      13.6\n2 Female        68.0      14.2\n\n\nUsing gtsummary\nlibrary(gtsummary)\nage_sex_table &lt;- mortality_df |&gt;\n  dplyr::select(age, sex) |&gt;  \n  tbl_summary(by = sex) |&gt; \n  add_p()\n\n# using the {gt} package\nas_gt(age_sex_table) |&gt; gt::as_latex()\nUsing the inline R code you can:\n\nThe median (IQR) age (in years) among males and females are \n`r inline_text(age_sex_table, variable = age, \ncolumn = \"Male\")` \nand `r inline_text(age_sex_table, \nvariable = age, column = \"Female\"), respectively.\n\nOutput:\nThe median (IQR) age (in years) among males and females are 68 (58, 75) and 70 (60, 78) , respectively.\nVisualize using ggplot2\n\nmortality_df |&gt; \n  ggplot(aes(x = sex, y = age)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nDistribution of Age groups and Waves\nage_group_wave_table &lt;- mortality_df |&gt; \n  dplyr::select(age_group, wave) |&gt;  \n  tbl_summary(by = wave) |&gt; \n  add_p()\n\n# using the {gt} package\nas_gt(age_group_wave_table) |&gt; gt::as_latex()\nUsing the inline R code you can:\n\nThe number of deaths in the First wave and Second wave of COVID-19 are  \n`r inline_text(age_group_wave_table, variable = age_group, \nlevel = \"&lt;60 Years\", column = \"First Wave\")` and \n`r inline_text(age_group_wave_table, variable = age_group, \nlevel = \"&gt;60 Years\", column = \"Second Wave\")` , respectively.\n\nOutput:\nThe number of deaths in the First wave and Second wave of COVID-19 are 1,079 (24%) and 12,467 (73%) , respectively.\nVisualize using ggplot2\n\nmortality_df |&gt; \n  ggplot(aes(x = wave, fill = age_group)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\nLets make more sense from this plot with some mutate() magic again..\n\ndf &lt;- mortality_df |&gt; \n  count(wave, age_group) |&gt; \n  na.omit() |&gt; \n  group_by(wave) |&gt; \n  mutate(prop = (n / sum(n))*100) |&gt; \n  ungroup()\n\n\ndf |&gt; \n  ggplot(aes(x = wave, y = prop,  fill = age_group)) +\n  geom_bar(position = \"dodge\", stat = \"identity\")\n\n\n\n\n\n\n\n\nNow let us render this!"
  },
  {
    "objectID": "quarto.html#conclusion",
    "href": "quarto.html#conclusion",
    "title": "Communicating Research with Quarto",
    "section": "",
    "text": "Quarto is awesome.\n\nThe ratio of markup to content is excellent.\nFor exploratory analyses, blog posts, and interactive documents\nFor journal articles, though knowledge on will be helpful.\n\nThe RStudio team have made the whole process very user friendly.\n\nRStudio provides useful short cut keys for compiling to HTML, and running code chunks.\nThese shortcut keys are presented in a clear way.\nCode completion on R code chunk options is really helpful. See also chunk options documentation on the knitr website."
  },
  {
    "objectID": "spatial_data_mani.html",
    "href": "spatial_data_mani.html",
    "title": "Spatial Data Manipulation",
    "section": "",
    "text": "It is expected that by end of the session, students will be able to understand as well as apply varied methods for manipulation of spatial datasets.\n\n\n\n\n\n\nSimple features are open standard objects which have been endorsed by Open Geospatial Consortium. They represent heirarchial data model for 17 geometry types.\nDuring this session and in subsequent session on Spatial Epidemiology, we shall be using sf package. This is because sf package has inbuilt capabilities of handling 7 most common data models of simple features. Further,R spatial ecosystem is more efficient with use of sf package compared to previous packages.\nShapefile format data can be imported into the R ecosystem using st_read() or read_sf() function. Both are similar in function with subtle differences. read_sf() chooses a few tidyverse defaults such as silent (st_read() gives a short report), returns a spatial tibble (st_read() returns a dataframe), doesnot convert strings to factors (st_read() converts to factors by default) and accepts list columns as input. For other formats st_as_sf() contains a family of conversion functions.\n\n\n\n\n\n\n\n\nTo know the crs of a spatial dataset, use sf::st_crs() function.\n\n\n\n\n\nWhen we have two or more related spatial datasets, and the CRS are different, we can transform the CRS using sf::st_transform() function.\n\n\n\n\n\n#Install packages. We need to install packages only once in the system.\n#install.packages(\"sf\")\n#install.packages(\"tidyverse\")\n#Load library. We need to load library everytime to use the functions\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\n\n\n\n\nworld &lt;- read_rds(here::here(\"data\", \"world_india_compliant.rds\"))\n\nst_crs(world)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\n\nindia &lt;- read_rds(here::here( \"data\", \"india_states.rds\"))\n\nindia &lt;- india |&gt; \n  janitor::clean_names()\n\nst_crs(india)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"unknown\"],\n        AREA[\"World - 85Â°S to 85Â°N\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\n# Transform to Geog CRS as that of world map\n\nindia &lt;- st_transform(india, crs = 4326)\n\n\n\n\n\nkerala &lt;- read_rds(here::here(\"data\",\"ker_panchayats.rds\"))\n\n# Clean names\n\nkerala &lt;- kerala |&gt; \n  janitor::clean_names()\n\n# Check CRS\nst_crs(kerala)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"unknown\"],\n        AREA[\"World\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\n\n# Reading RDS\ndf &lt;- read_rds(here::here( \"data\", \"dengue_sample.rds\"))\n\n# Cleaning names \ndf &lt;- df |&gt; \n  janitor::clean_names()\n\n# Converting as sf object\ndf &lt;- st_as_sf(df, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\n\n\n\n\n\n\nThe process of selecting only those locations which have a specified attribute is known as sub-setting and the output is subset. It is done to understand spatial distribution of a given variable and thus identify underlying patterns. sf objects have inherent dataframe class. Hence, tidyverse::filter() function can be used for obtaining spatial subsets.\n\n# Example: Subset district from Kerala map whose name is Thiruvananthapuram\n\ntvm &lt;- kerala %&gt;% \n  filter(district == \"Thiruvananthapuram\")\n\n#Overview\ntvm %&gt;% \n  ggplot()+\n  geom_sf()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopology is defined as spatial relationship between the objects. These include relationships such as common boundaries, intersections, one within the other, crossings, disjunctions, etc.\n\n# To identify block from which cases have been reported: \n#It will include all cases which are within as well as \n#on the boundary of a polygon.\n\n(list &lt;- st_intersects(df, tvm))\n\nSparse geometry binary predicate list of length 250, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: 64\n 2: 52\n 3: 68\n 4: 52\n 5: 64\n 6: 57\n 7: 53\n 8: 52\n 9: 52\n 10: 52\n\n\nSimilarly, st_within() is used to find the relationship for those strictly within boundaries, st_disjoint() is opposite of st_intersect(), st_touches() is for those objects which are touching each other (eg. polygons) and st_within_distance() for the objects within a buffer distance from the said object.\n\n\n\n\n\nPresence of a unique key is pre-requisite to joining of datasets. The same concept is utilised for overlay or joining of two or more spatial datasets with location coordinates as unique key. st_join() performs spatial joins using location coordinates as unique key. By default, left join is undertaken. in is performed.\n\ndim(df)\n\n[1] 250  19\n\nnames(df)\n\n [1] \"hash_short\"       \"age\"              \"sex1\"             \"day\"             \n [5] \"month\"            \"year\"             \"date\"             \"panchayat_code\"  \n [9] \"population\"       \"dengue\"           \"month2\"           \"dengue1\"         \n[13] \"join_count_n_9_0\" \"area\"             \"aream2\"           \"popden\"          \n[17] \"dengpop\"          \"dengpopdens\"      \"geometry\"        \n\ndim(tvm)\n\n[1] 83 46\n\nnames(tvm)\n\n [1] \"join_count\"   \"area\"         \"code\"         \"panchayat\"    \"block\"       \n [6] \"municipal\"    \"corporate\"    \"district\"     \"location\"     \"state_id\"    \n[11] \"district_id\"  \"hospital_id\"  \"nameof_hos\"   \"typeof_hos\"   \"place_name\"  \n[16] \"d\"            \"m\"            \"s\"            \"x\"            \"d1\"          \n[21] \"m1\"           \"s1\"           \"y\"            \"postoffice\"   \"ho_sattach\"  \n[26] \"pincode\"      \"lo_body_name\" \"lo_body_type\" \"phone_no\"     \"bed_count\"   \n[31] \"timing\"       \"ambu_servi\"   \"cr_savailab\"  \"cr_snameadd\"  \"ho_stiming\"  \n[36] \"doctors\"      \"nurses\"       \"paramedics\"   \"ambient_ser\"  \"waste_dispo\" \n[41] \"facility\"     \"service\"      \"depart_name\"  \"no_of_asha\"   \"no_of_jphn\"  \n[46] \"geometry\"    \n\njoined &lt;- st_join(df, tvm)\n\ndim(joined)\n\n[1] 250  64\n\nnames(joined)\n\n [1] \"hash_short\"       \"age\"              \"sex1\"             \"day\"             \n [5] \"month\"            \"year\"             \"date\"             \"panchayat_code\"  \n [9] \"population\"       \"dengue\"           \"month2\"           \"dengue1\"         \n[13] \"join_count_n_9_0\" \"area.x\"           \"aream2\"           \"popden\"          \n[17] \"dengpop\"          \"dengpopdens\"      \"geometry\"         \"join_count\"      \n[21] \"area.y\"           \"code\"             \"panchayat\"        \"block\"           \n[25] \"municipal\"        \"corporate\"        \"district\"         \"location\"        \n[29] \"state_id\"         \"district_id\"      \"hospital_id\"      \"nameof_hos\"      \n[33] \"typeof_hos\"       \"place_name\"       \"d\"                \"m\"               \n[37] \"s\"                \"x\"                \"d1\"               \"m1\"              \n[41] \"s1\"               \"y\"                \"postoffice\"       \"ho_sattach\"      \n[45] \"pincode\"          \"lo_body_name\"     \"lo_body_type\"     \"phone_no\"        \n[49] \"bed_count\"        \"timing\"           \"ambu_servi\"       \"cr_savailab\"     \n[53] \"cr_snameadd\"      \"ho_stiming\"       \"doctors\"          \"nurses\"          \n[57] \"paramedics\"       \"ambient_ser\"      \"waste_dispo\"      \"facility\"        \n[61] \"service\"          \"depart_name\"      \"no_of_asha\"       \"no_of_jphn\"      \n\njoined %&gt;% ggplot()+\n  geom_sf()\n\n\n\n\n\n\n\n\n\n\n\n\n\nAggregation of spatial data is a process of condensing the dataset to obtain stratified summary measures.\n\n# Example: To get list of number of cases from each block\njoined %&gt;% \n  group_by(block) %&gt;% \n  summarise(cases = sum(dengue))\n\nSimple feature collection with 13 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 76.71811 ymin: 8.331344 xmax: 77.19514 ymax: 8.811173\nGeodetic CRS:  WGS 84\n# A tibble: 13 × 3\n   block          cases                                                 geometry\n   &lt;chr&gt;          &lt;dbl&gt;                                           &lt;GEOMETRY [°]&gt;\n 1 Athiyannoor        7 MULTIPOINT ((76.97908 8.400357), (76.97289 8.412542), (…\n 2 Chirayinkeezhu     2    MULTIPOINT ((76.78766 8.689444), (76.78253 8.657362))\n 3 Kazhakkoottam     13 MULTIPOINT ((76.81978 8.603385), (76.92593 8.573853), (…\n 4 Kilimanoor         2    MULTIPOINT ((76.87685 8.766624), (76.82115 8.811173))\n 5 Nedumangadu       11 MULTIPOINT ((77.02653 8.549563), (77.0238 8.549926), (7…\n 6 Nemom             38 MULTIPOINT ((76.98578 8.422667), (77.0061 8.439469), (7…\n 7 Parasala           1                                POINT (77.08794 8.331344)\n 8 Perumkadavila      5 MULTIPOINT ((77.19514 8.453647), (77.15896 8.461809), (…\n 9 T. Rural Block     7 MULTIPOINT ((76.96103 8.556535), (76.96095 8.555607), (…\n10 Vamanapuram        1                                POINT (76.92178 8.677913)\n11 Varkala            1                                 POINT (76.8104 8.732897)\n12 Vellanadu         19 MULTIPOINT ((77.07907 8.502114), (77.07872 8.507087), (…\n13 &lt;NA&gt;             143 MULTIPOINT ((76.96146 8.426247), (76.9614 8.426524), (7…\n\n\n\n\n\n\nTo determine area of a polygon, st_area() function is used.\n\n#Example\ntvm %&gt;% \n  filter(panchayat == \"Pallickal\") %&gt;% \n  st_area()\n\n17038202 [m^2]\n\n\n\n\n\n\n\nWe can transform geometries with uniary and binary transformations. Uniary transformations are based on a single geometry in isolation and includes measures such as creation of buffers and centroids. Binary operations modify the shape of one geometry based on the other and includes measures such as clipping and geometry unions.\n\n\n\n\nThe centroid based processes identify the center of a sf object and is similar to measure of central tendency. There are many variations in ways to define centre of an sf object, however, use of geographical centre remains most common. st_centroid() provides centroid values.\n\npolygon_tvm &lt;- tvm %&gt;% \n  ggplot()+\n  geom_sf()+\n  labs(title = \"Map\")\n\npoint_tvm &lt;- st_centroid(tvm) %&gt;%  \n  ggplot()+\n  geom_sf()+\n  labs(title = \"Centroids\")\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nggpubr::ggarrange(polygon_tvm, point_tvm)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntvm &lt;- cbind(tvm, st_coordinates(st_centroid(tvm)))\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\n\n\n\n\nggplot()+\n  geom_sf(data = tvm %&gt;% \n  filter(panchayat == \"Pallickal\") %&gt;% \n  st_centroid() %&gt;% \n  st_buffer(dist = 5000) )+\n  geom_sf(data = df)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThough st_write() function helps in saving spatial datasets as shapefiles, it is recommended that RDS data format be used as it uses less space in the system and is computationally faster to load subsequently.\n\n\n\n\n\n\n\nTo know as well as to set CRS of a raster data, raster::projections() function is used. The further difference as compared to vector data is that CRS has to be provided in proj4 definition format for the raster datasets. It does not accept CRS input in espg format.\n\n\n\n\n\nSimilar to area calculations in a vector dataset, we would like to know about the resolution of the raster data. The res() function provides dimensions of each of the cell of the raster grid in metres (By default, UTM projection is in metres).\n\n\n\n\n\nTo extract value for a specified location raster::cellFromXY() function can be used.\nTo subset based on extent, the bounding box details can be used.\nTo subset based on extent of a polygon/ other available vector dataset\n\n\n\n\n\n\nTo extract values for a given location, there are multiple methods. The raster::extract() function is computationally demanding and takes a lot of time, especially for extracting raster data for a large number of polygons. An alternative from terra package as well as veloxraster appears promising but needs further deliberations.\n\n\n\n\nUnderstanding of map algebra makes raster processing faster. There are four subclasses of raster based operations as under:-\n\n\n\n\nThese operations act independently for each cell of the raster.\n\n\n\n\nAlso known a kernel, moving window or filter operations for raster data, they choose a central cell and its neighbourhood (such as surrounding eight cells, for example). The focal operation acts similar to spatial data aggregation and results in an output of aggregated values for each combination of central cell and its neighbourhood. raster::focal() function can be used for obtaining focal values by defining the matrix for central cell and its neighbourhood.\n\n\n\n\n\nThe principle behind zonal operations is same as that of focal operations. However, unlike in focal operations, zonal operations can go beyond a rectangular matrix of neighbourhood and can provide aggregated values for a pre-defined shape or distance or area, etc. The raster::zonal() function provides zonal statistics.\n\n\n\n\n\nThese operations are applied on the entire dataset and are used to summarise the characteristics of the given raster data such as mean, median, IQR, range etc of the datasets.\n\n\n\n\n\n\nTwo rasters can be merged using raster::merge() function in general. However, depending upon the raster characteristics, more refined methods are available such as from packages landsat, satellite, etc. Merging rasters require same resolution and crs of the rasters as prerequisites."
  },
  {
    "objectID": "spatial_data_mani.html#importing-spatial-data-as-sf-object-into-r.",
    "href": "spatial_data_mani.html#importing-spatial-data-as-sf-object-into-r.",
    "title": "Spatial Data Manipulation",
    "section": "",
    "text": "Simple features are open standard objects which have been endorsed by Open Geospatial Consortium. They represent heirarchial data model for 17 geometry types.\nDuring this session and in subsequent session on Spatial Epidemiology, we shall be using sf package. This is because sf package has inbuilt capabilities of handling 7 most common data models of simple features. Further,R spatial ecosystem is more efficient with use of sf package compared to previous packages.\nShapefile format data can be imported into the R ecosystem using st_read() or read_sf() function. Both are similar in function with subtle differences. read_sf() chooses a few tidyverse defaults such as silent (st_read() gives a short report), returns a spatial tibble (st_read() returns a dataframe), doesnot convert strings to factors (st_read() converts to factors by default) and accepts list columns as input. For other formats st_as_sf() contains a family of conversion functions."
  },
  {
    "objectID": "spatial_data_mani.html#understanding-and-tranformation-of-coordinated-reference-system-crs.",
    "href": "spatial_data_mani.html#understanding-and-tranformation-of-coordinated-reference-system-crs.",
    "title": "Spatial Data Manipulation",
    "section": "",
    "text": "To know the crs of a spatial dataset, use sf::st_crs() function.\n\n\n\n\n\nWhen we have two or more related spatial datasets, and the CRS are different, we can transform the CRS using sf::st_transform() function.\n\n\n\n\n\n#Install packages. We need to install packages only once in the system.\n#install.packages(\"sf\")\n#install.packages(\"tidyverse\")\n#Load library. We need to load library everytime to use the functions\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\n\n\n\n\nworld &lt;- read_rds(here::here(\"data\", \"world_india_compliant.rds\"))\n\nst_crs(world)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\n\nindia &lt;- read_rds(here::here( \"data\", \"india_states.rds\"))\n\nindia &lt;- india |&gt; \n  janitor::clean_names()\n\nst_crs(india)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"unknown\"],\n        AREA[\"World - 85Â°S to 85Â°N\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\n# Transform to Geog CRS as that of world map\n\nindia &lt;- st_transform(india, crs = 4326)\n\n\n\n\n\nkerala &lt;- read_rds(here::here(\"data\",\"ker_panchayats.rds\"))\n\n# Clean names\n\nkerala &lt;- kerala |&gt; \n  janitor::clean_names()\n\n# Check CRS\nst_crs(kerala)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"unknown\"],\n        AREA[\"World\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\n\n# Reading RDS\ndf &lt;- read_rds(here::here( \"data\", \"dengue_sample.rds\"))\n\n# Cleaning names \ndf &lt;- df |&gt; \n  janitor::clean_names()\n\n# Converting as sf object\ndf &lt;- st_as_sf(df, coords = c(\"longitude\", \"latitude\"), crs = 4326)"
  },
  {
    "objectID": "spatial_data_mani.html#manipulating-spatial-data-based-on-their-location-and-shape.",
    "href": "spatial_data_mani.html#manipulating-spatial-data-based-on-their-location-and-shape.",
    "title": "Spatial Data Manipulation",
    "section": "",
    "text": "The process of selecting only those locations which have a specified attribute is known as sub-setting and the output is subset. It is done to understand spatial distribution of a given variable and thus identify underlying patterns. sf objects have inherent dataframe class. Hence, tidyverse::filter() function can be used for obtaining spatial subsets.\n\n# Example: Subset district from Kerala map whose name is Thiruvananthapuram\n\ntvm &lt;- kerala %&gt;% \n  filter(district == \"Thiruvananthapuram\")\n\n#Overview\ntvm %&gt;% \n  ggplot()+\n  geom_sf()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopology is defined as spatial relationship between the objects. These include relationships such as common boundaries, intersections, one within the other, crossings, disjunctions, etc.\n\n# To identify block from which cases have been reported: \n#It will include all cases which are within as well as \n#on the boundary of a polygon.\n\n(list &lt;- st_intersects(df, tvm))\n\nSparse geometry binary predicate list of length 250, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: 64\n 2: 52\n 3: 68\n 4: 52\n 5: 64\n 6: 57\n 7: 53\n 8: 52\n 9: 52\n 10: 52\n\n\nSimilarly, st_within() is used to find the relationship for those strictly within boundaries, st_disjoint() is opposite of st_intersect(), st_touches() is for those objects which are touching each other (eg. polygons) and st_within_distance() for the objects within a buffer distance from the said object.\n\n\n\n\n\nPresence of a unique key is pre-requisite to joining of datasets. The same concept is utilised for overlay or joining of two or more spatial datasets with location coordinates as unique key. st_join() performs spatial joins using location coordinates as unique key. By default, left join is undertaken. in is performed.\n\ndim(df)\n\n[1] 250  19\n\nnames(df)\n\n [1] \"hash_short\"       \"age\"              \"sex1\"             \"day\"             \n [5] \"month\"            \"year\"             \"date\"             \"panchayat_code\"  \n [9] \"population\"       \"dengue\"           \"month2\"           \"dengue1\"         \n[13] \"join_count_n_9_0\" \"area\"             \"aream2\"           \"popden\"          \n[17] \"dengpop\"          \"dengpopdens\"      \"geometry\"        \n\ndim(tvm)\n\n[1] 83 46\n\nnames(tvm)\n\n [1] \"join_count\"   \"area\"         \"code\"         \"panchayat\"    \"block\"       \n [6] \"municipal\"    \"corporate\"    \"district\"     \"location\"     \"state_id\"    \n[11] \"district_id\"  \"hospital_id\"  \"nameof_hos\"   \"typeof_hos\"   \"place_name\"  \n[16] \"d\"            \"m\"            \"s\"            \"x\"            \"d1\"          \n[21] \"m1\"           \"s1\"           \"y\"            \"postoffice\"   \"ho_sattach\"  \n[26] \"pincode\"      \"lo_body_name\" \"lo_body_type\" \"phone_no\"     \"bed_count\"   \n[31] \"timing\"       \"ambu_servi\"   \"cr_savailab\"  \"cr_snameadd\"  \"ho_stiming\"  \n[36] \"doctors\"      \"nurses\"       \"paramedics\"   \"ambient_ser\"  \"waste_dispo\" \n[41] \"facility\"     \"service\"      \"depart_name\"  \"no_of_asha\"   \"no_of_jphn\"  \n[46] \"geometry\"    \n\njoined &lt;- st_join(df, tvm)\n\ndim(joined)\n\n[1] 250  64\n\nnames(joined)\n\n [1] \"hash_short\"       \"age\"              \"sex1\"             \"day\"             \n [5] \"month\"            \"year\"             \"date\"             \"panchayat_code\"  \n [9] \"population\"       \"dengue\"           \"month2\"           \"dengue1\"         \n[13] \"join_count_n_9_0\" \"area.x\"           \"aream2\"           \"popden\"          \n[17] \"dengpop\"          \"dengpopdens\"      \"geometry\"         \"join_count\"      \n[21] \"area.y\"           \"code\"             \"panchayat\"        \"block\"           \n[25] \"municipal\"        \"corporate\"        \"district\"         \"location\"        \n[29] \"state_id\"         \"district_id\"      \"hospital_id\"      \"nameof_hos\"      \n[33] \"typeof_hos\"       \"place_name\"       \"d\"                \"m\"               \n[37] \"s\"                \"x\"                \"d1\"               \"m1\"              \n[41] \"s1\"               \"y\"                \"postoffice\"       \"ho_sattach\"      \n[45] \"pincode\"          \"lo_body_name\"     \"lo_body_type\"     \"phone_no\"        \n[49] \"bed_count\"        \"timing\"           \"ambu_servi\"       \"cr_savailab\"     \n[53] \"cr_snameadd\"      \"ho_stiming\"       \"doctors\"          \"nurses\"          \n[57] \"paramedics\"       \"ambient_ser\"      \"waste_dispo\"      \"facility\"        \n[61] \"service\"          \"depart_name\"      \"no_of_asha\"       \"no_of_jphn\"      \n\njoined %&gt;% ggplot()+\n  geom_sf()\n\n\n\n\n\n\n\n\n\n\n\n\n\nAggregation of spatial data is a process of condensing the dataset to obtain stratified summary measures.\n\n# Example: To get list of number of cases from each block\njoined %&gt;% \n  group_by(block) %&gt;% \n  summarise(cases = sum(dengue))\n\nSimple feature collection with 13 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 76.71811 ymin: 8.331344 xmax: 77.19514 ymax: 8.811173\nGeodetic CRS:  WGS 84\n# A tibble: 13 × 3\n   block          cases                                                 geometry\n   &lt;chr&gt;          &lt;dbl&gt;                                           &lt;GEOMETRY [°]&gt;\n 1 Athiyannoor        7 MULTIPOINT ((76.97908 8.400357), (76.97289 8.412542), (…\n 2 Chirayinkeezhu     2    MULTIPOINT ((76.78766 8.689444), (76.78253 8.657362))\n 3 Kazhakkoottam     13 MULTIPOINT ((76.81978 8.603385), (76.92593 8.573853), (…\n 4 Kilimanoor         2    MULTIPOINT ((76.87685 8.766624), (76.82115 8.811173))\n 5 Nedumangadu       11 MULTIPOINT ((77.02653 8.549563), (77.0238 8.549926), (7…\n 6 Nemom             38 MULTIPOINT ((76.98578 8.422667), (77.0061 8.439469), (7…\n 7 Parasala           1                                POINT (77.08794 8.331344)\n 8 Perumkadavila      5 MULTIPOINT ((77.19514 8.453647), (77.15896 8.461809), (…\n 9 T. Rural Block     7 MULTIPOINT ((76.96103 8.556535), (76.96095 8.555607), (…\n10 Vamanapuram        1                                POINT (76.92178 8.677913)\n11 Varkala            1                                 POINT (76.8104 8.732897)\n12 Vellanadu         19 MULTIPOINT ((77.07907 8.502114), (77.07872 8.507087), (…\n13 &lt;NA&gt;             143 MULTIPOINT ((76.96146 8.426247), (76.9614 8.426524), (7…\n\n\n\n\n\n\nTo determine area of a polygon, st_area() function is used.\n\n#Example\ntvm %&gt;% \n  filter(panchayat == \"Pallickal\") %&gt;% \n  st_area()\n\n17038202 [m^2]"
  },
  {
    "objectID": "spatial_data_mani.html#geometry-based-functions.",
    "href": "spatial_data_mani.html#geometry-based-functions.",
    "title": "Spatial Data Manipulation",
    "section": "",
    "text": "We can transform geometries with uniary and binary transformations. Uniary transformations are based on a single geometry in isolation and includes measures such as creation of buffers and centroids. Binary operations modify the shape of one geometry based on the other and includes measures such as clipping and geometry unions.\n\n\n\n\nThe centroid based processes identify the center of a sf object and is similar to measure of central tendency. There are many variations in ways to define centre of an sf object, however, use of geographical centre remains most common. st_centroid() provides centroid values.\n\npolygon_tvm &lt;- tvm %&gt;% \n  ggplot()+\n  geom_sf()+\n  labs(title = \"Map\")\n\npoint_tvm &lt;- st_centroid(tvm) %&gt;%  \n  ggplot()+\n  geom_sf()+\n  labs(title = \"Centroids\")\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nggpubr::ggarrange(polygon_tvm, point_tvm)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntvm &lt;- cbind(tvm, st_coordinates(st_centroid(tvm)))\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\n\n\n\n\nggplot()+\n  geom_sf(data = tvm %&gt;% \n  filter(panchayat == \"Pallickal\") %&gt;% \n  st_centroid() %&gt;% \n  st_buffer(dist = 5000) )+\n  geom_sf(data = df)\n\nWarning: st_centroid assumes attributes are constant over geometries"
  },
  {
    "objectID": "spatial_data_mani.html#saving-spatial-data-and-its-output.",
    "href": "spatial_data_mani.html#saving-spatial-data-and-its-output.",
    "title": "Spatial Data Manipulation",
    "section": "",
    "text": "Though st_write() function helps in saving spatial datasets as shapefiles, it is recommended that RDS data format be used as it uses less space in the system and is computationally faster to load subsequently."
  },
  {
    "objectID": "spatial_data_mani.html#methods-for-spatial-data-manipulation-of-raster-datasets.",
    "href": "spatial_data_mani.html#methods-for-spatial-data-manipulation-of-raster-datasets.",
    "title": "Spatial Data Manipulation",
    "section": "",
    "text": "To know as well as to set CRS of a raster data, raster::projections() function is used. The further difference as compared to vector data is that CRS has to be provided in proj4 definition format for the raster datasets. It does not accept CRS input in espg format.\n\n\n\n\n\nSimilar to area calculations in a vector dataset, we would like to know about the resolution of the raster data. The res() function provides dimensions of each of the cell of the raster grid in metres (By default, UTM projection is in metres).\n\n\n\n\n\nTo extract value for a specified location raster::cellFromXY() function can be used.\nTo subset based on extent, the bounding box details can be used.\nTo subset based on extent of a polygon/ other available vector dataset"
  },
  {
    "objectID": "spatial_data_mani.html#extract-raster-values-for-a-given-location.",
    "href": "spatial_data_mani.html#extract-raster-values-for-a-given-location.",
    "title": "Spatial Data Manipulation",
    "section": "",
    "text": "To extract values for a given location, there are multiple methods. The raster::extract() function is computationally demanding and takes a lot of time, especially for extracting raster data for a large number of polygons. An alternative from terra package as well as veloxraster appears promising but needs further deliberations.\n\n\n\n\nUnderstanding of map algebra makes raster processing faster. There are four subclasses of raster based operations as under:-\n\n\n\n\nThese operations act independently for each cell of the raster.\n\n\n\n\nAlso known a kernel, moving window or filter operations for raster data, they choose a central cell and its neighbourhood (such as surrounding eight cells, for example). The focal operation acts similar to spatial data aggregation and results in an output of aggregated values for each combination of central cell and its neighbourhood. raster::focal() function can be used for obtaining focal values by defining the matrix for central cell and its neighbourhood.\n\n\n\n\n\nThe principle behind zonal operations is same as that of focal operations. However, unlike in focal operations, zonal operations can go beyond a rectangular matrix of neighbourhood and can provide aggregated values for a pre-defined shape or distance or area, etc. The raster::zonal() function provides zonal statistics.\n\n\n\n\n\nThese operations are applied on the entire dataset and are used to summarise the characteristics of the given raster data such as mean, median, IQR, range etc of the datasets."
  },
  {
    "objectID": "spatial_data_mani.html#merging-rasters.",
    "href": "spatial_data_mani.html#merging-rasters.",
    "title": "Spatial Data Manipulation",
    "section": "",
    "text": "Two rasters can be merged using raster::merge() function in general. However, depending upon the raster characteristics, more refined methods are available such as from packages landsat, satellite, etc. Merging rasters require same resolution and crs of the rasters as prerequisites."
  }
]